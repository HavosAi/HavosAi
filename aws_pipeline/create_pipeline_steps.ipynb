{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fce699a4",
   "metadata": {},
   "source": [
    "This notebooks contains code for creating and testing pipeline steps scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d09f2c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from HavosAi.src.sagemaker_pipeline.constants import BASE_DIR, INPUTS_DIR, OUTPUTS_DIR, MODELS_DIR, OUTCOMES_MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aeec783",
   "metadata": {},
   "source": [
    "### download artifacts for the pipeline to this notebook instance\n",
    "\n",
    "it is done so that we can debug the scripts here, before deploying them to the pipeline containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b7b76a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo mkdir -p {BASE_DIR}\n",
    "!sudo mkdir -p {INPUTS_DIR}\n",
    "!sudo mkdir -p {OUTPUTS_DIR}\n",
    "!sudo mkdir -p {MODELS_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "219321c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def download_s3_folder(bucket_name, s3_folder, local_dir=None):\n",
    "    \"\"\"\n",
    "    Download the contents of a folder directory\n",
    "    Args:\n",
    "        bucket_name: the name of the s3 bucket\n",
    "        s3_folder: the folder path in the s3 bucket\n",
    "        local_dir: a relative or absolute directory path in the local file system\n",
    "    \"\"\"\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    for obj in bucket.objects.filter(Prefix=s3_folder):\n",
    "        target = obj.key if local_dir is None \\\n",
    "            else os.path.join(local_dir, os.path.relpath(obj.key, s3_folder))\n",
    "        if not os.path.exists(os.path.dirname(target)):\n",
    "            os.makedirs(os.path.dirname(target))\n",
    "        if obj.key[-1] == '/':\n",
    "            continue\n",
    "        bucket.download_file(obj.key, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "317dd3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# artifacts = [input_data_path_in_bucket , search_index_path_in_bucket, \n",
    "#                  population_tags_path_in_bucket, \n",
    "#                  outcomes_bert_model_path_in_bucket\n",
    "#                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "04eab315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3 = boto3.resource(\"s3\")\n",
    "\n",
    "# for artifact in artifacts:\n",
    "#     if artifact.endswith('/'):\n",
    "#         download_s3_folder(default_bucket, artifact, local_dir=f\"{base_dir}/{artifact}\")\n",
    "#     else:\n",
    "#         s3.Bucket(default_bucket).download_file(\n",
    "#             artifact, f\"{base_dir}/{artifact}\"\n",
    "#         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656dca09",
   "metadata": {},
   "source": [
    "### Prepare for creating PipelineStep scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4198a4a",
   "metadata": {},
   "source": [
    "### Abbreviations Resolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2472432a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HavosAi/src/sagemaker_pipeline/AbbreviationsResolverStep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HavosAi/src/sagemaker_pipeline/AbbreviationsResolverStep.py\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from src.text_processing.abbreviations_resolver import AbbreviationsResolver\n",
    "from src.sagemaker_pipeline.constants import CONFIG_INPUTS_DIR, INPUTS_DIR, MODELS_INPUTS_DIR, OUTPUTS_DIR, OUTCOMES_MODEL\n",
    "\n",
    "  \n",
    "if __name__ == \"__main__\":\n",
    "    with open(f\"{CONFIG_INPUTS_DIR}/config.json\", \"r\") as file:\n",
    "        config = json.load(file)\n",
    "        print(\"file config.json: \", config)\n",
    "        \n",
    "    df = pd.read_csv(\n",
    "        f\"{INPUTS_DIR}/subfolder-0-reduced.csv\",\n",
    "        sep=';',\n",
    "        index_col=0,\n",
    "    )\n",
    "    df.to_csv(f\"{OUTPUTS_DIR}/AbbreviationsResolverOutput.csv\", sep=';')\n",
    "    \n",
    "    \n",
    "    if config[\"Steps\"][\"AbbreviationsResolverStep\"][\"Apply\"] == \"True\":\n",
    "        print(\"AbbreviationsResolverStep\", \"true\")\n",
    "        \n",
    "        abbreviations_resolver = AbbreviationsResolver([]) \n",
    "        abbreviations_resolver.load_model(f\"{MODELS_INPUTS_DIR}\")\n",
    "        \n",
    "        print(\"Test before dump:\")\n",
    "        print(type(abbreviations_resolver.abbreviations_finder_dict), \n",
    "              type(abbreviations_resolver.sorted_resolved_abbreviations))\n",
    "        \n",
    "        with open(f\"{OUTCOMES_MODEL}/abbreviation_resolver.pickle\", \"wb\") as outfile:\n",
    "            pickle.dump(abbreviations_resolver, outfile)\n",
    "            \n",
    "        print(\"Test after dump:\")\n",
    "        with open(f\"{OUTCOMES_MODEL}/abbreviation_resolver.pickle\", \"rb\") as outfile:\n",
    "            abbreviations_resolver2 = pickle.load(outfile)\n",
    "        print(type(abbreviations_resolver2.abbreviations_finder_dict), \n",
    "              type(abbreviations_resolver2.sorted_resolved_abbreviations))\n",
    "        \n",
    "    else:\n",
    "        print(\"AbbreviationsResolverStep\", \"false\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e77ec50",
   "metadata": {},
   "source": [
    "### SearchIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2e19e84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HavosAi/src/sagemaker_pipeline/SearchIndexStep.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%writefile HavosAi/src/sagemaker_pipeline/SearchIndexStep.py\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from src.text_processing.search_engine_insensitive_to_spelling import SearchEngineInsensitiveToSpelling\n",
    "from src.sagemaker_pipeline.constants import CONFIG_INPUTS_DIR, INPUTS_DIR, MODELS_INPUTS_DIR, OUTPUTS_DIR, OUTCOMES_MODEL\n",
    "\n",
    "  \n",
    "if __name__ == \"__main__\":\n",
    "    with open(f\"{CONFIG_INPUTS_DIR}/config.json\", \"r\") as file:\n",
    "        config = json.load(file)\n",
    "        print(\"file config.json: \", config)\n",
    "        \n",
    "    df = pd.read_csv(\n",
    "        f\"{INPUTS_DIR}/AbbreviationsResolverOutput.csv\",\n",
    "        sep=';',\n",
    "        index_col=0,\n",
    "    )\n",
    "    df.to_csv(f\"{OUTPUTS_DIR}/SearchIndexOutput.csv\", sep=';')\n",
    "    \n",
    "    \n",
    "    if config[\"Steps\"][\"SearchIndexStep\"][\"Apply\"] == \"True\":\n",
    "        print(\"SearchIndexStep\", \"true\")\n",
    "        search_engine = SearchEngineInsensitiveToSpelling(\n",
    "            abbreviation_folder=MODELS_INPUTS_DIR, \n",
    "            load_abbreviations=True,\n",
    "        )\n",
    "        search_engine.create_inverted_index(df)\n",
    "        with open(f\"{OUTCOMES_MODEL}/search_index.pickle\", \"wb\") as outfile:\n",
    "            pickle.dump(search_engine, outfile)\n",
    "    else:\n",
    "        print(\"SearchIndexStep\", \"false\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f343c6b",
   "metadata": {},
   "source": [
    "### AdvancedTextNormalizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "49493845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HavosAi/src/sagemaker_pipeline/AdvancedTextNormalizerStep.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%writefile HavosAi/src/sagemaker_pipeline/AdvancedTextNormalizerStep.py\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from src.text_processing.advanced_text_normalization import AdvancedTextNormalizer\n",
    "from src.sagemaker_pipeline.constants import CONFIG_INPUTS_DIR, INPUTS_DIR, MODELS_INPUTS_DIR, OUTPUTS_DIR, OUTCOMES_MODEL\n",
    "\n",
    "  \n",
    "if __name__ == \"__main__\":\n",
    "    with open(f\"{CONFIG_INPUTS_DIR}/config.json\", \"r\") as file:\n",
    "        config = json.load(file)\n",
    "        print(\"file config.json: \", config)\n",
    "        \n",
    "    df = pd.read_csv(\n",
    "            f\"{INPUTS_DIR}/SearchIndexOutput.csv\",\n",
    "        sep=';',\n",
    "        index_col=0,\n",
    "    )\n",
    "\n",
    "    with open(f\"{MODELS_INPUTS_DIR}/abbreviation_resolver.pickle\", \"rb\") as outfile:\n",
    "        abbreviations_resolver = pickle.load(outfile)\n",
    "    if config[\"Steps\"][\"AdvancedTextNormalizerStep\"][\"Apply\"] == \"True\":\n",
    "        print(\"AdvancedTextNormalizerStep\", \"true\")\n",
    "        advanced_text_normalizer = AdvancedTextNormalizer(abbreviations_resolver)\n",
    "        df = advanced_text_normalizer.normalize_text_for_df(df)\n",
    "    else:\n",
    "        print(\"AdvancedTextNormalizerStep\", \"false\")\n",
    "    df.to_csv(f\"{OUTPUTS_DIR}/AdvancedTextNormalizerOutput.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab986467",
   "metadata": {},
   "source": [
    "### KeywordsNormalizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f496741c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HavosAi/src/sagemaker_pipeline/KeywordsNormalizerStep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HavosAi/src/sagemaker_pipeline/KeywordsNormalizerStep.py\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from src.text_processing.keywords_normalizer import KeywordsNormalizer\n",
    "from src.sagemaker_pipeline.constants import CONFIG_INPUTS_DIR, INPUTS_DIR, OUTPUTS_DIR, OUTCOMES_MODEL\n",
    "\n",
    "  \n",
    "if __name__ == \"__main__\":\n",
    "    with open(f\"{CONFIG_INPUTS_DIR}/config.json\", \"r\") as file:\n",
    "        config = json.load(file)\n",
    "        print(\"file config.json: \", config)\n",
    "        \n",
    "    df = pd.read_csv(\n",
    "        f\"{INPUTS_DIR}/AdvancedTextNormalizerOutput.csv\",\n",
    "        sep=';',\n",
    "        index_col=0,\n",
    "    )\n",
    "\n",
    "    if config[\"Steps\"][\"KeywordsNormalizerStep\"][\"Apply\"] == \"True\":\n",
    "        print(\"KeywordsNormalizerStep\", \"true\")\n",
    "        # no column \"identificators\" in `key_words_column_names`, as we couldn't find it\n",
    "        df = KeywordsNormalizer().normalize_key_words(df, key_words_column_names=[\"keywords\",])\n",
    "    else:\n",
    "        print(\"KeywordsNormalizerStep\", \"false\")\n",
    "    df.to_csv(f\"{OUTPUTS_DIR}/KeywordsNormalizerOutput.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff3643e",
   "metadata": {},
   "source": [
    "### JournalNormalizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "47c82ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HavosAi/src/sagemaker_pipeline/JournalNormalizerStep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HavosAi/src/sagemaker_pipeline/JournalNormalizerStep.py\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from src.text_processing.journal_normalizer import JournalNormalizer\n",
    "from src.sagemaker_pipeline.constants import CONFIG_INPUTS_DIR, INPUTS_DIR, OUTPUTS_DIR\n",
    "\n",
    "  \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    with open(f\"{CONFIG_INPUTS_DIR}/config.json\", \"r\") as file:\n",
    "        config = json.load(file)\n",
    "        print(\"file config.json: \", config)\n",
    "        \n",
    "    df = pd.read_csv(\n",
    "            f\"{INPUTS_DIR}/KeywordsNormalizerOutput.csv\",\n",
    "        sep=';',\n",
    "        index_col=0,\n",
    "    )\n",
    "    \n",
    "    if config[\"Steps\"][\"JournalNormalizerStep\"][\"Apply\"] == \"True\":\n",
    "        print(\"JournalNormalizerStep\", \"true\")\n",
    "        df = JournalNormalizer().correct_journal_names(df, journal_column=\"journal_name\")\n",
    "    else:\n",
    "        print(\"JournalNormalizerStep\", \"false\")\n",
    "    df.to_csv(f\"{OUTPUTS_DIR}/JournalNormalizerOutput.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a9af43",
   "metadata": {},
   "source": [
    "### AuthorAndAffiliationsProcessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fddd601b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HavosAi/src/sagemaker_pipeline/AuthorAndAffiliationsProcessingStep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HavosAi/src/sagemaker_pipeline/AuthorAndAffiliationsProcessingStep.py\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from src.text_processing.author_and_affiliations_processing import AuthorAndAffiliationsProcessing\n",
    "from src.sagemaker_pipeline.constants import CONFIG_INPUTS_DIR, INPUTS_DIR, OUTPUTS_DIR\n",
    "\n",
    "  \n",
    "if __name__ == \"__main__\":\n",
    "    print(\"/opt/ml/processing/data\", os.listdir(\"/opt/ml/processing/data\"))\n",
    "    # tmp\n",
    "    df_ex = pd.read_excel(\"/opt/ml/processing/data/districts.xlsx\")\n",
    "    \n",
    "    with open(f\"{CONFIG_INPUTS_DIR}/config.json\", \"r\") as file:\n",
    "        config = json.load(file)\n",
    "        print(\"file config.json: \", config)\n",
    "        \n",
    "    df = pd.read_csv(\n",
    "        f\"{INPUTS_DIR}/JournalNormalizerOutput.csv\",\n",
    "        sep=';',\n",
    "        index_col=0,\n",
    "    )\n",
    "\n",
    "    if config[\"Steps\"][\"AuthorAndAffiliationsProcessingStep\"][\"Apply\"] == \"True\":\n",
    "        print(\"AuthorAndAffiliationsProcessingStep\", \"true\")\n",
    "        os.chdir(\"/opt/ml/processing/data\")\n",
    "        df_ex = pd.read_excel(\"../data/GeoRegions.xlsx\")\n",
    "        df = AuthorAndAffiliationsProcessing().process_authors_and_affiliations(df)\n",
    "        os.chdir(\"/\")\n",
    "    else:\n",
    "        print(\"AuthorAndAffiliationsProcessingStep\", \"false\")\n",
    "    df.to_csv(f\"{OUTPUTS_DIR}/AuthorAndAffiliationsProcessingOutput.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2964206a",
   "metadata": {},
   "source": [
    "### GeoNameFinder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e11d08f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HavosAi/src/sagemaker_pipeline/GeoNameFinderStep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HavosAi/src/sagemaker_pipeline/GeoNameFinderStep.py\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "from src.text_processing.geo_names_finder import GeoNameFinder\n",
    "from src.sagemaker_pipeline.constants import CONFIG_INPUTS_DIR, INPUTS_DIR, MODELS_INPUTS_DIR, OUTPUTS_DIR, OUTCOMES_MODEL\n",
    "\n",
    "  \n",
    "if __name__ == \"__main__\":\n",
    "    print(\"/opt/ml/processing/data\", os.listdir(\"/opt/ml/processing/data\"))\n",
    "    # tmp\n",
    "    df_ex = pd.read_excel(\"/opt/ml/processing/data/districts.xlsx\")\n",
    "    \n",
    "    with open(f\"{CONFIG_INPUTS_DIR}/config.json\", \"r\") as file:\n",
    "        config = json.load(file)\n",
    "        print(\"file config.json: \", config)\n",
    "        \n",
    "    df = pd.read_csv(\n",
    "            f\"{INPUTS_DIR}/AuthorAndAffiliationsProcessingOutput.csv\",\n",
    "        sep=';',\n",
    "        index_col=0,\n",
    "    )\n",
    "\n",
    "    with open(f\"{MODELS_INPUTS_DIR}/search_index.pickle\", \"rb\") as outfile:\n",
    "        search_index = pickle.load(outfile)\n",
    "    if config[\"Steps\"][\"GeoNameFinderStep\"][\"Apply\"] == \"True\":\n",
    "        print(\"GeoNameFinderStep\", \"true\")\n",
    "        os.chdir(\"/opt/ml/processing/data\")\n",
    "        df = GeoNameFinder().label_articles_with_geo_names(df, search_index)\n",
    "        os.chdir(\"/\")\n",
    "    else:\n",
    "        print(\"GeoNameFinderStep\", \"false\")\n",
    "    print(f\"saving to {OUTPUTS_DIR}/GeoNameFinderOutput.csv\")\n",
    "    df.to_csv(f\"{OUTPUTS_DIR}/GeoNameFinderOutput.csv\", sep=';')\n",
    "    print(\"saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e4f71f",
   "metadata": {},
   "source": [
    "### CropsSearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "81cda3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HavosAi/src/sagemaker_pipeline/CropsSearchStep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HavosAi/src/sagemaker_pipeline/CropsSearchStep.py\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "from src.text_processing.crops_finder import CropsSearch\n",
    "from src.sagemaker_pipeline.constants import CONFIG_INPUTS_DIR, INPUTS_DIR, MODELS_INPUTS_DIR, OUTPUTS_DIR, OUTCOMES_MODEL\n",
    "\n",
    "  \n",
    "if __name__ == \"__main__\":\n",
    "    print(\"/opt/ml/processing/data\", os.listdir(\"/opt/ml/processing/data\"))\n",
    "    # tmp\n",
    "    df_ex = pd.read_excel(\"/opt/ml/processing/data/districts.xlsx\")\n",
    "    \n",
    "    with open(f\"{CONFIG_INPUTS_DIR}/config.json\", \"r\") as file:\n",
    "        config = json.load(file)\n",
    "        print(\"file config.json: \", config)\n",
    "        \n",
    "    df = pd.read_csv(\n",
    "        f\"{INPUTS_DIR}/GeoNameFinderOutput.csv\",\n",
    "        sep=';',\n",
    "        index_col=0,\n",
    "    )\n",
    "\n",
    "    with open(f\"{MODELS_INPUTS_DIR}/search_index.pickle\", \"rb\") as outfile:\n",
    "        search_index = pickle.load(outfile)\n",
    "    if config[\"Steps\"][\"CropsSearchStep\"][\"Apply\"] == \"True\":\n",
    "        print(\"CropsSearchStep\", \"true\")\n",
    "        os.chdir(\"/opt/ml/processing/data\")\n",
    "        df = CropsSearch(\n",
    "            search_index, \n",
    "            \"../data/map_plant_products.xlsx\"\n",
    "        ).find_crops(df, column_name=\"plant_products_search\")\n",
    "        df = CropsSearch(\n",
    "            search_index, \n",
    "            \"../data/map_animal_products.xlsx\"\n",
    "        ).find_crops(df, column_name=\"animal_products_search\")\n",
    "        df = CropsSearch(\n",
    "            search_index, \n",
    "            \"../data/map_animals.xlsx\"\n",
    "        ).find_crops(df, column_name=\"animals_found\")\n",
    "        os.chdir(\"/\")\n",
    "    else:\n",
    "        print(\"CropsSearchStep\", \"false\")\n",
    "    df.to_csv(f\"{OUTPUTS_DIR}/CropsSearchOutput.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b6c3cb",
   "metadata": {},
   "source": [
    "### PopulationTagsFinder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bb9f198b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HavosAi/src/sagemaker_pipeline/PopulationTagsFinderStep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HavosAi/src/sagemaker_pipeline/PopulationTagsFinderStep.py\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from src.text_processing.population_tags_finder import PopulationTagsFinder\n",
    "from src.sagemaker_pipeline.constants import CONFIG_INPUTS_DIR, INPUTS_DIR, MODELS_INPUTS_DIR, OUTPUTS_DIR, OUTCOMES_MODEL\n",
    "\n",
    "  \n",
    "if __name__ == \"__main__\":\n",
    "    with open(f\"{CONFIG_INPUTS_DIR}/config.json\", \"r\") as file:\n",
    "        config = json.load(file)\n",
    "        print(\"file config.json: \", config)\n",
    "        \n",
    "    df = pd.read_csv(\n",
    "            f\"{INPUTS_DIR}/CropsSearchOutput.csv\",\n",
    "        sep=';',\n",
    "        index_col=0,\n",
    "    )\n",
    "\n",
    "    with open(f\"{MODELS_INPUTS_DIR}/search_index.pickle\", \"rb\") as outfile:\n",
    "        search_index = pickle.load(outfile)\n",
    "    if config[\"Steps\"][\"PopulationTagsFinderStep\"][\"Apply\"] == \"True\":\n",
    "        print(\"PopulationTagsFinderStep\", \"true\")\n",
    "        population_tags_finder = PopulationTagsFinder()\n",
    "        df = population_tags_finder.label_with_population_tags(df, search_index)\n",
    "    else:\n",
    "        print(\"PopulationTagsFinderStep\", \"false\")\n",
    "    df.to_csv(f\"{OUTPUTS_DIR}/PopulationTagsFinderOutput.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5328e2",
   "metadata": {},
   "source": [
    "### ColumnFiller\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "50356c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HavosAi/src/sagemaker_pipeline/ColumnFillerStep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HavosAi/src/sagemaker_pipeline/ColumnFillerStep.py\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "from src.text_processing.column_filler import ColumnFiller\n",
    "from src.sagemaker_pipeline.constants import CONFIG_INPUTS_DIR, INPUTS_DIR, SEARCH_INPUTS_DIR, ABBREV_INPUTS_DIR, OUTPUTS_DIR, OUTCOMES_MODEL\n",
    "\n",
    "  \n",
    "if __name__ == \"__main__\":\n",
    "    df_tmp = pd.read_excel(\"/opt/ml/processing/data/population_tags.xlsx\")\n",
    "    \n",
    "    with open(f\"{CONFIG_INPUTS_DIR}/config.json\", \"r\") as file:\n",
    "        config = json.load(file)\n",
    "        print(\"file config.json: \", config)\n",
    "        \n",
    "    df = pd.read_csv(\n",
    "            f\"{INPUTS_DIR}/PopulationTagsFinderOutput.csv\",\n",
    "        sep=';',\n",
    "        index_col=0,\n",
    "    )\n",
    "\n",
    "    with open(f\"{SEARCH_INPUTS_DIR}/search_index.pickle\", \"rb\") as outfile:\n",
    "        search_index = pickle.load(outfile)\n",
    "    with open(f\"{ABBREV_INPUTS_DIR}/abbreviation_resolver.pickle\", \"rb\") as outfile:\n",
    "        abbreviation_resolver = pickle.load(outfile)\n",
    "    \n",
    "    print(\"Abbreviation_resolver check:\")\n",
    "    print(type(abbreviation_resolver))\n",
    "    try:\n",
    "        print(type(abbreviation_resolver.abbreviations_finder_dict))\n",
    "    except:\n",
    "        print(\"error: abbreviations_finder_dict\")  \n",
    "    try:\n",
    "        print(type(abbreviation_resolver.resolved_abbreviations))\n",
    "    except:\n",
    "        print(\"error: resolved_abbreviations\") \n",
    "    try:\n",
    "        print(type(abbreviation_resolver.words_to_abbreviations))\n",
    "    except:\n",
    "        print(\"error: words_to_abbreviations\")\n",
    "    try:\n",
    "        print(type(abbreviation_resolver.sorted_resolved_abbreviations))\n",
    "    except:\n",
    "        print(\"error: sorted_resolved_abbreviations\")\n",
    "    try:\n",
    "        print(type(abbreviation_resolver.sorted_words_to_abbreviations))\n",
    "    except:\n",
    "        print(\"error: sorted_words_to_abbreviations\")\n",
    "    \n",
    "    if config[\"Steps\"][\"ColumnFillerStep\"][\"Apply\"] == \"True\":\n",
    "        print(\"ColumnFillerStep\", \"true\")\n",
    "        os.chdir(\"/opt/ml/processing/data\")\n",
    "        df = ColumnFiller(\n",
    "            dict_filename=\"../data/population_tags.xlsx\",\n",
    "        ).label_articles_with_outcomes(\n",
    "            \"gender_age_population_tags\", \n",
    "            df, \n",
    "            search_index, \n",
    "            abbreviation_resolver,\n",
    "        )\n",
    "        os.chdir(\"/\")\n",
    "    else:\n",
    "        print(\"ColumnFillerStep\", \"false\")\n",
    "    df.to_csv(f\"{OUTPUTS_DIR}/ColumnFillerOutput.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb82d64",
   "metadata": {},
   "source": [
    "### ProgramExtractor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b29a70fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HavosAi/src/sagemaker_pipeline/ProgramExtractorStep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HavosAi/src/sagemaker_pipeline/ProgramExtractorStep.py\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "from src.interventions_labeling_lib.programs_extractor import ProgramExtractor\n",
    "from src.sagemaker_pipeline.constants import CONFIG_INPUTS_DIR, INPUTS_DIR, SEARCH_INPUTS_DIR, ABBREV_INPUTS_DIR, OUTPUTS_DIR, OUTCOMES_MODEL\n",
    "\n",
    "  \n",
    "if __name__ == \"__main__\":\n",
    "    df_tmp = pd.read_excel(\"/opt/ml/processing/data/extracted_programs.xlsx\")\n",
    "    with open(f\"/opt/ml/processing/tmp/programs_extraction_model_2619/meta.json\", \"r\") as file:\n",
    "        json_tmp = json.load(file)\n",
    "    \n",
    "    with open(f\"{CONFIG_INPUTS_DIR}/config.json\", \"r\") as file:\n",
    "        config = json.load(file)\n",
    "        print(\"file config.json: \", config)\n",
    "        \n",
    "    df = pd.read_csv(\n",
    "            f\"{INPUTS_DIR}/ColumnFillerOutput.csv\",\n",
    "        sep=';',\n",
    "        index_col=0,\n",
    "    )\n",
    "\n",
    "    with open(f\"{SEARCH_INPUTS_DIR}/search_index.pickle\", \"rb\") as outfile:\n",
    "        search_index = pickle.load(outfile)\n",
    "    with open(f\"{ABBREV_INPUTS_DIR}/abbreviation_resolver.pickle\", \"rb\") as outfile:\n",
    "        abbreviation_resolver = pickle.load(outfile)\n",
    "    if config[\"Steps\"][\"ProgramExtractorStep\"][\"Apply\"] == \"True\":\n",
    "        print(\"ProgramExtractorStep\", \"true\")\n",
    "        os.chdir(\"/opt/ml/processing/data\")\n",
    "        df = ProgramExtractor([]).label_articles_with_programs(df, search_index, abbreviation_resolver)\n",
    "        os.chdir(\"/\")\n",
    "    else:\n",
    "        print(\"ProgramExtractorStep\", \"false\")\n",
    "    df.to_csv(f\"{OUTPUTS_DIR}/ProgramExtractorOutput.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b952ca",
   "metadata": {},
   "source": [
    "### Outcomes finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "936c01cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HavosAi/src/sagemaker_pipeline/OutcomesFinderStep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HavosAi/src/sagemaker_pipeline/OutcomesFinderStep.py\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from src.text_processing.all_column_filler import AllColumnFiller\n",
    "from src.sagemaker_pipeline.constants import  CONFIG_INPUTS_DIR, INPUTS_DIR, OUTPUTS_DIR, OUTCOMES_MODEL\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with open(f\"{CONFIG_INPUTS_DIR}/config.json\", \"r\") as file:\n",
    "        config = json.load(file)\n",
    "        print(\"file config.json: \", config)\n",
    "#     with open(f\"{CONFIG_OUTPUTS_DIR}/config.json\", \"w\") as file:\n",
    "#         json.dump(config, file)\n",
    "    df = pd.read_csv(\n",
    "        f\"{INPUTS_DIR}/ProgramExtractorOutput.csv\",\n",
    "        sep=';',\n",
    "        index_col=0,\n",
    "    )    \n",
    "    if config[\"Steps\"][\"OutcomesFinderStep\"][\"Apply\"] == \"True\":\n",
    "        print(\"OutcomesFinderStep\", \"true\")\n",
    "        column_info = dict()\n",
    "        column_info[\"model_folder\"] = OUTCOMES_MODEL\n",
    "        all_column_filler = AllColumnFiller()\n",
    "        df = all_column_filler.fill_outcomes(df, None, None, column_info)\n",
    "    else:\n",
    "        print(\"OutcomesFinderStep\", \"false\")\n",
    "    print(f'Outcomes finder. Saving df to {OUTPUTS_DIR}/OutcomesFinderOutput.csv')\n",
    "    df.to_csv(f\"{OUTPUTS_DIR}/OutcomesFinderOutput.csv\", sep=';')\n",
    "    print(f'Outcomes finder. Saved df.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a223a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2556393",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
