{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0f601c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maryia_Ivanina\\Anaconda3\\envs\\cornell\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import pickle\n",
    "from sklearn.utils import shuffle\n",
    "from importlib import reload\n",
    "from langdetect import detect\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from dateutil.parser import parse\n",
    "import spacy\n",
    "from time import time\n",
    "\n",
    "sys.path.append('../src')\n",
    "\n",
    "from commons import elastic\n",
    "from text_processing import text_normalizer\n",
    "text_normalizer = reload(text_normalizer)\n",
    "from text_processing import duplicate_finder\n",
    "from utilities import excel_writer, excel_reader, utils\n",
    "from text_processing import abbreviations_resolver\n",
    "abbreviations_resolver = reload(abbreviations_resolver)\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lmtzr = WordNetLemmatizer()\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "_abbreviations_resolver = abbreviations_resolver.AbbreviationsResolver([])\n",
    "_abbreviations_resolver.load_model(\"../model/abbreviations_dicts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f06661b",
   "metadata": {},
   "source": [
    "# Prepare data from txt to excel files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2083f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = {}\n",
    "global_id = 0\n",
    "mappings_for_columns = {\n",
    "    \"T1  - \": \"title\",\n",
    "    \"A1  - \": \"authors\",\n",
    "    \"AB  - \": \"abstract\",\n",
    "    \"DO  - \": \"doi\",\n",
    "    \"KW  - \": \"keywords\",\n",
    "    \"PY  - \": \"year\",\n",
    "    \"UR  - \": \"url\"\n",
    "}\n",
    "for file in os.listdir(\"../tmp/folder_with_egm_files/\"):\n",
    "    filename = file.split(\".\")[0]\n",
    "    file = os.path.join(\"../tmp/folder_with_egm_files/\", file)\n",
    "    keywords_started = False\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f.readlines():\n",
    "            if not line.strip():\n",
    "                global_id += 1\n",
    "                continue\n",
    "            if global_id not in all_docs:\n",
    "                all_docs[global_id] = {}\n",
    "                for key_ in mappings_for_columns:\n",
    "                    all_docs[global_id][mappings_for_columns[key_]] = []\n",
    "                all_docs[global_id][\"File\"] = filename\n",
    "            if line.startswith(\"KW  - \"):\n",
    "                keywords_started = True\n",
    "                continue\n",
    "            if keywords_started and re.search(\"^[A-Z0-9]{2}  - \", line):\n",
    "                keywords_started = False\n",
    "            if keywords_started:\n",
    "                all_docs[global_id][\"keywords\"].append(line.strip().lower())\n",
    "            for start_symbols in mappings_for_columns:\n",
    "                if start_symbols == \"KW  - \":\n",
    "                    continue\n",
    "                column_name = mappings_for_columns[start_symbols]\n",
    "                if line.startswith(start_symbols):\n",
    "                    all_docs[global_id][column_name].append(line.replace(start_symbols, \"\").strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b565c1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _id in all_docs:\n",
    "    for column in all_docs[_id]:\n",
    "        if type(all_docs[_id][column]) != str:\n",
    "            all_docs[_id][column] = \";\".join(all_docs[_id][column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23401785",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_docs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37545437",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    if df[\"year\"].values[i].strip():\n",
    "        df[\"year\"].values[i] = int(df[\"year\"].values[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11497c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n",
      "Saved to ../tmp/combined_egm_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "excel_writer.ExcelWriter().save_df_in_excel(df, \"../tmp/combined_egm_data.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb05a297",
   "metadata": {},
   "source": [
    "# Check Lab studies and Geo names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cc56363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read file ../tmp/combined_egm_data.xlsx: 0.11s\n",
      "Processed file ../tmp/combined_egm_data.xlsx: 0.06s\n"
     ]
    }
   ],
   "source": [
    "df = excel_reader.ExcelReader().read_df_from_excel(\"../tmp/combined_egm_data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "203354a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"identificators\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bb30993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if needed\n",
    "df[\"keywords\"] = \"\"\n",
    "df[\"title\"] = df[\"Title\"]\n",
    "df[\"abstract\"] = df[\"Abstract\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d0fe6de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Included (abstract)      100\n",
       "Lab studies              100\n",
       "Non-causal studies       100\n",
       "High-income countries    100\n",
       "No intervention          100\n",
       "Name: File, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"File\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d46f055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 articles\n",
      "Processed 499 articles\n",
      "Processed 0 abbreviations\n",
      "Processed 3000 abbreviations\n",
      "Processed 6000 abbreviations\n",
      "Processed 9000 abbreviations\n",
      "Processed 12000 abbreviations\n",
      "Processed 15000 abbreviations\n",
      "Processed 18000 abbreviations\n",
      "Processed 21000 abbreviations\n",
      "Processed 24000 abbreviations\n",
      "Processed 27000 abbreviations\n",
      "Processed 27129 abbreviations\n"
     ]
    }
   ],
   "source": [
    "from text_processing import search_engine_insensitive_to_spelling\n",
    "from text_processing import all_column_filler\n",
    "search_engine_inverted_index = search_engine_insensitive_to_spelling.SearchEngineInsensitiveToSpelling(\n",
    "        load_abbreviations = True)\n",
    "search_engine_inverted_index.create_inverted_index(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daff3441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started processing  {'column_filler_class': 'StudyTypeLabeller', 'folder': '../model/study_type_multi_new_data_with_keywords_agg_more_sub_groups_more_data_1', 'meta_folder': '../model/study_type_multi_meta_new_data_with_keywords_agg_more_sub_groups_more_data_1', 'scibert_model_folder': '../model/scibert_scivocab_uncased', 'model_with_agg': True}\n",
      "High level label: Book chapter\n",
      "High level label: Field study\n",
      "High level label: Greenhouse study\n",
      "High level label: Impact evaluation\n",
      "High level label: Laboratory study\n",
      "High level label: Meta analysis\n",
      "High level label: Modelling study\n",
      "High level label: Observational study\n",
      "High level label: Randomized controlled trials\n",
      "High level label: Review paper\n",
      "High level label: Systematic review\n",
      "High level label: Interview\n",
      "High level label: Survey/Questionnaire\n",
      "High level label: Experimental research\n",
      "High level label: Economic simulation\n",
      "High level label: Landscape/environmental simulation\n",
      "High level label: Policy analysis\n",
      "Labelled articles with outcomes: 212\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "INFO:tensorflow:Using config: {'_model_dir': '../model/study_type_multi_new_data_with_keywords_agg_more_sub_groups_more_data_1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000023F28DA26A0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '../model/study_type_multi_new_data_with_keywords_agg_more_sub_groups_more_data_1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000023F28DA26A0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config is done\n",
      "INFO:tensorflow:Writing example 0 of 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] effect of educational program for prevention of health hazards in poultry processing sla ##ughter ##house ' s workers in egypt [SEP] background : poultry workers are exposed to a variety of occupational health hazards on a daily basis . aim ( s ) : to assess the effect of educational program for prevention of health hazards on knowledge and self - reported practices of poultry processing sla ##ughter ##house ' s workers in egypt . method ( s ) : a quasi - experimental one group pretest - post ##test design was utilized . setting ; the study was conducted in large poultry sla ##ughter ##house at el men ##of ##ia govern ##ora ##te . a systematic random sample of 306 poultry workers was selected . two tools were used to collect data , the first is poultry processing sla ##ughter ##house ' s health hazards questionnaire and the other is an observational checklist for poultry work environment . result ( s ) : reveals that 3 . 4 % , 97 . 7 % and 88 . 8 % of workers had satisfactory level of knowledge in pre , post and follow up tests respectively . while 2 . 3 % , 96 . 1 % and 91 . 3 % of workers had satisfactory level of self - reported practices in pre , post and follow up tests respectively . there was a highly statistical significant correlation between workers ' knowledge , health belief scale and self - reported practices ( p [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] effect of educational program for prevention of health hazards in poultry processing sla ##ughter ##house ' s workers in egypt [SEP] background : poultry workers are exposed to a variety of occupational health hazards on a daily basis . aim ( s ) : to assess the effect of educational program for prevention of health hazards on knowledge and self - reported practices of poultry processing sla ##ughter ##house ' s workers in egypt . method ( s ) : a quasi - experimental one group pretest - post ##test design was utilized . setting ; the study was conducted in large poultry sla ##ughter ##house at el men ##of ##ia govern ##ora ##te . a systematic random sample of 306 poultry workers was selected . two tools were used to collect data , the first is poultry processing sla ##ughter ##house ' s health hazards questionnaire and the other is an observational checklist for poultry work environment . result ( s ) : reveals that 3 . 4 % , 97 . 7 % and 88 . 8 % of workers had satisfactory level of knowledge in pre , post and follow up tests respectively . while 2 . 3 % , 96 . 1 % and 91 . 3 % of workers had satisfactory level of self - reported practices in pre , post and follow up tests respectively . there was a highly statistical significant correlation between workers ' knowledge , health belief scale and self - reported practices ( p [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 907 131 6336 1618 168 5200 131 947 16227 121 20739 2307 17615 14683 8901 2505 112 5555 121 21946 103 2740 862 20739 5555 220 4724 147 106 3835 131 11069 947 16227 191 106 4122 2525 205 2579 145 112 546 862 147 1285 111 907 131 6336 1618 168 5200 131 947 16227 191 1767 137 1968 579 1214 5423 131 20739 2307 17615 14683 8901 2505 112 5555 121 21946 205 551 145 112 546 862 106 8889 579 1798 482 583 29342 579 1422 5528 899 241 6744 205 2707 1814 111 527 241 2728 121 1135 20739 17615 14683 8901 235 847 1801 1010 426 3141 7936 282 205 106 5158 1533 1498 131 27962 20739 5555 241 2350 205 502 3674 267 501 147 9921 453 422 111 705 165 20739 2307 17615 14683 8901 2505 112 947 16227 5692 137 111 494 165 130 11220 22919 168 20739 697 1451 205 1186 145 112 546 862 8234 198 239 205 286 1863 422 8276 205 450 1863 137 8203 205 493 1863 131 5555 883 11675 615 131 1767 121 382 422 1422 137 589 692 2732 1222 205 969 170 205 239 1863 422 7380 205 158 1863 137 9046 205 239 1863 131 5555 883 11675 615 131 1968 579 1214 5423 121 382 422 1422 137 589 692 2732 1222 205 461 241 106 2487 2397 684 2364 467 5555 2505 1767 422 947 8905 2211 137 1968 579 1214 5423 145 118 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 907 131 6336 1618 168 5200 131 947 16227 121 20739 2307 17615 14683 8901 2505 112 5555 121 21946 103 2740 862 20739 5555 220 4724 147 106 3835 131 11069 947 16227 191 106 4122 2525 205 2579 145 112 546 862 147 1285 111 907 131 6336 1618 168 5200 131 947 16227 191 1767 137 1968 579 1214 5423 131 20739 2307 17615 14683 8901 2505 112 5555 121 21946 205 551 145 112 546 862 106 8889 579 1798 482 583 29342 579 1422 5528 899 241 6744 205 2707 1814 111 527 241 2728 121 1135 20739 17615 14683 8901 235 847 1801 1010 426 3141 7936 282 205 106 5158 1533 1498 131 27962 20739 5555 241 2350 205 502 3674 267 501 147 9921 453 422 111 705 165 20739 2307 17615 14683 8901 2505 112 947 16227 5692 137 111 494 165 130 11220 22919 168 20739 697 1451 205 1186 145 112 546 862 8234 198 239 205 286 1863 422 8276 205 450 1863 137 8203 205 493 1863 131 5555 883 11675 615 131 1767 121 382 422 1422 137 589 692 2732 1222 205 969 170 205 239 1863 422 7380 205 158 1863 137 9046 205 239 1863 131 5555 883 11675 615 131 1968 579 1214 5423 121 382 422 1422 137 589 692 2732 1222 205 461 241 106 2487 2397 684 2364 467 5555 2505 1767 422 947 8905 2211 137 1968 579 1214 5423 145 118 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] \" i matter , i learn , i decide \" : an impact evaluation on knowledge , attitudes , and rights to prevent adolescent pregnancy [SEP] adolescent pregnancy is considered a priority public health issue because of its implications in the lives of young mothers , their children , and the well - being of the general population . in this paper , we describe an intervention targeting adolescents ( aged 11 - 19 years old ) in a rural context and estimate its impact on key outcomes relevant to early pregnancy prevention : knowledge and self - efficacy concerning sexual and reproductive health , knowledge of sexual and reproductive rights , and attitudes toward gender roles . our study used a quasi - experimental design comprising 74 ##7 adolescents . three difference - in - differences models ( raw , adjusted , and by exposure level ) with fixed effects estimated the changes in all outcome measures . our results showed that the intervention community had a significant improvement in all outcomes , and this improvement was larger in those who received the highest - exposure level of intervention compared to a control community . our study provides evidence that a community - based intervention , founded on comprehensive sexual education , is a promising approach to improve key outcomes related to early pregnancy in rural contexts . further research should be undertaken to test how similar strategies focusing on multi - layer early pregnancy determinants work on other sub - groups of [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] \" i matter , i learn , i decide \" : an impact evaluation on knowledge , attitudes , and rights to prevent adolescent pregnancy [SEP] adolescent pregnancy is considered a priority public health issue because of its implications in the lives of young mothers , their children , and the well - being of the general population . in this paper , we describe an intervention targeting adolescents ( aged 11 - 19 years old ) in a rural context and estimate its impact on key outcomes relevant to early pregnancy prevention : knowledge and self - efficacy concerning sexual and reproductive health , knowledge of sexual and reproductive rights , and attitudes toward gender roles . our study used a quasi - experimental design comprising 74 ##7 adolescents . three difference - in - differences models ( raw , adjusted , and by exposure level ) with fixed effects estimated the changes in all outcome measures . our results showed that the intervention community had a significant improvement in all outcomes , and this improvement was larger in those who received the highest - exposure level of intervention compared to a control community . our study provides evidence that a community - based intervention , founded on comprehensive sexual education , is a promising approach to improve key outcomes related to early pregnancy in rural contexts . further research should be undertaken to test how similar strategies focusing on multi - layer early pregnancy determinants work on other sub - groups of [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 1554 259 4067 422 259 6714 422 259 10130 1554 862 130 2141 2166 191 1767 422 8826 422 137 4040 147 3363 11644 5564 103 11644 5564 165 1574 106 7531 1771 947 3060 923 131 633 5214 121 111 11937 131 3182 8659 422 547 1808 422 137 111 804 579 1558 131 111 1196 1638 205 121 238 1203 422 185 3401 130 3832 6947 8088 145 6485 1021 579 371 1320 4289 546 121 106 7046 2220 137 2812 633 2141 191 1519 2952 2884 147 1926 5564 5200 862 1767 137 1968 579 4684 6275 5471 137 8042 947 422 1767 131 5471 137 8042 4040 422 137 8826 2536 4703 5370 205 580 527 501 106 8889 579 1798 899 12201 7667 30145 8088 205 874 1673 579 121 579 1595 1262 145 6908 422 5731 422 137 214 2718 615 546 190 2612 1056 2595 111 1334 121 355 3095 2554 205 580 545 1367 198 111 3832 2928 883 106 684 3523 121 355 2952 422 137 238 3523 241 2331 121 1052 975 2072 111 3435 579 2718 615 131 3832 1031 147 106 602 2928 205 580 527 2315 1775 198 106 2928 579 791 3832 422 24756 191 6124 5471 2870 422 165 106 6704 1139 147 1658 1519 2952 1482 147 1926 5564 121 7046 8653 205 911 849 1055 195 11058 147 856 539 868 3236 8808 191 869 579 2474 1926 5564 10687 697 191 494 414 579 1302 131 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 1554 259 4067 422 259 6714 422 259 10130 1554 862 130 2141 2166 191 1767 422 8826 422 137 4040 147 3363 11644 5564 103 11644 5564 165 1574 106 7531 1771 947 3060 923 131 633 5214 121 111 11937 131 3182 8659 422 547 1808 422 137 111 804 579 1558 131 111 1196 1638 205 121 238 1203 422 185 3401 130 3832 6947 8088 145 6485 1021 579 371 1320 4289 546 121 106 7046 2220 137 2812 633 2141 191 1519 2952 2884 147 1926 5564 5200 862 1767 137 1968 579 4684 6275 5471 137 8042 947 422 1767 131 5471 137 8042 4040 422 137 8826 2536 4703 5370 205 580 527 501 106 8889 579 1798 899 12201 7667 30145 8088 205 874 1673 579 121 579 1595 1262 145 6908 422 5731 422 137 214 2718 615 546 190 2612 1056 2595 111 1334 121 355 3095 2554 205 580 545 1367 198 111 3832 2928 883 106 684 3523 121 355 2952 422 137 238 3523 241 2331 121 1052 975 2072 111 3435 579 2718 615 131 3832 1031 147 106 602 2928 205 580 527 2315 1775 198 106 2928 579 791 3832 422 24756 191 6124 5471 2870 422 165 106 6704 1139 147 1658 1519 2952 1482 147 1926 5564 121 7046 8653 205 911 849 1055 195 11058 147 856 539 868 3236 8808 191 869 579 2474 1926 5564 10687 697 191 494 414 579 1302 131 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] effectiveness of the women ' s development team leaders in delivering nutrition education on pulse spr ##out ##ing in southern ethiopia [SEP] effectively implemented nutrition education can provide participants with the knowledge and skills to make healthy food choices in the context of their lifestyle ##s and economic resources . in ethiopia , the government equip ##s health extension workers ( he ##ws ) to provide nutrition education to communities by enabling he ##ws to transfer knowledge to women ' s development team leaders ( wd ##tl ##s ) who in turn share the knowledge with the one - to - five network leaders ( 1 - 5 ##n ##wl ) and members . the objective of this study was to examine the effectiveness of wd ##tl ##s in delivering nutrition education to women as the intervention group ( ig ) . this was compared to having trained he ##ws educ ##ate women directly ( the positive control group , pc ##g ) , and having women receive no specific education ( negative control group , nc ##g ) . a cluster randomized trial design was used . three ke ##bel ##es ( villages of 5000 people ) were pur ##pos ##ive ##ly selected from which the wd ##tl ##s were randomly selected and their respective 1 - 5 ##n ##wl and members were participants . nutrition education to teach pulse spr ##out ##ing was provided every other week for 6 months to intervention and positive control groups . focus group discussions and demonstration [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] effectiveness of the women ' s development team leaders in delivering nutrition education on pulse spr ##out ##ing in southern ethiopia [SEP] effectively implemented nutrition education can provide participants with the knowledge and skills to make healthy food choices in the context of their lifestyle ##s and economic resources . in ethiopia , the government equip ##s health extension workers ( he ##ws ) to provide nutrition education to communities by enabling he ##ws to transfer knowledge to women ' s development team leaders ( wd ##tl ##s ) who in turn share the knowledge with the one - to - five network leaders ( 1 - 5 ##n ##wl ) and members . the objective of this study was to examine the effectiveness of wd ##tl ##s in delivering nutrition education to women as the intervention group ( ig ) . this was compared to having trained he ##ws educ ##ate women directly ( the positive control group , pc ##g ) , and having women receive no specific education ( negative control group , nc ##g ) . a cluster randomized trial design was used . three ke ##bel ##es ( villages of 5000 people ) were pur ##pos ##ive ##ly selected from which the wd ##tl ##s were randomly selected and their respective 1 - 5 ##n ##wl and members were participants . nutrition education to teach pulse spr ##out ##ing was provided every other week for 6 months to intervention and positive control groups . focus group discussions and demonstration [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 4826 131 111 2007 2505 112 1120 4832 13189 121 17934 8556 2870 191 4989 8274 521 140 121 7663 25822 103 5419 3812 8556 2870 300 1584 1914 190 111 1767 137 5561 147 2113 4093 2599 7938 121 111 2220 131 547 13379 30113 137 3587 2965 205 121 25822 422 111 4270 6064 30113 947 3840 5555 145 299 4563 546 147 1584 8556 2870 147 5904 214 10619 299 4563 147 2268 1767 147 2007 2505 112 1120 4832 13189 145 19456 6687 30113 546 975 121 3216 4456 111 1767 190 111 482 579 147 579 2539 934 13189 145 158 579 305 30111 18367 546 137 3087 205 111 3201 131 238 527 241 147 4423 111 4826 131 19456 6687 30113 121 17934 8556 2870 147 2007 188 111 3832 583 145 2648 546 205 238 241 1031 147 2773 7222 299 4563 2138 217 2007 2533 145 111 1532 602 583 422 3658 30123 546 422 137 2773 2007 5380 425 1154 2870 145 1980 602 583 422 4996 30123 546 205 106 3306 5460 3303 899 241 501 205 874 1059 8740 123 145 23070 131 17088 2325 546 267 1434 451 1090 179 2350 263 334 111 19456 6687 30113 267 5007 2350 137 547 6284 158 579 305 30111 18367 137 3087 267 1914 205 8556 2870 147 19015 4989 8274 521 140 241 1966 1795 494 4082 168 370 2499 147 3832 137 1532 602 1302 205 1790 583 8020 137 12415 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 4826 131 111 2007 2505 112 1120 4832 13189 121 17934 8556 2870 191 4989 8274 521 140 121 7663 25822 103 5419 3812 8556 2870 300 1584 1914 190 111 1767 137 5561 147 2113 4093 2599 7938 121 111 2220 131 547 13379 30113 137 3587 2965 205 121 25822 422 111 4270 6064 30113 947 3840 5555 145 299 4563 546 147 1584 8556 2870 147 5904 214 10619 299 4563 147 2268 1767 147 2007 2505 112 1120 4832 13189 145 19456 6687 30113 546 975 121 3216 4456 111 1767 190 111 482 579 147 579 2539 934 13189 145 158 579 305 30111 18367 546 137 3087 205 111 3201 131 238 527 241 147 4423 111 4826 131 19456 6687 30113 121 17934 8556 2870 147 2007 188 111 3832 583 145 2648 546 205 238 241 1031 147 2773 7222 299 4563 2138 217 2007 2533 145 111 1532 602 583 422 3658 30123 546 422 137 2773 2007 5380 425 1154 2870 145 1980 602 583 422 4996 30123 546 205 106 3306 5460 3303 899 241 501 205 874 1059 8740 123 145 23070 131 17088 2325 546 267 1434 451 1090 179 2350 263 334 111 19456 6687 30113 267 5007 2350 137 547 6284 158 579 305 30111 18367 137 3087 267 1914 205 8556 2870 147 19015 4989 8274 521 140 241 1966 1795 494 4082 168 370 2499 147 3832 137 1532 602 1302 205 1790 583 8020 137 12415 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] impact of a growth enhancement support scheme on coc ##oa yield and income of coc ##oa farmers in os ##un state , nigeria [SEP] background . in an attempt to improve the yield of coc ##oa and farmers income , the federal government of nigeria in 2012 introduced the coc ##oa growth enhancement support ( ges ) scheme that subsid ##ized farm inputs to farmers . this article examines the effects of the scheme on coc ##oa yield and the income of coc ##oa farmers in os ##un state . material and methods . a multi ##stage sampling procedure was used to obtain data from 208 coc ##oa farmers of whom there were 100 participants and 108 non - participants of the scheme . data collected were analyzed using descriptive statistics , the binary logit regression model and the propensity score matching ( ps ##m ) model . results . descriptive statistics revealed no mean difference between some socioeconomic characteristics among the categories of farmers in the study area such as household size , farming experience , age and education . the results further revealed that participation in previous government intervention programs , access to extension services and access to credit were significant determinants of participation in the ges scheme . participation in the ges scheme increased coc ##oa yield and income of coc ##oa farmers by 42 . 30 kg . ha - 1 and 245 ##53 . 99 n . ha - 1 ( 59 . 71 a [UNK] ¬ . ha [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] impact of a growth enhancement support scheme on coc ##oa yield and income of coc ##oa farmers in os ##un state , nigeria [SEP] background . in an attempt to improve the yield of coc ##oa and farmers income , the federal government of nigeria in 2012 introduced the coc ##oa growth enhancement support ( ges ) scheme that subsid ##ized farm inputs to farmers . this article examines the effects of the scheme on coc ##oa yield and the income of coc ##oa farmers in os ##un state . material and methods . a multi ##stage sampling procedure was used to obtain data from 208 coc ##oa farmers of whom there were 100 participants and 108 non - participants of the scheme . data collected were analyzed using descriptive statistics , the binary logit regression model and the propensity score matching ( ps ##m ) model . results . descriptive statistics revealed no mean difference between some socioeconomic characteristics among the categories of farmers in the study area such as household size , farming experience , age and education . the results further revealed that participation in previous government intervention programs , access to extension services and access to credit were significant determinants of participation in the ges scheme . participation in the ges scheme increased coc ##oa yield and income of coc ##oa farmers by 42 . 30 kg . ha - 1 and 245 ##53 . 99 n . ha - 1 ( 59 . 71 a [UNK] ¬ . ha [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 2141 131 106 1503 6586 1385 2631 191 7864 8212 2210 137 5021 131 7864 8212 12288 121 3581 164 1098 422 17964 103 2740 205 121 130 5809 147 1658 111 2210 131 7864 8212 137 12288 5021 422 111 8012 4270 131 17964 121 4950 3376 111 7864 8212 1503 6586 1385 145 6194 546 2631 198 16263 645 5947 5671 147 12288 205 238 2148 15817 111 1056 131 111 2631 191 7864 8212 2210 137 111 5021 131 7864 8212 12288 121 3581 164 1098 205 1440 137 1045 205 106 869 7743 3597 2272 241 501 147 831 453 263 20645 7864 8212 12288 131 7861 461 267 1287 1914 137 11008 699 579 1914 131 111 2631 205 453 2760 267 2549 487 10363 4530 422 111 5067 25565 3089 437 137 111 15310 2867 4740 145 1409 30119 546 437 205 545 205 10363 4530 2861 425 1108 1673 467 693 13044 2087 1247 111 4468 131 12288 121 111 527 1590 555 188 5430 1243 422 19323 2899 422 1407 137 2870 205 111 545 911 2861 198 6163 121 1061 4270 3832 3996 422 1899 147 3840 2522 137 1899 147 8388 267 684 10687 131 6163 121 111 6194 2631 205 6163 121 111 6194 2631 1175 7864 8212 2210 137 5021 131 7864 8212 12288 214 4637 205 1339 5036 205 325 579 158 137 23427 4262 205 5970 146 205 325 579 158 145 5275 205 8621 106 101 12033 205 325 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 2141 131 106 1503 6586 1385 2631 191 7864 8212 2210 137 5021 131 7864 8212 12288 121 3581 164 1098 422 17964 103 2740 205 121 130 5809 147 1658 111 2210 131 7864 8212 137 12288 5021 422 111 8012 4270 131 17964 121 4950 3376 111 7864 8212 1503 6586 1385 145 6194 546 2631 198 16263 645 5947 5671 147 12288 205 238 2148 15817 111 1056 131 111 2631 191 7864 8212 2210 137 111 5021 131 7864 8212 12288 121 3581 164 1098 205 1440 137 1045 205 106 869 7743 3597 2272 241 501 147 831 453 263 20645 7864 8212 12288 131 7861 461 267 1287 1914 137 11008 699 579 1914 131 111 2631 205 453 2760 267 2549 487 10363 4530 422 111 5067 25565 3089 437 137 111 15310 2867 4740 145 1409 30119 546 437 205 545 205 10363 4530 2861 425 1108 1673 467 693 13044 2087 1247 111 4468 131 12288 121 111 527 1590 555 188 5430 1243 422 19323 2899 422 1407 137 2870 205 111 545 911 2861 198 6163 121 1061 4270 3832 3996 422 1899 147 3840 2522 137 1899 147 8388 267 684 10687 131 6163 121 111 6194 2631 205 6163 121 111 6194 2631 1175 7864 8212 2210 137 5021 131 7864 8212 12288 214 4637 205 1339 5036 205 325 579 158 137 23427 4262 205 5970 146 205 325 579 158 145 5275 205 8621 106 101 12033 205 325 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] first - line tuberculosis treatment with double - dose rifamp ##icin is well tolerated . [SEP] objective : to compare the occurrence of unf ##avour ##able treatment and safety outcomes of double - dose rifamp ##icin rm ##p ; 20 mg / kg / d , intervention with standard dose 10 mg / kg / d , control in a first - line tuberculosis tb treatment regimen for smear - positive tb patients in bangladesh . design : this was a randomised clinical trial . the primary efficacy and safety endpoints were the occurrence of an unf ##avour ##able treatment outcome death , failure , relapse or loss to follow - up and the occurrence of any serious drug - related adverse event sae . results : in primary efficacy analysis , among 34 ##3 control and 34 ##7 intervention patients , respectively 15 . 5 % and 11 . 8 % had an unf ##avour ##able outcome . in safety analysis , among 34 ##9 intervention and 35 ##2 control patients , respectively 4 . 3 % and 2 . 6 % experienced an sae . these differences were not significant . there was a significantly lower occurrence of sae ##s , explained by a lower occurrence of hepatic toxicity , in a rm ##p double - dose ##d but erroneous ##ly hz ##e iso ##nia ##zid + pyr ##azin ##amide + eth ##amb ##uto ##l under - dose ##d subgroup . conclusions : our findings show that there is no statistically significant [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] first - line tuberculosis treatment with double - dose rifamp ##icin is well tolerated . [SEP] objective : to compare the occurrence of unf ##avour ##able treatment and safety outcomes of double - dose rifamp ##icin rm ##p ; 20 mg / kg / d , intervention with standard dose 10 mg / kg / d , control in a first - line tuberculosis tb treatment regimen for smear - positive tb patients in bangladesh . design : this was a randomised clinical trial . the primary efficacy and safety endpoints were the occurrence of an unf ##avour ##able treatment outcome death , failure , relapse or loss to follow - up and the occurrence of any serious drug - related adverse event sae . results : in primary efficacy analysis , among 34 ##3 control and 34 ##7 intervention patients , respectively 15 . 5 % and 11 . 8 % had an unf ##avour ##able outcome . in safety analysis , among 34 ##9 intervention and 35 ##2 control patients , respectively 4 . 3 % and 2 . 6 % experienced an sae . these differences were not significant . there was a significantly lower occurrence of sae ##s , explained by a lower occurrence of hepatic toxicity , in a rm ##p double - dose ##d but erroneous ##ly hz ##e iso ##nia ##zid + pyr ##azin ##amide + eth ##amb ##uto ##l under - dose ##d subgroup . conclusions : our findings show that there is no statistically significant [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 705 579 972 9757 922 190 3917 579 2750 26994 11274 165 804 16539 205 103 3201 862 147 3745 111 5836 131 11490 28907 318 922 137 4104 2952 131 3917 579 2750 26994 11274 4842 30121 1814 1012 1529 1352 5036 1352 128 422 3832 190 1235 2750 566 1529 1352 5036 1352 128 422 602 121 106 705 579 972 9757 6226 922 11685 168 24333 579 1532 6226 568 121 23547 205 899 862 238 241 106 14136 1329 3303 205 111 1916 4684 137 4104 15753 267 111 5836 131 130 11490 28907 318 922 3095 2889 422 3018 422 12269 234 1738 147 589 579 692 137 111 5836 131 843 6083 1698 579 1482 5386 2607 29647 205 545 862 121 1916 4684 669 422 1247 3154 30138 602 137 3154 30145 3832 568 422 1222 884 205 305 1863 137 1021 205 493 1863 883 130 11490 28907 318 3095 205 121 4104 669 422 1247 3154 30141 3832 137 2638 30132 602 568 422 1222 286 205 239 1863 137 170 205 370 1863 5770 130 29647 205 407 1595 267 302 684 205 461 241 106 1357 1268 5836 131 29647 30113 422 4458 214 106 1268 5836 131 7221 6006 422 121 106 4842 30121 3917 579 2750 30118 563 22048 179 3854 30107 5682 11404 26197 473 6204 28498 5659 473 2311 6750 19663 30115 604 579 2750 30118 5909 205 3078 862 580 2116 405 198 461 165 425 4073 684 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 705 579 972 9757 922 190 3917 579 2750 26994 11274 165 804 16539 205 103 3201 862 147 3745 111 5836 131 11490 28907 318 922 137 4104 2952 131 3917 579 2750 26994 11274 4842 30121 1814 1012 1529 1352 5036 1352 128 422 3832 190 1235 2750 566 1529 1352 5036 1352 128 422 602 121 106 705 579 972 9757 6226 922 11685 168 24333 579 1532 6226 568 121 23547 205 899 862 238 241 106 14136 1329 3303 205 111 1916 4684 137 4104 15753 267 111 5836 131 130 11490 28907 318 922 3095 2889 422 3018 422 12269 234 1738 147 589 579 692 137 111 5836 131 843 6083 1698 579 1482 5386 2607 29647 205 545 862 121 1916 4684 669 422 1247 3154 30138 602 137 3154 30145 3832 568 422 1222 884 205 305 1863 137 1021 205 493 1863 883 130 11490 28907 318 3095 205 121 4104 669 422 1247 3154 30141 3832 137 2638 30132 602 568 422 1222 286 205 239 1863 137 170 205 370 1863 5770 130 29647 205 407 1595 267 302 684 205 461 241 106 1357 1268 5836 131 29647 30113 422 4458 214 106 1268 5836 131 7221 6006 422 121 106 4842 30121 3917 579 2750 30118 563 22048 179 3854 30107 5682 11404 26197 473 6204 28498 5659 473 2311 6750 19663 30115 604 579 2750 30118 5909 205 3078 862 580 2116 405 198 461 165 425 4073 684 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Maryia_Ivanina\\Anaconda3\\envs\\cornell\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Maryia_Ivanina\\Anaconda3\\envs\\cornell\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used for model gpu 0\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Maryia_Ivanina\\Anaconda3\\envs\\cornell\\lib\\site-packages\\bert\\modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Maryia_Ivanina\\Anaconda3\\envs\\cornell\\lib\\site-packages\\bert\\modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Maryia_Ivanina\\Anaconda3\\envs\\cornell\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Maryia_Ivanina\\Anaconda3\\envs\\cornell\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../model/study_type_multi_new_data_with_keywords_agg_more_sub_groups_more_data_1\\model.ckpt-6831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../model/study_type_multi_new_data_with_keywords_agg_more_sub_groups_more_data_1\\model.ckpt-6831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-67f0f536eb2a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m             {\"column_filler_class\": \"StudyTypeLabeller\", \"folder\": \"../model/study_type_multi_new_data_with_keywords_agg_more_sub_groups_more_data_1\",\n\u001b[0;32m      6\u001b[0m              \u001b[1;34m\"meta_folder\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"../model/study_type_multi_meta_new_data_with_keywords_agg_more_sub_groups_more_data_1\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m              \"scibert_model_folder\": \"../model/scibert_scivocab_uncased\", \"model_with_agg\": True},\n\u001b[0m\u001b[0;32m      8\u001b[0m         ]})\n",
      "\u001b[1;32m~\\cornell_project\\discovery\\src\\text_processing\\all_column_filler.py\u001b[0m in \u001b[0;36mfill_columns_for_df\u001b[1;34m(self, articles_df, search_engine_inverted_index, _abbreviations_resolver, settings_filename, settings_json)\u001b[0m\n\u001b[0;32m    243\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 \u001b[0mcolumn_logger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_status_logger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumn_info\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 245\u001b[1;33m                 \u001b[0marticles_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumn_classes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"column_filler_class\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticles_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearch_engine_inverted_index\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_abbreviations_resolver\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolumn_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn_logger\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcolumn_logger\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m                     \u001b[0mcolumn_logger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_status_for_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Finished\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\cornell_project\\discovery\\src\\text_processing\\all_column_filler.py\u001b[0m in \u001b[0;36mfill_study_type\u001b[1;34m(self, articles_df, search_engine_inverted_index, _abbreviations_resolver, column_info, status_logger)\u001b[0m\n\u001b[0;32m    184\u001b[0m          \u001b[0mmeta_folder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"study_type_multi_meta_agg\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;34m\"meta_folder\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcolumn_info\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mcolumn_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"meta_folder\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m          \u001b[0mscibert_model_folder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"../tmp/scibert_scivocab_uncased\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;34m\"scibert_model_folder\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcolumn_info\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mcolumn_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"scibert_model_folder\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m          model_with_agg=True if \"model_with_agg\" not in column_info else column_info[\"model_with_agg\"])\n\u001b[0m\u001b[0;32m    187\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0marticles_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\cornell_project\\discovery\\src\\study_design_type\\full_logic_study_type_labeler.py\u001b[0m in \u001b[0;36mlabel_df_with_study_type\u001b[1;34m(self, articles_df, search_engine_inverted_index, use_prediction, gpu_device_num, folder, meta_folder, scibert_model_folder, file_with_keywords, model_with_agg)\u001b[0m\n\u001b[0;32m    253\u001b[0m                   )\n\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_study_type_labeler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_with_meta_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticles_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"title\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"abstract\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwith_head_tail\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudy_type_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\cornell_project\\discovery\\src\\bert_models\\base_bert_model_meta_model_with_window_all.py\u001b[0m in \u001b[0;36mpredict_with_meta_model\u001b[1;34m(self, df, with_head_tail, recreate_model)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict_with_meta_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwith_head_tail\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecreate_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_dataset_for_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_tail\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwith_head_tail\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmeta_model_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'saved_model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[0mpredict_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\cornell_project\\discovery\\src\\bert_models\\base_bert_model_meta_model_with_window_all.py\u001b[0m in \u001b[0;36mprepare_dataset_for_train\u001b[1;34m(self, train, use_tail)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mprepare_dataset_for_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_tail\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mres_prob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_layers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_for_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_head\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwith_output_layer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[0mres_prob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_layers_agg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_for_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_head\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwith_output_layer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_agg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0muse_tail\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\cornell_project\\discovery\\src\\bert_models\\base_bert_model.py\u001b[0m in \u001b[0;36mpredict_for_df\u001b[1;34m(self, train, is_head, with_output_layer, window_agg)\u001b[0m\n\u001b[0;32m    597\u001b[0m             \u001b[0mtrain_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel_column\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel_column\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m         res_train = self.getPredictions(train_vals, get_probabilities = True,\n\u001b[1;32m--> 599\u001b[1;33m             is_head = is_head, with_output_layer=with_output_layer, window_agg=window_agg)\n\u001b[0m\u001b[0;32m    600\u001b[0m         \u001b[0mres_train_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mres_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m         \u001b[0mres_train_full\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mres_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\cornell_project\\discovery\\src\\bert_models\\base_bert_model_meta_model_with_window_all.py\u001b[0m in \u001b[0;36mgetPredictions\u001b[1;34m(self, in_sentences, get_probabilities, is_head, with_output_layer, window_agg)\u001b[0m\n\u001b[0;32m    130\u001b[0m                 seq_length=self.max_seq_length , is_training=False, drop_remainder=False)\n\u001b[0;32m    131\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict_input_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_final_predictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwith_output_layer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_probabilities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m             \u001b[0mindex2article\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\cornell_project\\discovery\\src\\bert_models\\base_bert_model.py\u001b[0m in \u001b[0;36mprepare_final_predictions\u001b[1;34m(self, predictions, with_output_layer, get_probabilities)\u001b[0m\n\u001b[0;32m    630\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mprepare_final_predictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwith_output_layer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_probabilities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 632\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mprediction\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    633\u001b[0m           \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m           \u001b[0mprobabilities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cornell\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, input_fn, predict_keys, hooks, checkpoint_path, yield_single_examples)\u001b[0m\n\u001b[0;32m    627\u001b[0m             hooks=all_hooks) as mon_sess:\n\u001b[0;32m    628\u001b[0m           \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 629\u001b[1;33m             \u001b[0mpreds_evaluated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    630\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0myield_single_examples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m               \u001b[1;32myield\u001b[0m \u001b[0mpreds_evaluated\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cornell\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    674\u001b[0m                           \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m                           \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m                           run_metadata=run_metadata)\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cornell\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1169\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1170\u001b[0m                               \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1171\u001b[1;33m                               run_metadata=run_metadata)\n\u001b[0m\u001b[0;32m   1172\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1173\u001b[0m         logging.info('An error was raised. This may be due to a preemption in '\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cornell\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1253\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1254\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1255\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1256\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1257\u001b[0m       \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cornell\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1325\u001b[0m                                   \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m                                   \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m                                   run_metadata=run_metadata)\n\u001b[0m\u001b[0;32m   1328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cornell\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1089\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1090\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1091\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1092\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1093\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_with_hooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cornell\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cornell\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cornell\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cornell\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cornell\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cornell\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "_all_column_filler = all_column_filler.AllColumnFiller()\n",
    "df = _all_column_filler.fill_columns_for_df(\n",
    "        df, search_engine_inverted_index, _abbreviations_resolver, settings_json = {\"columns\":[\n",
    "            #{\"column_filler_class\":\"GeoNameFinder\", \"world_bank_regions_file\": \"../data/IncomeLevelDivisionCountries.xlsx\"},\n",
    "            {\"column_filler_class\": \"StudyTypeLabeller\", \"folder\": \"../model/study_type_multi_new_data_with_keywords_agg_more_sub_groups_more_data_1\",\n",
    "             \"meta_folder\": \"../model/study_type_multi_meta_new_data_with_keywords_agg_more_sub_groups_more_data_1\",\n",
    "             \"scibert_model_folder\": \"../model/scibert_scivocab_uncased\", \"model_with_agg\": True},\n",
    "        ]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1f407ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n",
      "Saved to ../tmp/combined_data_processed_new.xlsx\n"
     ]
    }
   ],
   "source": [
    "excel_writer.ExcelWriter().save_df_in_excel(df, \"../tmp/combined_data_processed_new.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee60cf5",
   "metadata": {},
   "source": [
    "# Lab studies evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "211b6ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_study_true_vals = []\n",
    "lab_study_true_pred = []\n",
    "for i in range(len(df)):\n",
    "    if df[\"File\"].values[i] == \"Lab studies\":\n",
    "        lab_study_true_vals.append(1)\n",
    "    else:\n",
    "        lab_study_true_vals.append(0)\n",
    "    if \"Laboratory study\" in df[\"study_type\"].values[i]:\n",
    "        lab_study_true_pred.append(1)\n",
    "    else:\n",
    "        lab_study_true_pred.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "456a010b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[389  11]\n",
      " [ 74  26]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.97      0.90       400\n",
      "           1       0.70      0.26      0.38       100\n",
      "\n",
      "   micro avg       0.83      0.83      0.83       500\n",
      "   macro avg       0.77      0.62      0.64       500\n",
      "weighted avg       0.81      0.83      0.80       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(confusion_matrix(lab_study_true_vals, lab_study_true_pred))  \n",
    "print(classification_report(lab_study_true_vals, lab_study_true_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6263b0",
   "metadata": {},
   "source": [
    "# High income countries evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75e03863",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_locations_info_true_vals = []\n",
    "geo_locations_info_pred = []\n",
    "for i in range(len(df)):\n",
    "    if df[\"File\"].values[i] == 'Lab studies':\n",
    "        continue\n",
    "    if df[\"File\"].values[i] == 'High-income countries':\n",
    "        geo_locations_info_true_vals.append(1)\n",
    "    else:\n",
    "        geo_locations_info_true_vals.append(0)\n",
    "    if 'Low-income countries(995$ or less)' in df[\"world_bankdivision_regions\"].values[i] or 'Middle-income countries($996 to $3,895)' in df[\"world_bankdivision_regions\"].values[i] or \"Upper-middle-income countries ($3,896 to $12,055)\" in df[\"world_bankdivision_regions\"].values[i]:\n",
    "        geo_locations_info_pred.append(0)\n",
    "    elif len(df[\"world_bankdivision_regions\"].values[i]) == 0:\n",
    "        geo_locations_info_pred.append(0)\n",
    "    else:\n",
    "        geo_locations_info_pred.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a777805",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_locations_info_true_vals = []\n",
    "geo_locations_info_pred = []\n",
    "for i in range(len(df)):\n",
    "    if df[\"File\"].values[i] == 'Lab studies':\n",
    "        continue\n",
    "    if df[\"File\"].values[i] == 'High-income countries':\n",
    "        geo_locations_info_true_vals.append(1)\n",
    "    else:\n",
    "        if \"LMICs\" not in df[\"world_bankdivision_regions\"].values[i] and \"Transitional countries\" in df[\"world_bankdivision_regions\"].values[i]:\n",
    "            geo_locations_info_true_vals.append(1)\n",
    "        else:\n",
    "            geo_locations_info_true_vals.append(0)\n",
    "        #geo_locations_info_true_vals.append(0)\n",
    "    if \"LMICs\" in df[\"world_bankdivision_regions\"].values[i]:\n",
    "        geo_locations_info_pred.append(0)\n",
    "    elif len(df[\"world_bankdivision_regions\"].values[i]) == 0:\n",
    "        geo_locations_info_pred.append(0)\n",
    "    else:\n",
    "        geo_locations_info_pred.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93dfadcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[279  21]\n",
      " [ 23  77]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.93       300\n",
      "           1       0.79      0.77      0.78       100\n",
      "\n",
      "   micro avg       0.89      0.89      0.89       400\n",
      "   macro avg       0.85      0.85      0.85       400\n",
      "weighted avg       0.89      0.89      0.89       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#baseline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(confusion_matrix(geo_locations_info_true_vals, geo_locations_info_pred))\n",
    "print(classification_report(geo_locations_info_true_vals, geo_locations_info_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64bd37d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[279   6]\n",
      " [ 23  92]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.98      0.95       285\n",
      "           1       0.94      0.80      0.86       115\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       400\n",
      "   macro avg       0.93      0.89      0.91       400\n",
      "weighted avg       0.93      0.93      0.93       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# recalculated with labels changed for transitional countries not excluded in the dataset\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(confusion_matrix(geo_locations_info_true_vals, geo_locations_info_pred))\n",
    "print(classification_report(geo_locations_info_true_vals, geo_locations_info_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8eecf54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_locations_info_true_vals = []\n",
    "geo_locations_info_pred = []\n",
    "for i in range(len(df)):\n",
    "    if df[\"File\"].values[i] != 'Included (abstract)':\n",
    "        continue\n",
    "    if df[\"File\"].values[i] == 'High-income countries':\n",
    "        geo_locations_info_true_vals.append(1)\n",
    "    else:\n",
    "        geo_locations_info_true_vals.append(0)\n",
    "    if 'Low-income countries(995$ or less)' in df[\"world_bankdivision_regions\"].values[i] or 'Middle-income countries($996 to $3,895)' in df[\"world_bankdivision_regions\"].values[i] or \"Upper-middle-income countries ($3,896 to $12,055)\" in df[\"world_bankdivision_regions\"].values[i]:\n",
    "        geo_locations_info_pred.append(0)\n",
    "    elif len(df[\"world_bankdivision_regions\"].values[i]) == 0:\n",
    "        geo_locations_info_pred.append(0)\n",
    "    else:\n",
    "        geo_locations_info_pred.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0da8d25b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(geo_locations_info_true_vals, geo_locations_info_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a04985e",
   "metadata": {},
   "source": [
    "# Geo names extraction accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad1310a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read file ../tmp/labelled_geo_names.xlsx: 0.07s\n",
      "Processed file ../tmp/labelled_geo_names.xlsx: 0.08s\n"
     ]
    }
   ],
   "source": [
    "geo_names_df = excel_reader.ExcelReader().read_df_from_excel(\"../tmp/labelled_geo_names.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a82e9a4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['abstract', 'comments', 'countries_mentioned', 'country_groups',\n",
       "       'districts', 'edit_url', 'geo_regions', 'keywords', 'labelled',\n",
       "       'provinces', 'responsible_person', 'tags', 'title'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo_names_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb599741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labelled data: 169 All data: 500\n"
     ]
    }
   ],
   "source": [
    "print(\"Labelled data: %d All data: %d\" % (len(geo_names_df[geo_names_df[\"labelled\"] == 1]), len(geo_names_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5777a665",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_names_df_with_labelled = geo_names_df[geo_names_df[\"labelled\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ef26c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_names_df_wo_labelled = geo_names_df_with_labelled[[\"title\",\"abstract\",\"keywords\"]].copy()\n",
    "geo_names_df_wo_labelled[\"identificators\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42cb84bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 articles\n",
      "Processed 168 articles\n",
      "Processed 0 abbreviations\n",
      "Processed 3000 abbreviations\n",
      "Processed 6000 abbreviations\n",
      "Processed 9000 abbreviations\n",
      "Processed 12000 abbreviations\n",
      "Processed 15000 abbreviations\n",
      "Processed 18000 abbreviations\n",
      "Processed 21000 abbreviations\n",
      "Processed 24000 abbreviations\n",
      "Processed 27000 abbreviations\n",
      "Processed 27129 abbreviations\n"
     ]
    }
   ],
   "source": [
    "from text_processing import search_engine_insensitive_to_spelling\n",
    "from text_processing import all_column_filler\n",
    "search_engine_inverted_index_geo_names = search_engine_insensitive_to_spelling.SearchEngineInsensitiveToSpelling(\n",
    "        load_abbreviations = True)\n",
    "search_engine_inverted_index_geo_names.create_inverted_index(geo_names_df_wo_labelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e4fc075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started processing  {'column_filler_class': 'GeoNameFinder', 'world_bank_regions_file': '../data/IncomeLevelDivisionCountries.xlsx'}\n",
      "Read file ../data/GeoRegions.xlsx: 0.02s\n",
      "Processed file ../data/GeoRegions.xlsx: 0.00s\n",
      "Read file ../data/map_additional_regions.xlsx: 0.01s\n",
      "Processed file ../data/map_additional_regions.xlsx: 0.00s\n",
      "Read file ../data/IncomeLevelDivisionCountries.xlsx: 0.01s\n",
      "Processed file ../data/IncomeLevelDivisionCountries.xlsx: 0.00s\n",
      "Read file ../data/map_additional_country_names.xlsx: 0.00s\n",
      "Processed file ../data/map_additional_country_names.xlsx: 0.00s\n",
      "Read file ../data/map_country_adjectives.xlsx: 0.01s\n",
      "Processed file ../data/map_country_adjectives.xlsx: 0.01s\n",
      "Read file ../data/map_currencies.xlsx: 0.01s\n",
      "Processed file ../data/map_currencies.xlsx: 0.00s\n",
      "Read file ../data/map_country_groups.xlsx: 0.01s\n",
      "Processed file ../data/map_country_groups.xlsx: 0.01s\n",
      "Read file ../data/map_country_groups_income_level.xlsx: 0.00s\n",
      "Processed file ../data/map_country_groups_income_level.xlsx: 0.00s\n",
      "Read file ../data/provinces.xlsx: 2.11s\n",
      "Processed file ../data/provinces.xlsx: 1.50s\n",
      "Read file ../data/districts.xlsx: 1.56s\n",
      "Processed file ../data/districts.xlsx: 1.54s\n",
      "Processed 0 items\n",
      "Processed 168 items\n",
      "Processed for 27.361114740371704s\n"
     ]
    }
   ],
   "source": [
    "_all_column_filler = all_column_filler.AllColumnFiller()\n",
    "geo_names_df_wo_labelled = _all_column_filler.fill_columns_for_df(\n",
    "        geo_names_df_wo_labelled, search_engine_inverted_index_geo_names, _abbreviations_resolver, settings_json = {\"columns\":[\n",
    "            {\"column_filler_class\":\"GeoNameFinder\", \"world_bank_regions_file\": \"../data/IncomeLevelDivisionCountries.xlsx\"},\n",
    "        ]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29b11f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column:  countries_mentioned\n",
      "0.9119822485207102\n",
      "Column:  country_groups\n",
      "0.945759368836292\n",
      "Column:  districts\n",
      "0.9644970414201184\n",
      "Column:  geo_regions\n",
      "0.9452662721893491\n",
      "Column:  provinces\n",
      "0.935897435897436\n"
     ]
    }
   ],
   "source": [
    "def get_mean_intersection_over_union(predicted_vals, true_vals):\n",
    "    cnt = 0\n",
    "    cnt_intersection_over_union = 0.0\n",
    "    for pred, true in zip(predicted_vals, true_vals):\n",
    "        cnt += 1\n",
    "        cnt_intersect = len(set(pred).intersection(set(true)))\n",
    "        cnt_union = len(set(pred).union(set(true)))\n",
    "        if cnt_union == 0:\n",
    "            cnt_intersection_over_union += 1\n",
    "        else:\n",
    "            cnt_intersection_over_union += (cnt_intersect/cnt_union)\n",
    "    return cnt_intersection_over_union/cnt if cnt else 0.0\n",
    "\n",
    "for column in ['countries_mentioned', 'country_groups','districts', 'geo_regions', 'provinces']:\n",
    "    print(\"Column: \", column)\n",
    "    print(get_mean_intersection_over_union(geo_names_df_wo_labelled[column].values, geo_names_df_with_labelled[column].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e390a98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_names_df_wo_labelled_restricted_by_title = geo_names_df_wo_labelled.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f51b09bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 articles\n",
      "Processed 168 articles\n",
      "Processed 0 abbreviations\n",
      "Processed 3000 abbreviations\n",
      "Processed 6000 abbreviations\n",
      "Processed 9000 abbreviations\n",
      "Processed 12000 abbreviations\n",
      "Processed 15000 abbreviations\n",
      "Processed 18000 abbreviations\n",
      "Processed 21000 abbreviations\n",
      "Processed 24000 abbreviations\n",
      "Processed 27000 abbreviations\n",
      "Processed 27129 abbreviations\n"
     ]
    }
   ],
   "source": [
    "from text_processing import search_engine_insensitive_to_spelling\n",
    "from text_processing import all_column_filler\n",
    "search_engine_inverted_index_geo_names_title = search_engine_insensitive_to_spelling.SearchEngineInsensitiveToSpelling(\n",
    "        load_abbreviations = True, columns_to_process=[\"title\"])\n",
    "search_engine_inverted_index_geo_names_title.create_inverted_index(geo_names_df_wo_labelled_restricted_by_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "635bd507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started processing  {'column_filler_class': 'GeoNameFinder', 'world_bank_regions_file': '../data/IncomeLevelDivisionCountries.xlsx', 'columns_to_process': ['title'], 'prefix_for_columns': 'title_', 'check_only_country_names_and_divisions': True}\n",
      "Read file ../data/GeoRegions.xlsx: 0.01s\n",
      "Processed file ../data/GeoRegions.xlsx: 0.01s\n",
      "Read file ../data/map_additional_regions.xlsx: 0.01s\n",
      "Processed file ../data/map_additional_regions.xlsx: 0.00s\n",
      "Read file ../data/IncomeLevelDivisionCountries.xlsx: 0.01s\n",
      "Processed file ../data/IncomeLevelDivisionCountries.xlsx: 0.01s\n",
      "Read file ../data/map_additional_country_names.xlsx: 0.01s\n",
      "Processed file ../data/map_additional_country_names.xlsx: 0.00s\n",
      "Read file ../data/map_country_adjectives.xlsx: 0.01s\n",
      "Processed file ../data/map_country_adjectives.xlsx: 0.01s\n",
      "Read file ../data/map_currencies.xlsx: 0.01s\n",
      "Processed file ../data/map_currencies.xlsx: 0.00s\n",
      "Read file ../data/map_country_groups.xlsx: 0.01s\n",
      "Processed file ../data/map_country_groups.xlsx: 0.01s\n",
      "Read file ../data/map_country_groups_income_level.xlsx: 0.01s\n",
      "Processed file ../data/map_country_groups_income_level.xlsx: 0.00s\n",
      "Read file ../data/provinces.xlsx: 1.49s\n",
      "Processed file ../data/provinces.xlsx: 1.44s\n",
      "Read file ../data/districts.xlsx: 1.53s\n",
      "Processed file ../data/districts.xlsx: 1.54s\n",
      "Processed 0 items\n",
      "Processed 168 items\n",
      "Processed for 14.9690682888031s\n"
     ]
    }
   ],
   "source": [
    "_all_column_filler = all_column_filler.AllColumnFiller()\n",
    "geo_names_df_wo_labelled_restricted_by_title = _all_column_filler.fill_columns_for_df(\n",
    "        geo_names_df_wo_labelled_restricted_by_title, search_engine_inverted_index_geo_names_title, _abbreviations_resolver, settings_json = {\"columns\":[\n",
    "            {\"column_filler_class\":\"GeoNameFinder\", \"world_bank_regions_file\": \"../data/IncomeLevelDivisionCountries.xlsx\",\n",
    "            \"columns_to_process\":[\"title\"], \"prefix_for_columns\": \"title_\", \"check_only_country_names_and_divisions\": True},\n",
    "        ]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5ebd4858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>keywords</th>\n",
       "      <th>identificators</th>\n",
       "      <th>countries_mentioned</th>\n",
       "      <th>country_codes</th>\n",
       "      <th>provinces</th>\n",
       "      <th>districts</th>\n",
       "      <th>country_groups</th>\n",
       "      <th>geo_regions</th>\n",
       "      <th>world_bankdivision_regions</th>\n",
       "      <th>title_countries_mentioned</th>\n",
       "      <th>title_country_codes</th>\n",
       "      <th>title_provinces</th>\n",
       "      <th>title_districts</th>\n",
       "      <th>title_country_groups</th>\n",
       "      <th>title_geo_regions</th>\n",
       "      <th>title_world_bankdivision_regions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>The effect of education based on reasoned acti...</td>\n",
       "      <td>Introduction: Studies showed that about 25 per...</td>\n",
       "      <td>analytical techniques;behavior;studies;man;hom...</td>\n",
       "      <td></td>\n",
       "      <td>[Iran]</td>\n",
       "      <td>[IR]</td>\n",
       "      <td>[Iran/Fars]</td>\n",
       "      <td>[Iran/Fars/Sepidan]</td>\n",
       "      <td>[developing countries]</td>\n",
       "      <td>[Middle East and North Africa, Asia]</td>\n",
       "      <td>[LMICs]</td>\n",
       "      <td>[Iran]</td>\n",
       "      <td>[IR]</td>\n",
       "      <td>[Iran/Fars]</td>\n",
       "      <td>[Iran/Fars/Sepidan]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Middle East and North Africa]</td>\n",
       "      <td>[LMICs]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>Motivational interview on having Pap test amon...</td>\n",
       "      <td>Background. Cognitive and mental factors, such...</td>\n",
       "      <td>counseling;fisher exact test;health attitudes;...</td>\n",
       "      <td></td>\n",
       "      <td>[Iran]</td>\n",
       "      <td>[IR]</td>\n",
       "      <td>[Iran/Markazi]</td>\n",
       "      <td>[Iran/Markazi/Shazand]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Middle East and North Africa]</td>\n",
       "      <td>[LMICs]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>Effectiveness of intervention due to feedback ...</td>\n",
       "      <td>Background: The limited supply of red blood ce...</td>\n",
       "      <td>red blood cell transfusion;blood transportatio...</td>\n",
       "      <td></td>\n",
       "      <td>[Iran]</td>\n",
       "      <td>[IR]</td>\n",
       "      <td>[Iran/Razavi Khorasan]</td>\n",
       "      <td>[Iran/Razavi Khorasan/Mashhad]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Middle East and North Africa]</td>\n",
       "      <td>[LMICs]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>Evaluation of the clinical pharmacist role in ...</td>\n",
       "      <td>Background Lower urinary tract symptoms due to...</td>\n",
       "      <td>aged;aged, 80 and over;humans;jordan;*lower ur...</td>\n",
       "      <td></td>\n",
       "      <td>[Jordan]</td>\n",
       "      <td>[JO]</td>\n",
       "      <td>[Jordan/Amman]</td>\n",
       "      <td>[Jordan/Amman/Amman]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Middle East and North Africa]</td>\n",
       "      <td>[LMICs]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>Use of propensity score matching to create cou...</td>\n",
       "      <td>The design of HIV prevention trials in the con...</td>\n",
       "      <td>hiv prevention;vaccine effectiveness;hepatitis...</td>\n",
       "      <td></td>\n",
       "      <td>[Uganda]</td>\n",
       "      <td>[UG]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Sub-Saharan Africa]</td>\n",
       "      <td>[LMICs]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "331  The effect of education based on reasoned acti...   \n",
       "332  Motivational interview on having Pap test amon...   \n",
       "333  Effectiveness of intervention due to feedback ...   \n",
       "334  Evaluation of the clinical pharmacist role in ...   \n",
       "335  Use of propensity score matching to create cou...   \n",
       "\n",
       "                                              abstract  \\\n",
       "331  Introduction: Studies showed that about 25 per...   \n",
       "332  Background. Cognitive and mental factors, such...   \n",
       "333  Background: The limited supply of red blood ce...   \n",
       "334  Background Lower urinary tract symptoms due to...   \n",
       "335  The design of HIV prevention trials in the con...   \n",
       "\n",
       "                                              keywords identificators  \\\n",
       "331  analytical techniques;behavior;studies;man;hom...                  \n",
       "332  counseling;fisher exact test;health attitudes;...                  \n",
       "333  red blood cell transfusion;blood transportatio...                  \n",
       "334  aged;aged, 80 and over;humans;jordan;*lower ur...                  \n",
       "335  hiv prevention;vaccine effectiveness;hepatitis...                  \n",
       "\n",
       "    countries_mentioned country_codes               provinces  \\\n",
       "331              [Iran]          [IR]             [Iran/Fars]   \n",
       "332              [Iran]          [IR]          [Iran/Markazi]   \n",
       "333              [Iran]          [IR]  [Iran/Razavi Khorasan]   \n",
       "334            [Jordan]          [JO]          [Jordan/Amman]   \n",
       "335            [Uganda]          [UG]                      []   \n",
       "\n",
       "                          districts          country_groups  \\\n",
       "331             [Iran/Fars/Sepidan]  [developing countries]   \n",
       "332          [Iran/Markazi/Shazand]                      []   \n",
       "333  [Iran/Razavi Khorasan/Mashhad]                      []   \n",
       "334            [Jordan/Amman/Amman]                      []   \n",
       "335                              []                      []   \n",
       "\n",
       "                              geo_regions world_bankdivision_regions  \\\n",
       "331  [Middle East and North Africa, Asia]                    [LMICs]   \n",
       "332        [Middle East and North Africa]                    [LMICs]   \n",
       "333        [Middle East and North Africa]                    [LMICs]   \n",
       "334        [Middle East and North Africa]                    [LMICs]   \n",
       "335                  [Sub-Saharan Africa]                    [LMICs]   \n",
       "\n",
       "    title_countries_mentioned title_country_codes title_provinces  \\\n",
       "331                    [Iran]                [IR]     [Iran/Fars]   \n",
       "332                        []                  []              []   \n",
       "333                        []                  []              []   \n",
       "334                        []                  []              []   \n",
       "335                        []                  []              []   \n",
       "\n",
       "         title_districts title_country_groups               title_geo_regions  \\\n",
       "331  [Iran/Fars/Sepidan]                   []  [Middle East and North Africa]   \n",
       "332                   []                   []                              []   \n",
       "333                   []                   []                              []   \n",
       "334                   []                   []                              []   \n",
       "335                   []                   []                              []   \n",
       "\n",
       "    title_world_bankdivision_regions  \n",
       "331                          [LMICs]  \n",
       "332                               []  \n",
       "333                               []  \n",
       "334                               []  \n",
       "335                               []  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo_names_df_wo_labelled_restricted_by_title.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9b84d930",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(geo_names_df_wo_labelled_restricted_by_title)):\n",
    "    if geo_names_df_wo_labelled_restricted_by_title[\"title_countries_mentioned\"].values[i]:\n",
    "        geo_names_df_wo_labelled_restricted_by_title[\"countries_mentioned\"].values[i] = geo_names_df_wo_labelled_restricted_by_title[\"title_countries_mentioned\"].values[i]\n",
    "        for column in [\"provinces\", \"districts\"]:\n",
    "            new_array = []\n",
    "            for district in geo_names_df_wo_labelled_restricted_by_title[column].values[i]:\n",
    "                if district.split(\"/\")[0] in geo_names_df_wo_labelled_restricted_by_title[\"countries_mentioned\"].values[i]:\n",
    "                    new_array.append(district)\n",
    "            geo_names_df_wo_labelled_restricted_by_title[column].values[i] = new_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c93eabdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column:  countries_mentioned\n",
      "0.8982448725363706\n",
      "Column:  country_groups\n",
      "0.945759368836292\n",
      "Column:  districts\n",
      "0.9704142011834319\n",
      "Column:  geo_regions\n",
      "0.9452662721893491\n",
      "Column:  provinces\n",
      "0.9437869822485208\n"
     ]
    }
   ],
   "source": [
    "for column in ['countries_mentioned', 'country_groups','districts', 'geo_regions', 'provinces']:\n",
    "    print(\"Column: \", column)\n",
    "    print(get_mean_intersection_over_union(geo_names_df_wo_labelled_restricted_by_title[column].values, geo_names_df_with_labelled[column].values))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
