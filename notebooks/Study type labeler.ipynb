{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hsZvic2YxnTz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maryia_Ivanina\\AppData\\Local\\Continuum\\anaconda3\\envs\\cornell\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Maryia_Ivanina\\AppData\\Local\\Continuum\\anaconda3\\envs\\cornell\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Maryia_Ivanina\\AppData\\Local\\Continuum\\anaconda3\\envs\\cornell\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Maryia_Ivanina\\AppData\\Local\\Continuum\\anaconda3\\envs\\cornell\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Maryia_Ivanina\\AppData\\Local\\Continuum\\anaconda3\\envs\\cornell\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Maryia_Ivanina\\AppData\\Local\\Continuum\\anaconda3\\envs\\cornell\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = False\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import numpy as np\n",
    "import nltk\n",
    "from importlib import reload\n",
    "import os\n",
    "\n",
    "sys.path.append('../src')\n",
    "from study_design_type.study_type_labels import StudyTypeLabels\n",
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization\n",
    "from utilities import excel_reader, excel_writer\n",
    "from sklearn.utils import shuffle\n",
    "#import eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "jviywGyWyKsA",
    "outputId": "166f3005-d219-404f-b201-2a0b75480360"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert-tensorflow in /home/maryia_pavlovets/.conda/envs/maryia_pavlovets_env/lib/python3.7/site-packages (1.0.1)\r\n",
      "Requirement already satisfied: six in /home/maryia_pavlovets/.conda/envs/maryia_pavlovets_env/lib/python3.7/site-packages (from bert-tensorflow) (1.12.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def load_train_study_type(label, proportions_to_use=None):\n",
    "    all_df = pd.DataFrame()\n",
    "    test = None\n",
    "    replicated_df = pd.DataFrame()\n",
    "    for file in os.listdir(\"../tmp/study_type_data\"):\n",
    "        if file == \"final_study_type_test_data.xlsx\":\n",
    "            continue\n",
    "        replicate = 1\n",
    "        if file == \"labelled_multi.xlsx\":\n",
    "            replicate = 4\n",
    "        print(file, replicate)\n",
    "        if file == \"valid_data.xlsx\":\n",
    "            test = excel_reader.ExcelReader().read_df_from_excel(os.path.join(\"../tmp/study_type_data\", file))\n",
    "            test[\"label\"] = 0\n",
    "            for i in range(len(test)):\n",
    "                test[\"label\"].values[i] = 1 if test[label].values[i] >= 0.5 else 0\n",
    "            test = test[[\"title\", \"abstract\", \"label\"]]\n",
    "        if replicate > 1:\n",
    "            for i in range(replicate):\n",
    "                train_1 = excel_reader.ExcelReader().read_df_from_excel(os.path.join(\"../tmp/study_type_data\", file))\n",
    "                train_1[\"label\"] = 0\n",
    "                for i in range(len(train_1)):\n",
    "                    train_1[\"label\"].values[i] = 1 if train_1[label].values[i] >= 0.5 else 0\n",
    "                replicated_df = pd.concat([replicated_df, train_1[[\"title\", \"abstract\", \"label\"]]],axis=0)\n",
    "        else:\n",
    "            for i in range(replicate):\n",
    "                train_1 = excel_reader.ExcelReader().read_df_from_excel(os.path.join(\"../tmp/study_type_data\", file))\n",
    "                train_1[\"label\"] = 0\n",
    "                for i in range(len(train_1)):\n",
    "                    train_1[\"label\"].values[i] = 1 if train_1[label].values[i] >= 0.5 else 0\n",
    "                all_df = pd.concat([all_df, train_1[[\"title\", \"abstract\", \"label\"]]],axis=0)\n",
    "    df_merged = pd.concat([all_df, replicated_df],axis=0)\n",
    "    count_ones = len(df_merged[df_merged[\"label\"] == 1])\n",
    "    count_zeros_to_take = len(all_df)\n",
    "    if proportions_to_use is not None:\n",
    "        count_zeros = count_ones * proportions_to_use\n",
    "        count_zeros_to_take = count_zeros - len(replicated_df[replicated_df[\"label\"] == 0])\n",
    "        count_zeros_to_take = max(0, count_zeros_to_take)\n",
    "    all_df = pd.concat([all_df[all_df[\"label\"] == 1], shuffle(all_df[all_df[\"label\"] == 0])[:count_zeros_to_take], replicated_df],axis=0)\n",
    "    print(\"Train \", len(all_df))\n",
    "    print(all_df[\"label\"].value_counts())\n",
    "    print(\"Test \", len(test))\n",
    "    print(test[\"label\"].value_counts())\n",
    "    all_df = shuffle(all_df)\n",
    "    return all_df, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "priority_labels = {\"No category\": 0, \"Field study\": 0, \"Laboratory study\": 1, 'Modeling study': 3, 'Observational study': 2, 'Review paper': 4}\n",
    "\n",
    "study_labels = [\"Observational study\", \"Modeling study\", \"Laboratory study\", \"Review paper\", \"Field study\"]\n",
    "\n",
    "def create_one_label_df(test):\n",
    "    test[\"label\"] = 5\n",
    "    for i in range(len(test)):\n",
    "        priority_labels_data = [\n",
    "            (StudyTypeLabels.STUDY_TYPE_LABEL_TO_NUMBER[column], priority_labels[column]) for column in test.columns if column in priority_labels and test[column].values[i] >= 0.5]\n",
    "        if len(priority_labels_data):\n",
    "            test[\"label\"].values[i] = sorted(priority_labels_data, key=lambda x: x[1])[0][0]\n",
    "    test = test[[\"title\", \"abstract\", \"label\"]]\n",
    "    return test\n",
    "\n",
    "def return_all_df_labels(test):\n",
    "    test[\"label\"] = \"\"\n",
    "    for i in range(len(test)):\n",
    "        priority_labels_data = [\n",
    "            (StudyTypeLabels.STUDY_TYPE_LABEL_TO_NUMBER[column], priority_labels[column]) for column in test.columns if column in priority_labels and test[column].values[i] >= 0.5]\n",
    "        if len(priority_labels_data):\n",
    "            test[\"label\"].values[i] = [r[0] for r in sorted(priority_labels_data, key=lambda x: x[1])]\n",
    "        else:\n",
    "            test[\"label\"].values[i] = [5]\n",
    "    test = test[[\"title\", \"abstract\", \"label\"]]\n",
    "    return test\n",
    "\n",
    "def load_train_study_type_one_label():\n",
    "    all_df = pd.DataFrame()\n",
    "    test = None\n",
    "    for file in os.listdir(\"../tmp/study_type_data\"):\n",
    "        replicate = 1\n",
    "        if file == \"labelled_multi.xlsx\":\n",
    "            replicate = 4\n",
    "        print(file, replicate)\n",
    "        if file == \"valid_data.xlsx\":\n",
    "            test = excel_reader.ExcelReader().read_df_from_excel(os.path.join(\"../tmp/study_type_data\", file))\n",
    "            test = create_one_label_df(test)\n",
    "        for i in range(replicate):\n",
    "            train_1 = excel_reader.ExcelReader().read_df_from_excel(os.path.join(\"../tmp/study_type_data\", file))\n",
    "            train_1 = create_one_label_df(train_1)\n",
    "            all_df = pd.concat([all_df, train_1[[\"title\", \"abstract\", \"label\"]]],axis=0)\n",
    "    print(\"Train \", len(all_df))\n",
    "    print(all_df[\"label\"].value_counts())\n",
    "    print(\"Test \", len(test))\n",
    "    print(test[\"label\"].value_counts())\n",
    "    all_df = shuffle(all_df)\n",
    "    return all_df, test\n",
    "\n",
    "def create_multi_label_target(df):\n",
    "    df[\"label\"] = \"\"\n",
    "    for i in range(len(df)):\n",
    "        target_labels = []\n",
    "        for column in study_labels:\n",
    "            target_labels.append(df[column].values[i])\n",
    "        df[\"label\"].values[i] = target_labels\n",
    "    return df[[\"title\", \"abstract\", \"label\"]]\n",
    "\n",
    "def load_train_study_type_multi_label():\n",
    "    all_df = pd.DataFrame()\n",
    "    test = None\n",
    "    for file in os.listdir(\"../tmp/study_type_data\"):\n",
    "        replicate = 1\n",
    "        if file == \"labelled_multi.xlsx\":\n",
    "            replicate = 4\n",
    "        print(file, replicate)\n",
    "        if file == \"valid_data.xlsx\":\n",
    "            test = excel_reader.ExcelReader().read_df_from_excel(os.path.join(\"../tmp/study_type_data\", file))\n",
    "            test = create_multi_label_target(test)\n",
    "        for i in range(replicate):\n",
    "            train_1 = excel_reader.ExcelReader().read_df_from_excel(os.path.join(\"../tmp/study_type_data\", file))\n",
    "            train_1 = create_multi_label_target(train_1)\n",
    "            all_df = pd.concat([all_df, train_1],axis=0)\n",
    "    print(\"Train \", len(all_df))\n",
    "    print(\"Test \", len(test))\n",
    "    all_df = shuffle(all_df)\n",
    "    return all_df, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_study_type_test_data.xlsx 1\n",
      "Read file ../tmp/study_type_data\\final_study_type_test_data.xlsx: 0.05s\n",
      "Processed file ../tmp/study_type_data\\final_study_type_test_data.xlsx: 0.01s\n",
      "labelled_multi.xlsx 4\n",
      "Read file ../tmp/study_type_data\\labelled_multi.xlsx: 0.12s\n",
      "Processed file ../tmp/study_type_data\\labelled_multi.xlsx: 0.04s\n",
      "Read file ../tmp/study_type_data\\labelled_multi.xlsx: 0.11s\n",
      "Processed file ../tmp/study_type_data\\labelled_multi.xlsx: 0.03s\n",
      "Read file ../tmp/study_type_data\\labelled_multi.xlsx: 0.11s\n",
      "Processed file ../tmp/study_type_data\\labelled_multi.xlsx: 0.04s\n",
      "Read file ../tmp/study_type_data\\labelled_multi.xlsx: 0.23s\n",
      "Processed file ../tmp/study_type_data\\labelled_multi.xlsx: 0.06s\n",
      "train_keywords_plus_multi.xlsx 1\n",
      "Read file ../tmp/study_type_data\\train_keywords_plus_multi.xlsx: 2.10s\n",
      "Processed file ../tmp/study_type_data\\train_keywords_plus_multi.xlsx: 1.06s\n",
      "valid_data.xlsx 1\n",
      "Read file ../tmp/study_type_data\\valid_data.xlsx: 0.10s\n",
      "Processed file ../tmp/study_type_data\\valid_data.xlsx: 0.06s\n",
      "Read file ../tmp/study_type_data\\valid_data.xlsx: 0.11s\n",
      "Processed file ../tmp/study_type_data\\valid_data.xlsx: 0.07s\n",
      "Train  20252\n",
      "Test  888\n",
      "Read file ../tmp/study_type_data/final_study_type_test_data.xlsx: 0.04s\n",
      "Processed file ../tmp/study_type_data/final_study_type_test_data.xlsx: 0.01s\n"
     ]
    }
   ],
   "source": [
    "train, test = load_train_study_type_multi_label()\n",
    "test_plus_stefan_all_labels = create_multi_label_target(excel_reader.ExcelReader().read_df_from_excel(\"../tmp/study_type_data/final_study_type_test_data.xlsx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_study_type_test_data.xlsx 1\n",
      "Read file ../tmp/study_type_data\\final_study_type_test_data.xlsx: 1.08s\n",
      "Processed file ../tmp/study_type_data\\final_study_type_test_data.xlsx: 0.02s\n",
      "labelled_multi.xlsx 4\n",
      "Read file ../tmp/study_type_data\\labelled_multi.xlsx: 0.24s\n",
      "Processed file ../tmp/study_type_data\\labelled_multi.xlsx: 0.08s\n",
      "Read file ../tmp/study_type_data\\labelled_multi.xlsx: 0.19s\n",
      "Processed file ../tmp/study_type_data\\labelled_multi.xlsx: 0.06s\n",
      "Read file ../tmp/study_type_data\\labelled_multi.xlsx: 0.18s\n",
      "Processed file ../tmp/study_type_data\\labelled_multi.xlsx: 0.07s\n",
      "Read file ../tmp/study_type_data\\labelled_multi.xlsx: 0.18s\n",
      "Processed file ../tmp/study_type_data\\labelled_multi.xlsx: 0.05s\n",
      "train_keywords_plus_multi.xlsx 1\n",
      "Read file ../tmp/study_type_data\\train_keywords_plus_multi.xlsx: 2.64s\n",
      "Processed file ../tmp/study_type_data\\train_keywords_plus_multi.xlsx: 1.62s\n",
      "valid_data.xlsx 1\n",
      "Read file ../tmp/study_type_data\\valid_data.xlsx: 0.17s\n",
      "Processed file ../tmp/study_type_data\\valid_data.xlsx: 0.10s\n",
      "Read file ../tmp/study_type_data\\valid_data.xlsx: 0.15s\n",
      "Processed file ../tmp/study_type_data\\valid_data.xlsx: 0.09s\n",
      "Train  20252\n",
      "0    7270\n",
      "3    3247\n",
      "4    2600\n",
      "1    2466\n",
      "5    2431\n",
      "2    2238\n",
      "Name: label, dtype: int64\n",
      "Test  888\n",
      "0    316\n",
      "3    144\n",
      "1    119\n",
      "5    116\n",
      "4     97\n",
      "2     96\n",
      "Name: label, dtype: int64\n",
      "Read file ../tmp/study_type_data/final_study_type_test_data.xlsx: 0.04s\n",
      "Processed file ../tmp/study_type_data/final_study_type_test_data.xlsx: 0.01s\n",
      "Read file ../tmp/study_type_data/final_study_type_test_data.xlsx: 0.07s\n",
      "Processed file ../tmp/study_type_data/final_study_type_test_data.xlsx: 0.02s\n"
     ]
    }
   ],
   "source": [
    "train, test = load_train_study_type_one_label()\n",
    "test_plus_stefan_one_label = create_one_label_df(excel_reader.ExcelReader().read_df_from_excel(\"../tmp/study_type_data/final_study_type_test_data.xlsx\"))\n",
    "test_plus_stefan_all_labels = return_all_df_labels(excel_reader.ExcelReader().read_df_from_excel(\"../tmp/study_type_data/final_study_type_test_data.xlsx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used for model gpu 0\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n"
     ]
    }
   ],
   "source": [
    "r = _study_type_labeler.predict_with_meta_model(test_plus_stefan_one_label, with_head_tail = True, recreate_model = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[75  3  1  2  6  4]\n",
      " [ 7 29  0  1  2  0]\n",
      " [ 2  0 22  0  1  3]\n",
      " [ 4  0  0 26  1  1]\n",
      " [ 5  0  0  1 24  0]\n",
      " [ 7  0  0  0  1 22]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.82      0.79        91\n",
      "           1       0.91      0.74      0.82        39\n",
      "           2       0.96      0.79      0.86        28\n",
      "           3       0.87      0.81      0.84        32\n",
      "           4       0.69      0.80      0.74        30\n",
      "           5       0.73      0.73      0.73        30\n",
      "\n",
      "   micro avg       0.79      0.79      0.79       250\n",
      "   macro avg       0.82      0.78      0.80       250\n",
      "weighted avg       0.80      0.79      0.79       250\n",
      "\n",
      "F1 score:  0.795915228306712\n"
     ]
    }
   ],
   "source": [
    "true_results = []\n",
    "for i in range(len(r[1])):\n",
    "    if r[1][i] in test_plus_stefan_all_labels[\"label\"].values[i]:\n",
    "        true_results.append(r[1][i])\n",
    "    else:\n",
    "        true_results.append(test_plus_stefan_one_label[\"label\"].values[i])\n",
    "_study_type_labeler.print_summary(true_results, r[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[73 14  1  4  6  4]\n",
      " [ 7 14  0  2  2  0]\n",
      " [ 4  3 21  0  1  3]\n",
      " [ 4  0  0 23  1  1]\n",
      " [ 5  1  1  1 24  0]\n",
      " [ 7  0  0  0  1 22]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.72      0.72       102\n",
      "           1       0.44      0.56      0.49        25\n",
      "           2       0.91      0.66      0.76        32\n",
      "           3       0.77      0.79      0.78        29\n",
      "           4       0.69      0.75      0.72        32\n",
      "           5       0.73      0.73      0.73        30\n",
      "\n",
      "   micro avg       0.71      0.71      0.71       250\n",
      "   macro avg       0.71      0.70      0.70       250\n",
      "weighted avg       0.72      0.71      0.71       250\n",
      "\n",
      "F1 score:  0.7011748286282953\n"
     ]
    }
   ],
   "source": [
    "_study_type_labeler.print_summary(test_plus_stefan_one_label[\"label\"], r[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_models import base_bert_model_meta_model\n",
    "base_bert_model_meta_model = reload(base_bert_model_meta_model)\n",
    "from bert_models import base_bert_model_meta_model_with_window_all\n",
    "base_bert_model_meta_model_with_window_all = reload(base_bert_model_meta_model_with_window_all)\n",
    "from study_design_type import study_type_labeler\n",
    "study_type_labeler = reload(study_type_labeler)\n",
    "from study_design_type import study_type_labeler_window_agg\n",
    "study_type_labeler_window_agg = reload(study_type_labeler_window_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../tmp/scibert_scivocab_uncased\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'study_type_multi', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001F31DA00C50>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'study_type_multi', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001F31DA00C50>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config is done\n",
      "INFO:tensorflow:Writing example 0 of 20252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 20252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] who informal consultation on fever management in peripheral health care settings : a global review of evidence and practice . [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] who informal consultation on fever management in peripheral health care settings : a global review of evidence and practice . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 975 13217 15841 191 10551 1837 121 5270 947 1459 5877 862 106 2523 1579 131 1775 137 2794 205 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 975 13217 15841 191 10551 1837 121 5270 947 1459 5877 862 106 2523 1579 131 1775 137 2794 205 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] reflections on conducting evaluations for rural development interventions in china [SEP] an appropriate evaluation methodology is critical in collecting valid data in complex development intervention contexts . this paper explores this issue by putting forward an appropriate evaluation methodology for development interventions in rural china . it draws on the experience of an impact evaluation of a sustainable agricultural biodiversity management project conducted in ha ##ina ##n , china in 2010 . the authors propose that evaluation be culturally responsive and the evaluation design be rooted in the particular cultural context where an evaluation is conducted . the appropriate use of the participatory rural appraisal ( pra ) approach and methods helps generate data that are relevant and meaningful for evaluation purposes in rural china [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] reflections on conducting evaluations for rural development interventions in china [SEP] an appropriate evaluation methodology is critical in collecting valid data in complex development intervention contexts . this paper explores this issue by putting forward an appropriate evaluation methodology for development interventions in rural china . it draws on the experience of an impact evaluation of a sustainable agricultural biodiversity management project conducted in ha ##ina ##n , china in 2010 . the authors propose that evaluation be culturally responsive and the evaluation design be rooted in the particular cultural context where an evaluation is conducted . the appropriate use of the participatory rural appraisal ( pra ) approach and methods helps generate data that are relevant and meaningful for evaluation purposes in rural china [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 14959 191 11842 9963 168 7046 1120 5434 121 3640 103 130 2826 2166 4964 165 2616 121 12543 2119 453 121 1127 1120 3832 8653 205 238 1203 20247 238 3060 214 17408 4200 130 2826 2166 4964 168 1120 5434 121 7046 3640 205 256 20517 191 111 2899 131 130 2141 2166 131 106 10886 7675 15818 1837 2575 2728 121 325 1869 30111 422 3640 121 4398 205 111 1991 4459 198 2166 195 25500 14346 137 111 2166 899 195 19758 121 111 1349 6656 2220 582 130 2166 165 2728 205 111 2826 626 131 111 24873 7046 21972 145 19933 546 1139 137 1045 8978 3730 453 198 220 2884 137 9473 168 2166 5699 121 7046 3640 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 14959 191 11842 9963 168 7046 1120 5434 121 3640 103 130 2826 2166 4964 165 2616 121 12543 2119 453 121 1127 1120 3832 8653 205 238 1203 20247 238 3060 214 17408 4200 130 2826 2166 4964 168 1120 5434 121 7046 3640 205 256 20517 191 111 2899 131 130 2141 2166 131 106 10886 7675 15818 1837 2575 2728 121 325 1869 30111 422 3640 121 4398 205 111 1991 4459 198 2166 195 25500 14346 137 111 2166 899 195 19758 121 111 1349 6656 2220 582 130 2166 165 2728 205 111 2826 626 131 111 24873 7046 21972 145 19933 546 1139 137 1045 8978 3730 453 198 220 2884 137 9473 168 2166 5699 121 7046 3640 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] determinants of child nutritional status in the eastern province of za ##mb ##ia : the role of improved maize varieties [SEP] using household survey data from a sample of 81 ##0 households , this paper analyses the determinants of children ' s nutritional status and evaluates the impacts of improved maize varieties on child malnutrition in eastern za ##mb ##ia . the paper uses an endogenous switching regression technique , combined with propensity score matching , to assess the determinants of child malnutrition and impacts of improved maize varieties on nutritional status . the study finds that child nutrition worse ##ns with the age of the child and improves with education of household head and female household members , number of adult females in the household , and access to better san ##itation . the study also finds a robust and significant impact of improved maize varieties on child malnutrition . the empirical results indicate that adoption of improved maize varieties reduces the probability of stu ##nt ##ing by an average of about 26 % [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] determinants of child nutritional status in the eastern province of za ##mb ##ia : the role of improved maize varieties [SEP] using household survey data from a sample of 81 ##0 households , this paper analyses the determinants of children ' s nutritional status and evaluates the impacts of improved maize varieties on child malnutrition in eastern za ##mb ##ia . the paper uses an endogenous switching regression technique , combined with propensity score matching , to assess the determinants of child malnutrition and impacts of improved maize varieties on nutritional status . the study finds that child nutrition worse ##ns with the age of the child and improves with education of household head and female household members , number of adult females in the household , and access to better san ##itation . the study also finds a robust and significant impact of improved maize varieties on child malnutrition . the empirical results indicate that adoption of improved maize varieties reduces the probability of stu ##nt ##ing by an average of about 26 % [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 10687 131 1326 11073 2726 121 111 10309 11768 131 14751 9111 426 862 111 1447 131 3007 13156 13600 103 487 5430 3241 453 263 106 1498 131 8959 30131 9654 422 238 1203 2519 111 10687 131 1808 2505 112 11073 2726 137 18224 111 7681 131 3007 13156 13600 191 1326 23609 121 10309 14751 9111 426 205 111 1203 3294 130 6364 5980 3089 2358 422 3109 190 15310 2867 4740 422 147 1285 111 10687 131 1326 23609 137 7681 131 3007 13156 13600 191 11073 2726 205 111 527 11822 198 1326 8556 8946 1290 190 111 1407 131 111 1326 137 8112 190 2870 131 5430 2795 137 3672 5430 3087 422 649 131 3873 5003 121 111 5430 422 137 1899 147 1883 5754 2202 205 111 527 469 11822 106 3701 137 684 2141 131 3007 13156 13600 191 1326 23609 205 111 4808 545 2645 198 9714 131 3007 13156 13600 5174 111 2064 131 22231 3372 140 214 130 1568 131 1011 2381 1863 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 10687 131 1326 11073 2726 121 111 10309 11768 131 14751 9111 426 862 111 1447 131 3007 13156 13600 103 487 5430 3241 453 263 106 1498 131 8959 30131 9654 422 238 1203 2519 111 10687 131 1808 2505 112 11073 2726 137 18224 111 7681 131 3007 13156 13600 191 1326 23609 121 10309 14751 9111 426 205 111 1203 3294 130 6364 5980 3089 2358 422 3109 190 15310 2867 4740 422 147 1285 111 10687 131 1326 23609 137 7681 131 3007 13156 13600 191 11073 2726 205 111 527 11822 198 1326 8556 8946 1290 190 111 1407 131 111 1326 137 8112 190 2870 131 5430 2795 137 3672 5430 3087 422 649 131 3873 5003 121 111 5430 422 137 1899 147 1883 5754 2202 205 111 527 469 11822 106 3701 137 684 2141 131 3007 13156 13600 191 1326 23609 205 111 4808 545 2645 198 9714 131 3007 13156 13600 5174 111 2064 131 22231 3372 140 214 130 1568 131 1011 2381 1863 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] promoting small ##holder seed enterprises : quality seed production of rice , maize , sorghum and mill ##et in northern camer ##oon [SEP] food and agriculture organization of the united nations ( fa ##o ) conducted two projects in camer ##oon on strengthening rice , and sorghum , maize and mill ##et seed production by small ##holder seed enterprises ( ss ##es ) . farmer groups ( respectively , 25 ss ##es and 114 ss ##es ) were formed or strengthened and trained . sixty - six selected lead farmers were trained on the admin ##istering and managing of revol ##ving funds . the groups were then linked to the extension service ( for continuous technical advice ) , agriculture research for development institution ( continuous supply of needed seed ) , national seed service ( seed certification ) and to financial institutions ( financial support ) . the results obtained showed that 60 and 59 . 6 per cent , respectively , of the groups sustained their activities two to three years after the projects ended . total certified rice seed produced increased to 800 ton ##nes ( t ) against 267 ##t at the beginning . for the other cereal project , the total certified seed produced was 71 ##9 . 2 ##t against 49 ##7 ##t at its beginning . the use of quality seed rice together with good agro ##nom ##ic practices increased yields ( up to 8 . 0 ##t / ha against a scan ##t 2 ##t / [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] promoting small ##holder seed enterprises : quality seed production of rice , maize , sorghum and mill ##et in northern camer ##oon [SEP] food and agriculture organization of the united nations ( fa ##o ) conducted two projects in camer ##oon on strengthening rice , and sorghum , maize and mill ##et seed production by small ##holder seed enterprises ( ss ##es ) . farmer groups ( respectively , 25 ss ##es and 114 ss ##es ) were formed or strengthened and trained . sixty - six selected lead farmers were trained on the admin ##istering and managing of revol ##ving funds . the groups were then linked to the extension service ( for continuous technical advice ) , agriculture research for development institution ( continuous supply of needed seed ) , national seed service ( seed certification ) and to financial institutions ( financial support ) . the results obtained showed that 60 and 59 . 6 per cent , respectively , of the groups sustained their activities two to three years after the projects ended . total certified rice seed produced increased to 800 ton ##nes ( t ) against 267 ##t at the beginning . for the other cereal project , the total certified seed produced was 71 ##9 . 2 ##t against 49 ##7 ##t at its beginning . the use of quality seed rice together with good agro ##nom ##ic practices increased yields ( up to 8 . 0 ##t / ha against a scan ##t 2 ##t / [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 10302 952 13643 3678 14271 862 1671 3678 1865 131 8058 422 13156 422 29938 137 3210 160 121 7926 12197 9840 103 2599 137 9879 3288 131 111 4274 14461 145 2016 30112 546 2728 502 5994 121 12197 9840 191 19909 8058 422 137 29938 422 13156 137 3210 160 3678 1865 214 952 13643 3678 14271 145 3712 123 546 205 24830 1302 145 1222 422 1552 3712 123 137 14390 3712 123 546 267 4127 234 24822 137 7222 205 20151 579 2781 2350 1269 12288 267 7222 191 111 2363 22754 137 11170 131 27914 1130 11190 205 111 1302 267 666 5094 147 111 3840 2289 145 168 2668 4147 11402 546 422 9879 849 168 1120 5051 145 2668 4494 131 2764 3678 546 422 2227 3678 2289 145 3678 20390 546 137 147 4437 7238 145 4437 1385 546 205 111 545 1151 1367 198 2242 137 5275 205 370 309 1312 422 1222 422 131 111 1302 8845 547 2687 502 147 874 1320 647 111 5994 20673 205 1114 22565 8058 3678 2772 1175 147 9931 13354 6820 145 105 546 2089 27125 30108 235 111 5570 205 168 111 494 29582 2575 422 111 1114 22565 3678 2772 241 8621 30141 205 170 30108 2089 5069 30145 30108 235 633 5570 205 111 626 131 1671 3678 8058 2738 190 1846 22362 25429 141 5423 1175 5072 145 692 147 493 205 244 30108 1352 325 2089 106 6242 30108 170 30108 1352 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 10302 952 13643 3678 14271 862 1671 3678 1865 131 8058 422 13156 422 29938 137 3210 160 121 7926 12197 9840 103 2599 137 9879 3288 131 111 4274 14461 145 2016 30112 546 2728 502 5994 121 12197 9840 191 19909 8058 422 137 29938 422 13156 137 3210 160 3678 1865 214 952 13643 3678 14271 145 3712 123 546 205 24830 1302 145 1222 422 1552 3712 123 137 14390 3712 123 546 267 4127 234 24822 137 7222 205 20151 579 2781 2350 1269 12288 267 7222 191 111 2363 22754 137 11170 131 27914 1130 11190 205 111 1302 267 666 5094 147 111 3840 2289 145 168 2668 4147 11402 546 422 9879 849 168 1120 5051 145 2668 4494 131 2764 3678 546 422 2227 3678 2289 145 3678 20390 546 137 147 4437 7238 145 4437 1385 546 205 111 545 1151 1367 198 2242 137 5275 205 370 309 1312 422 1222 422 131 111 1302 8845 547 2687 502 147 874 1320 647 111 5994 20673 205 1114 22565 8058 3678 2772 1175 147 9931 13354 6820 145 105 546 2089 27125 30108 235 111 5570 205 168 111 494 29582 2575 422 111 1114 22565 3678 2772 241 8621 30141 205 170 30108 2089 5069 30145 30108 235 633 5570 205 111 626 131 1671 3678 8058 2738 190 1846 22362 25429 141 5423 1175 5072 145 692 147 493 205 244 30108 1352 325 2089 106 6242 30108 170 30108 1352 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] catalogue of the insect fauna associated with food stores in the province of mat ##anz ##as . [SEP] a catalogue of detected pest insects associated to food warehouse ##s of mat ##anz ##as city is offered . entom ##ological records available in provincial plant health laboratory were verified and the species occurrence captured by means of insect trap systems placed in six food warehouse ##s during the period july 2006 to march 2007 is informed too . thirty eight species of nox ##ious insects belonging to three orders and 14 families are reported ; the cole ##opter ##ons were the taxa of highest incidence , represented by 33 species , of those which ah ##as ##ver ##us adv ##ena wal ##tl . , za ##bro ##tes sub ##fas ##cia ##tu ##s ( boh ##eman ) , or ##yz ##ae ##phil ##us sur ##ina ##men ##sis lin ##na ##eu ##s , carp ##ophilus dim ##idia ##tu ##s ( fabric ##ius ) and trib ##oli ##um cast ##ane ##um herb ##st . , were the most frequent ##s . rice and bean ##s were the foods of highest attack , although severe infest ##ations was also observed in powder ##ed milk , nutri ##tious past ##ure , coffee , lenti ##l , and seeds and concentrated , products that constituted new host reports for carp ##ophilus dim ##idia ##tu ##s . lip ##osc ##eli ##s sp . ( pso ##cop ##tera : lip ##osc ##eli ##da ##e ) is informed for the first time as pest [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] catalogue of the insect fauna associated with food stores in the province of mat ##anz ##as . [SEP] a catalogue of detected pest insects associated to food warehouse ##s of mat ##anz ##as city is offered . entom ##ological records available in provincial plant health laboratory were verified and the species occurrence captured by means of insect trap systems placed in six food warehouse ##s during the period july 2006 to march 2007 is informed too . thirty eight species of nox ##ious insects belonging to three orders and 14 families are reported ; the cole ##opter ##ons were the taxa of highest incidence , represented by 33 species , of those which ah ##as ##ver ##us adv ##ena wal ##tl . , za ##bro ##tes sub ##fas ##cia ##tu ##s ( boh ##eman ) , or ##yz ##ae ##phil ##us sur ##ina ##men ##sis lin ##na ##eu ##s , carp ##ophilus dim ##idia ##tu ##s ( fabric ##ius ) and trib ##oli ##um cast ##ane ##um herb ##st . , were the most frequent ##s . rice and bean ##s were the foods of highest attack , although severe infest ##ations was also observed in powder ##ed milk , nutri ##tious past ##ure , coffee , lenti ##l , and seeds and concentrated , products that constituted new host reports for carp ##ophilus dim ##idia ##tu ##s . lip ##osc ##eli ##s sp . ( pso ##cop ##tera : lip ##osc ##eli ##da ##e ) is informed for the first time as pest [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 28251 131 111 12483 28881 1111 190 2599 11714 121 111 11768 131 740 8271 142 205 103 106 28251 131 2490 19038 14577 1111 147 2599 25791 30113 131 740 8271 142 5523 165 8232 205 20061 872 5934 1427 121 23805 2529 947 3131 267 8219 137 111 1578 5836 8026 214 2033 131 12483 11639 1078 4032 121 2781 2599 25791 30113 781 111 1275 5129 6186 147 5234 5248 165 6841 3872 205 12498 4834 1578 131 17520 3456 14577 9386 147 874 8483 137 1128 5498 220 1214 1814 111 16376 13331 646 267 111 12026 131 3435 3970 422 3716 214 3307 1578 422 131 1052 334 7839 142 207 153 1365 9793 4178 6687 205 422 14751 7519 1813 414 16145 19639 13004 30113 145 21344 9918 546 422 234 24813 1611 17811 153 557 1869 4393 4554 3158 648 3061 30113 422 17328 24089 5107 17935 13004 30113 145 6154 11629 546 137 12439 7780 200 8474 840 200 10703 187 205 422 267 111 755 5808 30113 205 8058 137 23022 30113 267 111 11107 131 3435 3689 422 1363 3167 24445 288 241 469 1058 121 10519 119 7435 422 4962 7106 3648 221 422 17677 422 18809 30115 422 137 9182 137 8899 422 2800 198 19245 758 3757 3578 168 17328 24089 5107 17935 13004 30113 205 2266 26624 19478 30113 273 205 145 17685 1564 24969 862 2266 26624 19478 2526 30107 546 165 6841 168 111 705 532 188 19038 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 28251 131 111 12483 28881 1111 190 2599 11714 121 111 11768 131 740 8271 142 205 103 106 28251 131 2490 19038 14577 1111 147 2599 25791 30113 131 740 8271 142 5523 165 8232 205 20061 872 5934 1427 121 23805 2529 947 3131 267 8219 137 111 1578 5836 8026 214 2033 131 12483 11639 1078 4032 121 2781 2599 25791 30113 781 111 1275 5129 6186 147 5234 5248 165 6841 3872 205 12498 4834 1578 131 17520 3456 14577 9386 147 874 8483 137 1128 5498 220 1214 1814 111 16376 13331 646 267 111 12026 131 3435 3970 422 3716 214 3307 1578 422 131 1052 334 7839 142 207 153 1365 9793 4178 6687 205 422 14751 7519 1813 414 16145 19639 13004 30113 145 21344 9918 546 422 234 24813 1611 17811 153 557 1869 4393 4554 3158 648 3061 30113 422 17328 24089 5107 17935 13004 30113 145 6154 11629 546 137 12439 7780 200 8474 840 200 10703 187 205 422 267 111 755 5808 30113 205 8058 137 23022 30113 267 111 11107 131 3435 3689 422 1363 3167 24445 288 241 469 1058 121 10519 119 7435 422 4962 7106 3648 221 422 17677 422 18809 30115 422 137 9182 137 8899 422 2800 198 19245 758 3757 3578 168 17328 24089 5107 17935 13004 30113 205 2266 26624 19478 30113 273 205 145 17685 1564 24969 862 2266 26624 19478 2526 30107 546 165 6841 168 111 705 532 188 19038 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 10000 of 20252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 10000 of 20252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 20000 of 20252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 20000 of 20252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used for model gpu 0\n",
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from study_type_multi\\model.ckpt-2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from study_type_multi\\model.ckpt-2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared train from beginning\n",
      "INFO:tensorflow:Writing example 0 of 20252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 20252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] who informal consultation on fever management in peripheral health care settings : a global review of evidence and practice . [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] who informal consultation on fever management in peripheral health care settings : a global review of evidence and practice . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 975 13217 15841 191 10551 1837 121 5270 947 1459 5877 862 106 2523 1579 131 1775 137 2794 205 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 975 13217 15841 191 10551 1837 121 5270 947 1459 5877 862 106 2523 1579 131 1775 137 2794 205 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] reflections on conducting evaluations for rural development interventions in china [SEP] an appropriate evaluation methodology is critical in collecting valid data in complex development intervention contexts . this paper explores this issue by putting forward an appropriate evaluation methodology for development interventions in rural china . it draws on the experience of an impact evaluation of a sustainable agricultural biodiversity management project conducted in ha ##ina ##n , china in 2010 . the authors propose that evaluation be culturally responsive and the evaluation design be rooted in the particular cultural context where an evaluation is conducted . the appropriate use of the participatory rural appraisal ( pra ) approach and methods helps generate data that are relevant and meaningful for evaluation purposes in rural china [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] reflections on conducting evaluations for rural development interventions in china [SEP] an appropriate evaluation methodology is critical in collecting valid data in complex development intervention contexts . this paper explores this issue by putting forward an appropriate evaluation methodology for development interventions in rural china . it draws on the experience of an impact evaluation of a sustainable agricultural biodiversity management project conducted in ha ##ina ##n , china in 2010 . the authors propose that evaluation be culturally responsive and the evaluation design be rooted in the particular cultural context where an evaluation is conducted . the appropriate use of the participatory rural appraisal ( pra ) approach and methods helps generate data that are relevant and meaningful for evaluation purposes in rural china [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 14959 191 11842 9963 168 7046 1120 5434 121 3640 103 130 2826 2166 4964 165 2616 121 12543 2119 453 121 1127 1120 3832 8653 205 238 1203 20247 238 3060 214 17408 4200 130 2826 2166 4964 168 1120 5434 121 7046 3640 205 256 20517 191 111 2899 131 130 2141 2166 131 106 10886 7675 15818 1837 2575 2728 121 325 1869 30111 422 3640 121 4398 205 111 1991 4459 198 2166 195 25500 14346 137 111 2166 899 195 19758 121 111 1349 6656 2220 582 130 2166 165 2728 205 111 2826 626 131 111 24873 7046 21972 145 19933 546 1139 137 1045 8978 3730 453 198 220 2884 137 9473 168 2166 5699 121 7046 3640 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 14959 191 11842 9963 168 7046 1120 5434 121 3640 103 130 2826 2166 4964 165 2616 121 12543 2119 453 121 1127 1120 3832 8653 205 238 1203 20247 238 3060 214 17408 4200 130 2826 2166 4964 168 1120 5434 121 7046 3640 205 256 20517 191 111 2899 131 130 2141 2166 131 106 10886 7675 15818 1837 2575 2728 121 325 1869 30111 422 3640 121 4398 205 111 1991 4459 198 2166 195 25500 14346 137 111 2166 899 195 19758 121 111 1349 6656 2220 582 130 2166 165 2728 205 111 2826 626 131 111 24873 7046 21972 145 19933 546 1139 137 1045 8978 3730 453 198 220 2884 137 9473 168 2166 5699 121 7046 3640 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] determinants of child nutritional status in the eastern province of za ##mb ##ia : the role of improved maize varieties [SEP] using household survey data from a sample of 81 ##0 households , this paper analyses the determinants of children ' s nutritional status and evaluates the impacts of improved maize varieties on child malnutrition in eastern za ##mb ##ia . the paper uses an endogenous switching regression technique , combined with propensity score matching , to assess the determinants of child malnutrition and impacts of improved maize varieties on nutritional status . the study finds that child nutrition worse ##ns with the age of the child and improves with education of household head and female household members , number of adult females in the household , and access to better san ##itation . the study also finds a robust and significant impact of improved maize varieties on child malnutrition . the empirical results indicate that adoption of improved maize varieties reduces the probability of stu ##nt ##ing by an average of about 26 % [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] determinants of child nutritional status in the eastern province of za ##mb ##ia : the role of improved maize varieties [SEP] using household survey data from a sample of 81 ##0 households , this paper analyses the determinants of children ' s nutritional status and evaluates the impacts of improved maize varieties on child malnutrition in eastern za ##mb ##ia . the paper uses an endogenous switching regression technique , combined with propensity score matching , to assess the determinants of child malnutrition and impacts of improved maize varieties on nutritional status . the study finds that child nutrition worse ##ns with the age of the child and improves with education of household head and female household members , number of adult females in the household , and access to better san ##itation . the study also finds a robust and significant impact of improved maize varieties on child malnutrition . the empirical results indicate that adoption of improved maize varieties reduces the probability of stu ##nt ##ing by an average of about 26 % [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 10687 131 1326 11073 2726 121 111 10309 11768 131 14751 9111 426 862 111 1447 131 3007 13156 13600 103 487 5430 3241 453 263 106 1498 131 8959 30131 9654 422 238 1203 2519 111 10687 131 1808 2505 112 11073 2726 137 18224 111 7681 131 3007 13156 13600 191 1326 23609 121 10309 14751 9111 426 205 111 1203 3294 130 6364 5980 3089 2358 422 3109 190 15310 2867 4740 422 147 1285 111 10687 131 1326 23609 137 7681 131 3007 13156 13600 191 11073 2726 205 111 527 11822 198 1326 8556 8946 1290 190 111 1407 131 111 1326 137 8112 190 2870 131 5430 2795 137 3672 5430 3087 422 649 131 3873 5003 121 111 5430 422 137 1899 147 1883 5754 2202 205 111 527 469 11822 106 3701 137 684 2141 131 3007 13156 13600 191 1326 23609 205 111 4808 545 2645 198 9714 131 3007 13156 13600 5174 111 2064 131 22231 3372 140 214 130 1568 131 1011 2381 1863 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 10687 131 1326 11073 2726 121 111 10309 11768 131 14751 9111 426 862 111 1447 131 3007 13156 13600 103 487 5430 3241 453 263 106 1498 131 8959 30131 9654 422 238 1203 2519 111 10687 131 1808 2505 112 11073 2726 137 18224 111 7681 131 3007 13156 13600 191 1326 23609 121 10309 14751 9111 426 205 111 1203 3294 130 6364 5980 3089 2358 422 3109 190 15310 2867 4740 422 147 1285 111 10687 131 1326 23609 137 7681 131 3007 13156 13600 191 11073 2726 205 111 527 11822 198 1326 8556 8946 1290 190 111 1407 131 111 1326 137 8112 190 2870 131 5430 2795 137 3672 5430 3087 422 649 131 3873 5003 121 111 5430 422 137 1899 147 1883 5754 2202 205 111 527 469 11822 106 3701 137 684 2141 131 3007 13156 13600 191 1326 23609 205 111 4808 545 2645 198 9714 131 3007 13156 13600 5174 111 2064 131 22231 3372 140 214 130 1568 131 1011 2381 1863 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] promoting small ##holder seed enterprises : quality seed production of rice , maize , sorghum and mill ##et in northern camer ##oon [SEP] technical advice ) , agriculture research for development institution ( continuous supply of needed seed ) , national seed service ( seed certification ) and to financial institutions ( financial support ) . the results obtained showed that 60 and 59 . 6 per cent , respectively , of the groups sustained their activities two to three years after the projects ended . total certified rice seed produced increased to 800 ton ##nes ( t ) against 267 ##t at the beginning . for the other cereal project , the total certified seed produced was 71 ##9 . 2 ##t against 49 ##7 ##t at its beginning . the use of quality seed rice together with good agro ##nom ##ic practices increased yields ( up to 8 . 0 ##t / ha against a scan ##t 2 ##t / ha before ) . all interviewed farmers stressed that the seed business was profitable and helped achieve higher living standards and divers ##ify their activities . seasonal incomes were more than us $ 1 , 78 ##3 . 31 . finally , both fa ##o projects helped generate farmers ' income to about us $ 2 , 114 . 87 ##1 , produced 44 ##7 , 95 ##4 ##t of cereal grain and thus improved food security and alleviate ##d poverty . the camer ##oon ##ian success could be repeated elsewhere . [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] promoting small ##holder seed enterprises : quality seed production of rice , maize , sorghum and mill ##et in northern camer ##oon [SEP] technical advice ) , agriculture research for development institution ( continuous supply of needed seed ) , national seed service ( seed certification ) and to financial institutions ( financial support ) . the results obtained showed that 60 and 59 . 6 per cent , respectively , of the groups sustained their activities two to three years after the projects ended . total certified rice seed produced increased to 800 ton ##nes ( t ) against 267 ##t at the beginning . for the other cereal project , the total certified seed produced was 71 ##9 . 2 ##t against 49 ##7 ##t at its beginning . the use of quality seed rice together with good agro ##nom ##ic practices increased yields ( up to 8 . 0 ##t / ha against a scan ##t 2 ##t / ha before ) . all interviewed farmers stressed that the seed business was profitable and helped achieve higher living standards and divers ##ify their activities . seasonal incomes were more than us $ 1 , 78 ##3 . 31 . finally , both fa ##o projects helped generate farmers ' income to about us $ 2 , 114 . 87 ##1 , produced 44 ##7 , 95 ##4 ##t of cereal grain and thus improved food security and alleviate ##d poverty . the camer ##oon ##ian success could be repeated elsewhere . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 10302 952 13643 3678 14271 862 1671 3678 1865 131 8058 422 13156 422 29938 137 3210 160 121 7926 12197 9840 103 4147 11402 546 422 9879 849 168 1120 5051 145 2668 4494 131 2764 3678 546 422 2227 3678 2289 145 3678 20390 546 137 147 4437 7238 145 4437 1385 546 205 111 545 1151 1367 198 2242 137 5275 205 370 309 1312 422 1222 422 131 111 1302 8845 547 2687 502 147 874 1320 647 111 5994 20673 205 1114 22565 8058 3678 2772 1175 147 9931 13354 6820 145 105 546 2089 27125 30108 235 111 5570 205 168 111 494 29582 2575 422 111 1114 22565 3678 2772 241 8621 30141 205 170 30108 2089 5069 30145 30108 235 633 5570 205 111 626 131 1671 3678 8058 2738 190 1846 22362 25429 141 5423 1175 5072 145 692 147 493 205 244 30108 1352 325 2089 106 6242 30108 170 30108 1352 325 1548 546 205 355 17985 12288 16191 198 111 3678 3948 241 26205 137 12080 3120 1001 5404 5103 137 4135 2674 547 2687 205 10810 25596 267 475 506 227 3250 158 422 7898 30138 205 3877 205 2285 422 655 2016 30112 5994 12080 3730 12288 2505 5021 147 1011 227 3250 170 422 14390 205 8468 30130 422 2772 3894 30145 422 5508 30140 30108 131 29582 9235 137 959 3007 2599 3594 137 19893 30118 10884 205 111 12197 9840 1026 2108 968 195 4235 9781 205 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 10302 952 13643 3678 14271 862 1671 3678 1865 131 8058 422 13156 422 29938 137 3210 160 121 7926 12197 9840 103 4147 11402 546 422 9879 849 168 1120 5051 145 2668 4494 131 2764 3678 546 422 2227 3678 2289 145 3678 20390 546 137 147 4437 7238 145 4437 1385 546 205 111 545 1151 1367 198 2242 137 5275 205 370 309 1312 422 1222 422 131 111 1302 8845 547 2687 502 147 874 1320 647 111 5994 20673 205 1114 22565 8058 3678 2772 1175 147 9931 13354 6820 145 105 546 2089 27125 30108 235 111 5570 205 168 111 494 29582 2575 422 111 1114 22565 3678 2772 241 8621 30141 205 170 30108 2089 5069 30145 30108 235 633 5570 205 111 626 131 1671 3678 8058 2738 190 1846 22362 25429 141 5423 1175 5072 145 692 147 493 205 244 30108 1352 325 2089 106 6242 30108 170 30108 1352 325 1548 546 205 355 17985 12288 16191 198 111 3678 3948 241 26205 137 12080 3120 1001 5404 5103 137 4135 2674 547 2687 205 10810 25596 267 475 506 227 3250 158 422 7898 30138 205 3877 205 2285 422 655 2016 30112 5994 12080 3730 12288 2505 5021 147 1011 227 3250 170 422 14390 205 8468 30130 422 2772 3894 30145 422 5508 30140 30108 131 29582 9235 137 959 3007 2599 3594 137 19893 30118 10884 205 111 12197 9840 1026 2108 968 195 4235 9781 205 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] catalogue of the insect fauna associated with food stores in the province of mat ##anz ##as . [SEP] laboratory were verified and the species occurrence captured by means of insect trap systems placed in six food warehouse ##s during the period july 2006 to march 2007 is informed too . thirty eight species of nox ##ious insects belonging to three orders and 14 families are reported ; the cole ##opter ##ons were the taxa of highest incidence , represented by 33 species , of those which ah ##as ##ver ##us adv ##ena wal ##tl . , za ##bro ##tes sub ##fas ##cia ##tu ##s ( boh ##eman ) , or ##yz ##ae ##phil ##us sur ##ina ##men ##sis lin ##na ##eu ##s , carp ##ophilus dim ##idia ##tu ##s ( fabric ##ius ) and trib ##oli ##um cast ##ane ##um herb ##st . , were the most frequent ##s . rice and bean ##s were the foods of highest attack , although severe infest ##ations was also observed in powder ##ed milk , nutri ##tious past ##ure , coffee , lenti ##l , and seeds and concentrated , products that constituted new host reports for carp ##ophilus dim ##idia ##tu ##s . lip ##osc ##eli ##s sp . ( pso ##cop ##tera : lip ##osc ##eli ##da ##e ) is informed for the first time as pest insect of warehouse in our province , due to their detection in several foods and the skin damage which it caused to the warehouse ##s workers . [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] catalogue of the insect fauna associated with food stores in the province of mat ##anz ##as . [SEP] laboratory were verified and the species occurrence captured by means of insect trap systems placed in six food warehouse ##s during the period july 2006 to march 2007 is informed too . thirty eight species of nox ##ious insects belonging to three orders and 14 families are reported ; the cole ##opter ##ons were the taxa of highest incidence , represented by 33 species , of those which ah ##as ##ver ##us adv ##ena wal ##tl . , za ##bro ##tes sub ##fas ##cia ##tu ##s ( boh ##eman ) , or ##yz ##ae ##phil ##us sur ##ina ##men ##sis lin ##na ##eu ##s , carp ##ophilus dim ##idia ##tu ##s ( fabric ##ius ) and trib ##oli ##um cast ##ane ##um herb ##st . , were the most frequent ##s . rice and bean ##s were the foods of highest attack , although severe infest ##ations was also observed in powder ##ed milk , nutri ##tious past ##ure , coffee , lenti ##l , and seeds and concentrated , products that constituted new host reports for carp ##ophilus dim ##idia ##tu ##s . lip ##osc ##eli ##s sp . ( pso ##cop ##tera : lip ##osc ##eli ##da ##e ) is informed for the first time as pest insect of warehouse in our province , due to their detection in several foods and the skin damage which it caused to the warehouse ##s workers . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 28251 131 111 12483 28881 1111 190 2599 11714 121 111 11768 131 740 8271 142 205 103 3131 267 8219 137 111 1578 5836 8026 214 2033 131 12483 11639 1078 4032 121 2781 2599 25791 30113 781 111 1275 5129 6186 147 5234 5248 165 6841 3872 205 12498 4834 1578 131 17520 3456 14577 9386 147 874 8483 137 1128 5498 220 1214 1814 111 16376 13331 646 267 111 12026 131 3435 3970 422 3716 214 3307 1578 422 131 1052 334 7839 142 207 153 1365 9793 4178 6687 205 422 14751 7519 1813 414 16145 19639 13004 30113 145 21344 9918 546 422 234 24813 1611 17811 153 557 1869 4393 4554 3158 648 3061 30113 422 17328 24089 5107 17935 13004 30113 145 6154 11629 546 137 12439 7780 200 8474 840 200 10703 187 205 422 267 111 755 5808 30113 205 8058 137 23022 30113 267 111 11107 131 3435 3689 422 1363 3167 24445 288 241 469 1058 121 10519 119 7435 422 4962 7106 3648 221 422 17677 422 18809 30115 422 137 9182 137 8899 422 2800 198 19245 758 3757 3578 168 17328 24089 5107 17935 13004 30113 205 2266 26624 19478 30113 273 205 145 17685 1564 24969 862 2266 26624 19478 2526 30107 546 165 6841 168 111 705 532 188 19038 12483 131 25791 121 580 11768 422 1074 147 547 1995 121 1323 11107 137 111 3843 3810 334 256 3111 147 111 25791 30113 5555 205 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 28251 131 111 12483 28881 1111 190 2599 11714 121 111 11768 131 740 8271 142 205 103 3131 267 8219 137 111 1578 5836 8026 214 2033 131 12483 11639 1078 4032 121 2781 2599 25791 30113 781 111 1275 5129 6186 147 5234 5248 165 6841 3872 205 12498 4834 1578 131 17520 3456 14577 9386 147 874 8483 137 1128 5498 220 1214 1814 111 16376 13331 646 267 111 12026 131 3435 3970 422 3716 214 3307 1578 422 131 1052 334 7839 142 207 153 1365 9793 4178 6687 205 422 14751 7519 1813 414 16145 19639 13004 30113 145 21344 9918 546 422 234 24813 1611 17811 153 557 1869 4393 4554 3158 648 3061 30113 422 17328 24089 5107 17935 13004 30113 145 6154 11629 546 137 12439 7780 200 8474 840 200 10703 187 205 422 267 111 755 5808 30113 205 8058 137 23022 30113 267 111 11107 131 3435 3689 422 1363 3167 24445 288 241 469 1058 121 10519 119 7435 422 4962 7106 3648 221 422 17677 422 18809 30115 422 137 9182 137 8899 422 2800 198 19245 758 3757 3578 168 17328 24089 5107 17935 13004 30113 205 2266 26624 19478 30113 273 205 145 17685 1564 24969 862 2266 26624 19478 2526 30107 546 165 6841 168 111 705 532 188 19038 12483 131 25791 121 580 11768 422 1074 147 547 1995 121 1323 11107 137 111 3843 3810 334 256 3111 147 111 25791 30113 5555 205 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 10000 of 20252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 10000 of 20252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 20000 of 20252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 20000 of 20252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used for model gpu 0\n",
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from study_type_multi\\model.ckpt-2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from study_type_multi\\model.ckpt-2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared train from end\n",
      "INFO:tensorflow:Writing example 0 of 888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] impact of promoting sustainable agriculture in born ##o ( pros ##ab ) program on adoption of improved crop varieties in born ##o state of nigeria [SEP] this paper evaluates the impact of the intervention of promoting sustainable agriculture in born ##o ( pros ##ab ) project on the live ##li ##hood ##s of farming households in born ##o state , nigeria . specifically , the paper identifies and provides information on farmers ' adoption of improved crop varieties introduced by pros ##ab ; measures their adoption rates , and analyzes the factors that affect the probability and intensity of adoption of the crop varieties . a multi - stage sampling procedure was used to select respondents in three agro ##ec ##ological zones ( ae ##zs ) - the northern guinea sav ##anna ( ng ##s ) , southern guinea sav ##anna ( sg ##s ) , and the sah ##elian sav ##anna ( ss ) , and a structured questionnaire was used to collect data on their socioeconomic characteristics and adoption parameters in the 2009 period . data were analyzed using descriptive statistics and the to ##bit regression techniques . the paper demonstrates how adoption and use decisions were significantly influenced by education , membership in associations , and yield in the ng ##s ; yield and labor in the sg ##s , and age and labor in the ss , implying that these variables , and particularly yield , are important for consideration by development agencies who are interested in improving agricultural production [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] impact of promoting sustainable agriculture in born ##o ( pros ##ab ) program on adoption of improved crop varieties in born ##o state of nigeria [SEP] this paper evaluates the impact of the intervention of promoting sustainable agriculture in born ##o ( pros ##ab ) project on the live ##li ##hood ##s of farming households in born ##o state , nigeria . specifically , the paper identifies and provides information on farmers ' adoption of improved crop varieties introduced by pros ##ab ; measures their adoption rates , and analyzes the factors that affect the probability and intensity of adoption of the crop varieties . a multi - stage sampling procedure was used to select respondents in three agro ##ec ##ological zones ( ae ##zs ) - the northern guinea sav ##anna ( ng ##s ) , southern guinea sav ##anna ( sg ##s ) , and the sah ##elian sav ##anna ( ss ) , and a structured questionnaire was used to collect data on their socioeconomic characteristics and adoption parameters in the 2009 period . data were analyzed using descriptive statistics and the to ##bit regression techniques . the paper demonstrates how adoption and use decisions were significantly influenced by education , membership in associations , and yield in the ng ##s ; yield and labor in the sg ##s , and age and labor in the ss , implying that these variables , and particularly yield , are important for consideration by development agencies who are interested in improving agricultural production [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 2141 131 10302 10886 9879 121 9856 30112 145 12000 201 546 1618 191 9714 131 3007 8501 13600 121 9856 30112 1098 131 17964 103 238 1203 18224 111 2141 131 111 3832 131 10302 10886 9879 121 9856 30112 145 12000 201 546 2575 191 111 6489 3206 4402 30113 131 19323 9654 121 9856 30112 1098 422 17964 205 3697 422 111 1203 12369 137 2315 776 191 12288 2505 9714 131 3007 8501 13600 3376 214 12000 201 1814 2554 547 9714 1975 422 137 21127 111 1391 198 2606 111 2064 137 3165 131 9714 131 111 8501 13600 205 106 869 579 2410 3597 2272 241 501 147 5470 6634 121 874 22362 155 872 10166 145 8674 22882 546 579 111 7926 16616 7110 25215 145 3312 30113 546 422 7663 16616 7110 25215 145 10410 30113 546 422 137 111 16905 14005 7110 25215 145 3712 546 422 137 106 7908 5692 241 501 147 9921 453 191 547 13044 2087 137 9714 1496 121 111 4758 1275 205 453 267 2549 487 10363 4530 137 111 147 2386 3089 2190 205 111 1203 7499 539 9714 137 626 5207 267 1357 6452 214 2870 422 9610 121 6225 422 137 2210 121 111 3312 30113 1814 2210 137 2193 121 111 10410 30113 422 137 1407 137 2193 121 111 3712 422 14466 198 407 1762 422 137 3220 2210 422 220 1060 168 5976 214 1120 11934 975 220 7301 121 6017 7675 1865 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 2141 131 10302 10886 9879 121 9856 30112 145 12000 201 546 1618 191 9714 131 3007 8501 13600 121 9856 30112 1098 131 17964 103 238 1203 18224 111 2141 131 111 3832 131 10302 10886 9879 121 9856 30112 145 12000 201 546 2575 191 111 6489 3206 4402 30113 131 19323 9654 121 9856 30112 1098 422 17964 205 3697 422 111 1203 12369 137 2315 776 191 12288 2505 9714 131 3007 8501 13600 3376 214 12000 201 1814 2554 547 9714 1975 422 137 21127 111 1391 198 2606 111 2064 137 3165 131 9714 131 111 8501 13600 205 106 869 579 2410 3597 2272 241 501 147 5470 6634 121 874 22362 155 872 10166 145 8674 22882 546 579 111 7926 16616 7110 25215 145 3312 30113 546 422 7663 16616 7110 25215 145 10410 30113 546 422 137 111 16905 14005 7110 25215 145 3712 546 422 137 106 7908 5692 241 501 147 9921 453 191 547 13044 2087 137 9714 1496 121 111 4758 1275 205 453 267 2549 487 10363 4530 137 111 147 2386 3089 2190 205 111 1203 7499 539 9714 137 626 5207 267 1357 6452 214 2870 422 9610 121 6225 422 137 2210 121 111 3312 30113 1814 2210 137 2193 121 111 10410 30113 422 137 1407 137 2193 121 111 3712 422 14466 198 407 1762 422 137 3220 2210 422 220 1060 168 5976 214 1120 11934 975 220 7301 121 6017 7675 1865 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] exploring indigenous soil knowledge in go ##ure ##l yo ##ba , sen ##eg ##al : two years as a peace corp ##s volunteer [SEP] through my participation in the master ##s international program at the university of california , davis ( uc ##d ) and during my service as a sustainable agriculture peace corp ##s volunteer in the village of go ##ure ##l yo ##ba , sen ##eg ##al , i explored the nature of indigenous soil knowledge to better understand how farmers living at a subs ##istence level recognize soil resources and whether this knowledge is used to make crop management decisions . a two - part investigation was conducted , first , in the village where farmers were interviewed and second , back in the usa where soils were analysed . village ##rs recognize seven main types of soil . these are fa ##hr ##o , bal ##ee ##ri , ken ##yer ##i , seen ##o , ka ##hran - ka ##hran , wen ##do ##u , and ka ##hr ##el , which are differentiated by colour , texture , drainage class , and vegetation . the first four are agricultural soils , and the village ##rs know the crops best suited to each soil . the soils of go ##ure ##l yo ##ba were found to have low fertility with average total n ( for samples tested , n = 16 ) of 0 . 25 g kg ##1 . average total c and extract ##able p was 2 . 9 [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] exploring indigenous soil knowledge in go ##ure ##l yo ##ba , sen ##eg ##al : two years as a peace corp ##s volunteer [SEP] through my participation in the master ##s international program at the university of california , davis ( uc ##d ) and during my service as a sustainable agriculture peace corp ##s volunteer in the village of go ##ure ##l yo ##ba , sen ##eg ##al , i explored the nature of indigenous soil knowledge to better understand how farmers living at a subs ##istence level recognize soil resources and whether this knowledge is used to make crop management decisions . a two - part investigation was conducted , first , in the village where farmers were interviewed and second , back in the usa where soils were analysed . village ##rs recognize seven main types of soil . these are fa ##hr ##o , bal ##ee ##ri , ken ##yer ##i , seen ##o , ka ##hran - ka ##hran , wen ##do ##u , and ka ##hr ##el , which are differentiated by colour , texture , drainage class , and vegetation . the first four are agricultural soils , and the village ##rs know the crops best suited to each soil . the soils of go ##ure ##l yo ##ba were found to have low fertility with average total n ( for samples tested , n = 16 ) of 0 . 25 g kg ##1 . average total c and extract ##able p was 2 . 9 [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 12038 16508 3115 1767 121 796 221 30115 1778 3331 422 1311 4980 120 862 502 1320 188 106 20244 7241 30113 23069 103 833 1536 6163 121 111 7945 30113 2565 1618 235 111 1224 131 6739 422 12763 145 6359 30118 546 137 781 1536 2289 188 106 10886 9879 20244 7241 30113 23069 121 111 17719 131 796 221 30115 1778 3331 422 1311 4980 120 422 259 7293 111 2540 131 16508 3115 1767 147 1883 2008 539 12288 5404 235 106 1763 28674 615 8974 3115 2965 137 1681 238 1767 165 501 147 2113 8501 1837 5207 205 106 502 579 1188 4534 241 2728 422 705 422 121 111 17719 582 12288 267 17985 137 971 422 1542 121 111 2394 582 10511 267 6814 205 17719 2771 8974 5152 936 1910 131 3115 205 407 220 2016 2506 30112 422 2679 1900 212 422 10248 6883 30109 422 2187 30112 422 5656 25178 579 5656 25178 422 11592 2904 30120 422 137 5656 2506 154 422 334 220 9923 214 9389 422 8690 422 12808 844 422 137 9723 205 111 705 1379 220 7675 10511 422 137 111 17719 2771 871 111 12760 2172 13022 147 535 3115 205 111 10511 131 796 221 30115 1778 3331 267 797 147 360 629 12508 190 1568 1114 146 145 168 1488 2733 422 146 275 1107 546 131 244 205 1552 159 5036 30130 205 1568 1114 115 137 4070 318 118 241 170 205 514 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 12038 16508 3115 1767 121 796 221 30115 1778 3331 422 1311 4980 120 862 502 1320 188 106 20244 7241 30113 23069 103 833 1536 6163 121 111 7945 30113 2565 1618 235 111 1224 131 6739 422 12763 145 6359 30118 546 137 781 1536 2289 188 106 10886 9879 20244 7241 30113 23069 121 111 17719 131 796 221 30115 1778 3331 422 1311 4980 120 422 259 7293 111 2540 131 16508 3115 1767 147 1883 2008 539 12288 5404 235 106 1763 28674 615 8974 3115 2965 137 1681 238 1767 165 501 147 2113 8501 1837 5207 205 106 502 579 1188 4534 241 2728 422 705 422 121 111 17719 582 12288 267 17985 137 971 422 1542 121 111 2394 582 10511 267 6814 205 17719 2771 8974 5152 936 1910 131 3115 205 407 220 2016 2506 30112 422 2679 1900 212 422 10248 6883 30109 422 2187 30112 422 5656 25178 579 5656 25178 422 11592 2904 30120 422 137 5656 2506 154 422 334 220 9923 214 9389 422 8690 422 12808 844 422 137 9723 205 111 705 1379 220 7675 10511 422 137 111 17719 2771 871 111 12760 2172 13022 147 535 3115 205 111 10511 131 796 221 30115 1778 3331 267 797 147 360 629 12508 190 1568 1114 146 145 168 1488 2733 422 146 275 1107 546 131 244 205 1552 159 5036 30130 205 1568 1114 115 137 4070 318 118 241 170 205 514 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] farm households ' perception on climate change and adaptation practices a case from mountain district of ne ##pal [SEP] purpose - this paper aims to assess the farming community ' s perception on important parameters of climate change and identify major practices and technologies adopted to mitigate the impacts of climate change and their determinants in mountain district of ne ##pal . being an agr ##arian economy and dependent on mon ##soon rain , the impact of climate change on agricultural productivity and production has been experienced . different adaptation strategies have been adopted by the communities to cope with the consequences of climate change . design / methodology / approach - four village development committees ( vd ##cs ) situated in the buffer zone of lang ##tan ##g national park of ras ##uw ##a district representing rice , wheat , maize and potato production area was pur ##pos ##ive ##ly selected for this study . a two - stage sampling technique was adopted for data and information collection . thirty households from each vd ##c were randomly selected . data on the socio - economic and climate change perception were collected using structured questionnaire . a binary logistic regression technique was used to identify the determinants of climate change adaptation technologies and practices . findings - the farmers ' decisions whether to adopt climate change adaptation technologies are governed by the size of land ##holding , perceived threat of climate change to food security , education level and gender of the interview ##ee [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] farm households ' perception on climate change and adaptation practices a case from mountain district of ne ##pal [SEP] purpose - this paper aims to assess the farming community ' s perception on important parameters of climate change and identify major practices and technologies adopted to mitigate the impacts of climate change and their determinants in mountain district of ne ##pal . being an agr ##arian economy and dependent on mon ##soon rain , the impact of climate change on agricultural productivity and production has been experienced . different adaptation strategies have been adopted by the communities to cope with the consequences of climate change . design / methodology / approach - four village development committees ( vd ##cs ) situated in the buffer zone of lang ##tan ##g national park of ras ##uw ##a district representing rice , wheat , maize and potato production area was pur ##pos ##ive ##ly selected for this study . a two - stage sampling technique was adopted for data and information collection . thirty households from each vd ##c were randomly selected . data on the socio - economic and climate change perception were collected using structured questionnaire . a binary logistic regression technique was used to identify the determinants of climate change adaptation technologies and practices . findings - the farmers ' decisions whether to adopt climate change adaptation technologies are governed by the size of land ##holding , perceived threat of climate change to food security , education level and gender of the interview ##ee [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 5947 9654 2505 6028 191 6002 1477 137 5846 5423 106 820 263 12273 10689 131 287 11014 103 3559 579 238 1203 7415 147 1285 111 19323 2928 2505 112 6028 191 1060 1496 131 6002 1477 137 2743 1626 5423 137 4198 6018 147 16735 111 7681 131 6002 1477 137 547 10687 121 12273 10689 131 287 11014 205 1558 130 4545 9371 7425 137 3644 191 775 27484 7112 422 111 2141 131 6002 1477 191 7675 7381 137 1865 434 528 5770 205 643 5846 3236 360 528 6018 214 111 5904 147 14649 190 111 6091 131 6002 1477 205 899 1352 4964 1352 1139 579 1379 17719 1120 23073 145 15009 1647 546 15657 121 111 3520 5493 131 1973 16161 30123 2227 7713 131 9461 17553 30110 10689 5674 8058 422 10094 422 13156 137 16480 1865 1590 241 1434 451 1090 179 2350 168 238 527 205 106 502 579 2410 3597 2358 241 6018 168 453 137 776 3445 205 12498 9654 263 535 15009 30116 267 5007 2350 205 453 191 111 8001 579 3587 137 6002 1477 6028 267 2760 487 7908 5692 205 106 5067 7312 3089 2358 241 501 147 2743 111 10687 131 6002 1477 5846 4198 137 5423 205 2116 579 111 12288 2505 5207 1681 147 9265 6002 1477 5846 4198 220 17384 214 111 1243 131 2882 28696 422 5545 8057 131 6002 1477 147 2599 3594 422 2870 615 137 4703 131 111 4489 1900 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 5947 9654 2505 6028 191 6002 1477 137 5846 5423 106 820 263 12273 10689 131 287 11014 103 3559 579 238 1203 7415 147 1285 111 19323 2928 2505 112 6028 191 1060 1496 131 6002 1477 137 2743 1626 5423 137 4198 6018 147 16735 111 7681 131 6002 1477 137 547 10687 121 12273 10689 131 287 11014 205 1558 130 4545 9371 7425 137 3644 191 775 27484 7112 422 111 2141 131 6002 1477 191 7675 7381 137 1865 434 528 5770 205 643 5846 3236 360 528 6018 214 111 5904 147 14649 190 111 6091 131 6002 1477 205 899 1352 4964 1352 1139 579 1379 17719 1120 23073 145 15009 1647 546 15657 121 111 3520 5493 131 1973 16161 30123 2227 7713 131 9461 17553 30110 10689 5674 8058 422 10094 422 13156 137 16480 1865 1590 241 1434 451 1090 179 2350 168 238 527 205 106 502 579 2410 3597 2358 241 6018 168 453 137 776 3445 205 12498 9654 263 535 15009 30116 267 5007 2350 205 453 191 111 8001 579 3587 137 6002 1477 6028 267 2760 487 7908 5692 205 106 5067 7312 3089 2358 241 501 147 2743 111 10687 131 6002 1477 5846 4198 137 5423 205 2116 579 111 12288 2505 5207 1681 147 9265 6002 1477 5846 4198 220 17384 214 111 1243 131 2882 28696 422 5545 8057 131 6002 1477 147 2599 3594 422 2870 615 137 4703 131 111 4489 1900 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] association of small ##holder dairy farmers management and milk ##ing practices with bacterial quality of milk in mb ##ey ##a , tanzania [SEP] the study determine the association between some of management ##s and milk ##ing practices with bacterial counts in dairy herd among small holder dairy farmers of mb ##ey ##a and mb ##oz ##i districts of mb ##ey ##a , tanzania . a cross - sectional study was conducted with the aim of assessing housing structures , condition and management , milk ##ing procedure and milk handling in the study area . a total of 192 raw milk samples were collected from farmers herd ##s with at least one lact ##ating dairy cow . samples were tested for total bacteria count ( tb ##c ) , total coli ##form counts ( tcc ) and total staphylococcus counts ( tsc ) using standard procedures . all respondents practiced hand milk ##ing and 96 . 9 % of the respondents washed hands before milk ##ing . about 57 . 8 % of the respondents used same tow ##el to dry the ud ##der of all milk ##ing cows in the herd per milk ##ing time . furthermore , 42 . 2 % of the respondents used individual tow ##el for drying ud ##der and tea ##t . none of the respondents used pre milk ##ing , post milk ##ing dip ##ping or dry cow therapy . milk from cows kept in bar ##ns made from concrete floor had ( p < 0 . 00 [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] association of small ##holder dairy farmers management and milk ##ing practices with bacterial quality of milk in mb ##ey ##a , tanzania [SEP] the study determine the association between some of management ##s and milk ##ing practices with bacterial counts in dairy herd among small holder dairy farmers of mb ##ey ##a and mb ##oz ##i districts of mb ##ey ##a , tanzania . a cross - sectional study was conducted with the aim of assessing housing structures , condition and management , milk ##ing procedure and milk handling in the study area . a total of 192 raw milk samples were collected from farmers herd ##s with at least one lact ##ating dairy cow . samples were tested for total bacteria count ( tb ##c ) , total coli ##form counts ( tcc ) and total staphylococcus counts ( tsc ) using standard procedures . all respondents practiced hand milk ##ing and 96 . 9 % of the respondents washed hands before milk ##ing . about 57 . 8 % of the respondents used same tow ##el to dry the ud ##der of all milk ##ing cows in the herd per milk ##ing time . furthermore , 42 . 2 % of the respondents used individual tow ##el for drying ud ##der and tea ##t . none of the respondents used pre milk ##ing , post milk ##ing dip ##ping or dry cow therapy . milk from cows kept in bar ##ns made from concrete floor had ( p < 0 . 00 [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 2274 131 952 13643 14874 12288 1837 137 7435 140 5423 190 4667 1671 131 7435 121 6369 9257 30110 422 27281 103 111 527 2100 111 2274 467 693 131 1837 30113 137 7435 140 5423 190 4667 7274 121 14874 24593 1247 952 19347 14874 12288 131 6369 9257 30110 137 6369 9627 30109 17618 131 6369 9257 30110 422 27281 205 106 2057 579 27554 527 241 2728 190 111 2579 131 7836 10792 2484 422 803 137 1837 422 7435 140 2272 137 7435 8731 121 111 527 1590 205 106 1114 131 13054 6908 7435 1488 267 2760 263 12288 24593 30113 190 235 1823 482 6779 560 14874 8506 205 1488 267 2733 168 1114 4738 2232 145 6226 30116 546 422 1114 5703 321 7274 145 28331 546 137 1114 16808 7274 145 26800 546 487 1235 3622 205 355 6634 28371 1500 7435 140 137 7380 205 514 1863 131 111 6634 6069 12053 1548 7435 140 205 1011 5309 205 493 1863 131 111 6634 501 855 10755 154 147 5965 111 8722 343 131 355 7435 140 15627 121 111 24593 309 7435 140 532 205 2403 422 4637 205 170 1863 131 111 6634 501 1161 10755 154 168 15211 8722 343 137 15297 30108 205 4580 131 111 6634 501 382 7435 140 422 1422 7435 140 6332 5354 234 5965 8506 2223 205 7435 263 15627 6890 121 2261 1290 1827 263 9209 10298 883 145 118 962 244 205 3641 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 2274 131 952 13643 14874 12288 1837 137 7435 140 5423 190 4667 1671 131 7435 121 6369 9257 30110 422 27281 103 111 527 2100 111 2274 467 693 131 1837 30113 137 7435 140 5423 190 4667 7274 121 14874 24593 1247 952 19347 14874 12288 131 6369 9257 30110 137 6369 9627 30109 17618 131 6369 9257 30110 422 27281 205 106 2057 579 27554 527 241 2728 190 111 2579 131 7836 10792 2484 422 803 137 1837 422 7435 140 2272 137 7435 8731 121 111 527 1590 205 106 1114 131 13054 6908 7435 1488 267 2760 263 12288 24593 30113 190 235 1823 482 6779 560 14874 8506 205 1488 267 2733 168 1114 4738 2232 145 6226 30116 546 422 1114 5703 321 7274 145 28331 546 137 1114 16808 7274 145 26800 546 487 1235 3622 205 355 6634 28371 1500 7435 140 137 7380 205 514 1863 131 111 6634 6069 12053 1548 7435 140 205 1011 5309 205 493 1863 131 111 6634 501 855 10755 154 147 5965 111 8722 343 131 355 7435 140 15627 121 111 24593 309 7435 140 532 205 2403 422 4637 205 170 1863 131 111 6634 501 1161 10755 154 168 15211 8722 343 137 15297 30108 205 4580 131 111 6634 501 382 7435 140 422 1422 7435 140 6332 5354 234 5965 8506 2223 205 7435 263 15627 6890 121 2261 1290 1827 263 9209 10298 883 145 118 962 244 205 3641 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] assessing maize farmers ' adaptation strategies to climate change and variability in ghana [SEP] this study examined the adaptation strategies of maize farmers to climate change and variability in the eastern region of ghana using primary data collected from 150 maize farming households by the administration of structured questionnaires . the results of the multin ##omial logit regression revealed that rainfall perception , access to credit , and farming experience significantly influenced the adoption of recommended agricultural practices , whereas the adoption of soil - related strategies is influenced by gender and rainfall perception . farming experience and rainfall perception influenced the adoption of improved varieties strategies . this study highlights the need for the development of water resources for maize production in the context of the changing climate . in this respect , the crucial roles of the ghana irrigation development authority , the agricultural extension division of the ministry of food and agriculture , and other international organizations such as the japan international cooperation agency and the world bank regarding the development of irrigation facilities and the associated capacity building of the farmers are very important . finally , the formation of water user associations for the small ##holder farmers regarding the usage and maintenance of the irrigation facilities would be a step in the right direction [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] assessing maize farmers ' adaptation strategies to climate change and variability in ghana [SEP] this study examined the adaptation strategies of maize farmers to climate change and variability in the eastern region of ghana using primary data collected from 150 maize farming households by the administration of structured questionnaires . the results of the multin ##omial logit regression revealed that rainfall perception , access to credit , and farming experience significantly influenced the adoption of recommended agricultural practices , whereas the adoption of soil - related strategies is influenced by gender and rainfall perception . farming experience and rainfall perception influenced the adoption of improved varieties strategies . this study highlights the need for the development of water resources for maize production in the context of the changing climate . in this respect , the crucial roles of the ghana irrigation development authority , the agricultural extension division of the ministry of food and agriculture , and other international organizations such as the japan international cooperation agency and the world bank regarding the development of irrigation facilities and the associated capacity building of the farmers are very important . finally , the formation of water user associations for the small ##holder farmers regarding the usage and maintenance of the irrigation facilities would be a step in the right direction [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 7836 13156 12288 2505 5846 3236 147 6002 1477 137 4638 121 26274 103 238 527 2959 111 5846 3236 131 13156 12288 147 6002 1477 137 4638 121 111 10309 1025 131 26274 487 1916 453 2760 263 5413 13156 19323 9654 214 111 3762 131 7908 11682 205 111 545 131 111 18231 4025 25565 3089 2861 198 12660 6028 422 1899 147 8388 422 137 19323 2899 1357 6452 111 9714 131 5805 7675 5423 422 2255 111 9714 131 3115 579 1482 3236 165 6452 214 4703 137 12660 6028 205 19323 2899 137 12660 6028 6452 111 9714 131 3007 13600 3236 205 238 527 11878 111 965 168 111 1120 131 1506 2965 168 13156 1865 121 111 2220 131 111 5468 6002 205 121 238 2646 422 111 5784 5370 131 111 26274 15055 1120 11226 422 111 7675 3840 5277 131 111 9795 131 2599 137 9879 422 137 494 2565 6938 555 188 111 4838 2565 9226 8118 137 111 2399 5108 3560 111 1120 131 15055 7845 137 111 1111 2900 4736 131 111 12288 220 1248 1060 205 2285 422 111 2256 131 1506 1824 6225 168 111 952 13643 12288 3560 111 6323 137 5365 131 111 15055 7845 989 195 106 1371 121 111 2083 3041 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 7836 13156 12288 2505 5846 3236 147 6002 1477 137 4638 121 26274 103 238 527 2959 111 5846 3236 131 13156 12288 147 6002 1477 137 4638 121 111 10309 1025 131 26274 487 1916 453 2760 263 5413 13156 19323 9654 214 111 3762 131 7908 11682 205 111 545 131 111 18231 4025 25565 3089 2861 198 12660 6028 422 1899 147 8388 422 137 19323 2899 1357 6452 111 9714 131 5805 7675 5423 422 2255 111 9714 131 3115 579 1482 3236 165 6452 214 4703 137 12660 6028 205 19323 2899 137 12660 6028 6452 111 9714 131 3007 13600 3236 205 238 527 11878 111 965 168 111 1120 131 1506 2965 168 13156 1865 121 111 2220 131 111 5468 6002 205 121 238 2646 422 111 5784 5370 131 111 26274 15055 1120 11226 422 111 7675 3840 5277 131 111 9795 131 2599 137 9879 422 137 494 2565 6938 555 188 111 4838 2565 9226 8118 137 111 2399 5108 3560 111 1120 131 15055 7845 137 111 1111 2900 4736 131 111 12288 220 1248 1060 205 2285 422 111 2256 131 1506 1824 6225 168 111 952 13643 12288 3560 111 6323 137 5365 131 111 15055 7845 989 195 106 1371 121 111 2083 3041 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used for model gpu 0\n",
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from study_type_multi\\model.ckpt-2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from study_type_multi\\model.ckpt-2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared train from beginning\n",
      "INFO:tensorflow:Writing example 0 of 888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] impact of promoting sustainable agriculture in born ##o ( pros ##ab ) program on adoption of improved crop varieties in born ##o state of nigeria [SEP] promoting sustainable agriculture in born ##o ( pros ##ab ) project on the live ##li ##hood ##s of farming households in born ##o state , nigeria . specifically , the paper identifies and provides information on farmers ' adoption of improved crop varieties introduced by pros ##ab ; measures their adoption rates , and analyzes the factors that affect the probability and intensity of adoption of the crop varieties . a multi - stage sampling procedure was used to select respondents in three agro ##ec ##ological zones ( ae ##zs ) - the northern guinea sav ##anna ( ng ##s ) , southern guinea sav ##anna ( sg ##s ) , and the sah ##elian sav ##anna ( ss ) , and a structured questionnaire was used to collect data on their socioeconomic characteristics and adoption parameters in the 2009 period . data were analyzed using descriptive statistics and the to ##bit regression techniques . the paper demonstrates how adoption and use decisions were significantly influenced by education , membership in associations , and yield in the ng ##s ; yield and labor in the sg ##s , and age and labor in the ss , implying that these variables , and particularly yield , are important for consideration by development agencies who are interested in improving agricultural production and the live ##li ##hood ##s of the people [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] impact of promoting sustainable agriculture in born ##o ( pros ##ab ) program on adoption of improved crop varieties in born ##o state of nigeria [SEP] promoting sustainable agriculture in born ##o ( pros ##ab ) project on the live ##li ##hood ##s of farming households in born ##o state , nigeria . specifically , the paper identifies and provides information on farmers ' adoption of improved crop varieties introduced by pros ##ab ; measures their adoption rates , and analyzes the factors that affect the probability and intensity of adoption of the crop varieties . a multi - stage sampling procedure was used to select respondents in three agro ##ec ##ological zones ( ae ##zs ) - the northern guinea sav ##anna ( ng ##s ) , southern guinea sav ##anna ( sg ##s ) , and the sah ##elian sav ##anna ( ss ) , and a structured questionnaire was used to collect data on their socioeconomic characteristics and adoption parameters in the 2009 period . data were analyzed using descriptive statistics and the to ##bit regression techniques . the paper demonstrates how adoption and use decisions were significantly influenced by education , membership in associations , and yield in the ng ##s ; yield and labor in the sg ##s , and age and labor in the ss , implying that these variables , and particularly yield , are important for consideration by development agencies who are interested in improving agricultural production and the live ##li ##hood ##s of the people [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 2141 131 10302 10886 9879 121 9856 30112 145 12000 201 546 1618 191 9714 131 3007 8501 13600 121 9856 30112 1098 131 17964 103 10302 10886 9879 121 9856 30112 145 12000 201 546 2575 191 111 6489 3206 4402 30113 131 19323 9654 121 9856 30112 1098 422 17964 205 3697 422 111 1203 12369 137 2315 776 191 12288 2505 9714 131 3007 8501 13600 3376 214 12000 201 1814 2554 547 9714 1975 422 137 21127 111 1391 198 2606 111 2064 137 3165 131 9714 131 111 8501 13600 205 106 869 579 2410 3597 2272 241 501 147 5470 6634 121 874 22362 155 872 10166 145 8674 22882 546 579 111 7926 16616 7110 25215 145 3312 30113 546 422 7663 16616 7110 25215 145 10410 30113 546 422 137 111 16905 14005 7110 25215 145 3712 546 422 137 106 7908 5692 241 501 147 9921 453 191 547 13044 2087 137 9714 1496 121 111 4758 1275 205 453 267 2549 487 10363 4530 137 111 147 2386 3089 2190 205 111 1203 7499 539 9714 137 626 5207 267 1357 6452 214 2870 422 9610 121 6225 422 137 2210 121 111 3312 30113 1814 2210 137 2193 121 111 10410 30113 422 137 1407 137 2193 121 111 3712 422 14466 198 407 1762 422 137 3220 2210 422 220 1060 168 5976 214 1120 11934 975 220 7301 121 6017 7675 1865 137 111 6489 3206 4402 30113 131 111 2325 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 2141 131 10302 10886 9879 121 9856 30112 145 12000 201 546 1618 191 9714 131 3007 8501 13600 121 9856 30112 1098 131 17964 103 10302 10886 9879 121 9856 30112 145 12000 201 546 2575 191 111 6489 3206 4402 30113 131 19323 9654 121 9856 30112 1098 422 17964 205 3697 422 111 1203 12369 137 2315 776 191 12288 2505 9714 131 3007 8501 13600 3376 214 12000 201 1814 2554 547 9714 1975 422 137 21127 111 1391 198 2606 111 2064 137 3165 131 9714 131 111 8501 13600 205 106 869 579 2410 3597 2272 241 501 147 5470 6634 121 874 22362 155 872 10166 145 8674 22882 546 579 111 7926 16616 7110 25215 145 3312 30113 546 422 7663 16616 7110 25215 145 10410 30113 546 422 137 111 16905 14005 7110 25215 145 3712 546 422 137 106 7908 5692 241 501 147 9921 453 191 547 13044 2087 137 9714 1496 121 111 4758 1275 205 453 267 2549 487 10363 4530 137 111 147 2386 3089 2190 205 111 1203 7499 539 9714 137 626 5207 267 1357 6452 214 2870 422 9610 121 6225 422 137 2210 121 111 3312 30113 1814 2210 137 2193 121 111 10410 30113 422 137 1407 137 2193 121 111 3712 422 14466 198 407 1762 422 137 3220 2210 422 220 1060 168 5976 214 1120 11934 975 220 7301 121 6017 7675 1865 137 111 6489 3206 4402 30113 131 111 2325 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] exploring indigenous soil knowledge in go ##ure ##l yo ##ba , sen ##eg ##al : two years as a peace corp ##s volunteer [SEP] . a two - part investigation was conducted , first , in the village where farmers were interviewed and second , back in the usa where soils were analysed . village ##rs recognize seven main types of soil . these are fa ##hr ##o , bal ##ee ##ri , ken ##yer ##i , seen ##o , ka ##hran - ka ##hran , wen ##do ##u , and ka ##hr ##el , which are differentiated by colour , texture , drainage class , and vegetation . the first four are agricultural soils , and the village ##rs know the crops best suited to each soil . the soils of go ##ure ##l yo ##ba were found to have low fertility with average total n ( for samples tested , n = 16 ) of 0 . 25 g kg ##1 . average total c and extract ##able p was 2 . 9 g kg ##1 and 2 . 6 kg ha ##1 , respectively . farmers in go ##ure ##l yo ##ba have good working knowledge of their soil resources , but factors such as risk management behaviours and climate change prevent them from always using this knowledge to make the best crop management decisions . simple steps such as concentr ##ating manure along crop rows are recommended to build on what they already understand about soils to improve crop performance [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] exploring indigenous soil knowledge in go ##ure ##l yo ##ba , sen ##eg ##al : two years as a peace corp ##s volunteer [SEP] . a two - part investigation was conducted , first , in the village where farmers were interviewed and second , back in the usa where soils were analysed . village ##rs recognize seven main types of soil . these are fa ##hr ##o , bal ##ee ##ri , ken ##yer ##i , seen ##o , ka ##hran - ka ##hran , wen ##do ##u , and ka ##hr ##el , which are differentiated by colour , texture , drainage class , and vegetation . the first four are agricultural soils , and the village ##rs know the crops best suited to each soil . the soils of go ##ure ##l yo ##ba were found to have low fertility with average total n ( for samples tested , n = 16 ) of 0 . 25 g kg ##1 . average total c and extract ##able p was 2 . 9 g kg ##1 and 2 . 6 kg ha ##1 , respectively . farmers in go ##ure ##l yo ##ba have good working knowledge of their soil resources , but factors such as risk management behaviours and climate change prevent them from always using this knowledge to make the best crop management decisions . simple steps such as concentr ##ating manure along crop rows are recommended to build on what they already understand about soils to improve crop performance [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 12038 16508 3115 1767 121 796 221 30115 1778 3331 422 1311 4980 120 862 502 1320 188 106 20244 7241 30113 23069 103 205 106 502 579 1188 4534 241 2728 422 705 422 121 111 17719 582 12288 267 17985 137 971 422 1542 121 111 2394 582 10511 267 6814 205 17719 2771 8974 5152 936 1910 131 3115 205 407 220 2016 2506 30112 422 2679 1900 212 422 10248 6883 30109 422 2187 30112 422 5656 25178 579 5656 25178 422 11592 2904 30120 422 137 5656 2506 154 422 334 220 9923 214 9389 422 8690 422 12808 844 422 137 9723 205 111 705 1379 220 7675 10511 422 137 111 17719 2771 871 111 12760 2172 13022 147 535 3115 205 111 10511 131 796 221 30115 1778 3331 267 797 147 360 629 12508 190 1568 1114 146 145 168 1488 2733 422 146 275 1107 546 131 244 205 1552 159 5036 30130 205 1568 1114 115 137 4070 318 118 241 170 205 514 159 5036 30130 137 170 205 370 5036 325 30130 422 1222 205 12288 121 796 221 30115 1778 3331 360 1846 3630 1767 131 547 3115 2965 422 563 1391 555 188 1265 1837 13765 137 6002 1477 3363 1445 263 3259 487 238 1767 147 2113 111 2172 8501 1837 5207 205 2177 3565 555 188 1112 560 26096 2252 8501 10392 220 5805 147 2875 191 1792 698 3440 2008 1011 10511 147 1658 8501 1150 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 12038 16508 3115 1767 121 796 221 30115 1778 3331 422 1311 4980 120 862 502 1320 188 106 20244 7241 30113 23069 103 205 106 502 579 1188 4534 241 2728 422 705 422 121 111 17719 582 12288 267 17985 137 971 422 1542 121 111 2394 582 10511 267 6814 205 17719 2771 8974 5152 936 1910 131 3115 205 407 220 2016 2506 30112 422 2679 1900 212 422 10248 6883 30109 422 2187 30112 422 5656 25178 579 5656 25178 422 11592 2904 30120 422 137 5656 2506 154 422 334 220 9923 214 9389 422 8690 422 12808 844 422 137 9723 205 111 705 1379 220 7675 10511 422 137 111 17719 2771 871 111 12760 2172 13022 147 535 3115 205 111 10511 131 796 221 30115 1778 3331 267 797 147 360 629 12508 190 1568 1114 146 145 168 1488 2733 422 146 275 1107 546 131 244 205 1552 159 5036 30130 205 1568 1114 115 137 4070 318 118 241 170 205 514 159 5036 30130 137 170 205 370 5036 325 30130 422 1222 205 12288 121 796 221 30115 1778 3331 360 1846 3630 1767 131 547 3115 2965 422 563 1391 555 188 1265 1837 13765 137 6002 1477 3363 1445 263 3259 487 238 1767 147 2113 111 2172 8501 1837 5207 205 2177 3565 555 188 1112 560 26096 2252 8501 10392 220 5805 147 2875 191 1792 698 3440 2008 1011 10511 147 1658 8501 1150 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] farm households ' perception on climate change and adaptation practices a case from mountain district of ne ##pal [SEP] ##tan ##g national park of ras ##uw ##a district representing rice , wheat , maize and potato production area was pur ##pos ##ive ##ly selected for this study . a two - stage sampling technique was adopted for data and information collection . thirty households from each vd ##c were randomly selected . data on the socio - economic and climate change perception were collected using structured questionnaire . a binary logistic regression technique was used to identify the determinants of climate change adaptation technologies and practices . findings - the farmers ' decisions whether to adopt climate change adaptation technologies are governed by the size of land ##holding , perceived threat of climate change to food security , education level and gender of the interview ##ee , perception on the increased incidence of drought ##s during rain ##y season and income received from the off - farm sources . in a community where agricultural activity is the dominant means of living , adaptive strategies help to increase the capacity of a farming system to survive external shocks and cope with the consequences . original ##ity / value - the assessment of farm - level adaptation strategies and factors influencing their adoption decision is important to formulate policies and design programs . this will also help to recognize adaptation as a tool for managing a variety of risks associated with climate change in agricultural sector [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] farm households ' perception on climate change and adaptation practices a case from mountain district of ne ##pal [SEP] ##tan ##g national park of ras ##uw ##a district representing rice , wheat , maize and potato production area was pur ##pos ##ive ##ly selected for this study . a two - stage sampling technique was adopted for data and information collection . thirty households from each vd ##c were randomly selected . data on the socio - economic and climate change perception were collected using structured questionnaire . a binary logistic regression technique was used to identify the determinants of climate change adaptation technologies and practices . findings - the farmers ' decisions whether to adopt climate change adaptation technologies are governed by the size of land ##holding , perceived threat of climate change to food security , education level and gender of the interview ##ee , perception on the increased incidence of drought ##s during rain ##y season and income received from the off - farm sources . in a community where agricultural activity is the dominant means of living , adaptive strategies help to increase the capacity of a farming system to survive external shocks and cope with the consequences . original ##ity / value - the assessment of farm - level adaptation strategies and factors influencing their adoption decision is important to formulate policies and design programs . this will also help to recognize adaptation as a tool for managing a variety of risks associated with climate change in agricultural sector [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 5947 9654 2505 6028 191 6002 1477 137 5846 5423 106 820 263 12273 10689 131 287 11014 103 16161 30123 2227 7713 131 9461 17553 30110 10689 5674 8058 422 10094 422 13156 137 16480 1865 1590 241 1434 451 1090 179 2350 168 238 527 205 106 502 579 2410 3597 2358 241 6018 168 453 137 776 3445 205 12498 9654 263 535 15009 30116 267 5007 2350 205 453 191 111 8001 579 3587 137 6002 1477 6028 267 2760 487 7908 5692 205 106 5067 7312 3089 2358 241 501 147 2743 111 10687 131 6002 1477 5846 4198 137 5423 205 2116 579 111 12288 2505 5207 1681 147 9265 6002 1477 5846 4198 220 17384 214 111 1243 131 2882 28696 422 5545 8057 131 6002 1477 147 2599 3594 422 2870 615 137 4703 131 111 4489 1900 422 6028 191 111 1175 3970 131 13509 30113 781 7112 30126 7843 137 5021 2072 263 111 1874 579 5947 3117 205 121 106 2928 582 7675 1071 165 111 5521 2033 131 5404 422 4640 3236 2279 147 1242 111 2900 131 106 19323 429 147 14413 3458 13245 137 14649 190 111 6091 205 2592 208 1352 973 579 111 2790 131 5947 579 615 5846 3236 137 1391 12657 547 9714 2015 165 1060 147 15835 5691 137 899 3996 205 238 650 469 2279 147 8974 5846 188 106 3130 168 11170 106 3835 131 6733 1111 190 6002 1477 121 7675 5672 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 5947 9654 2505 6028 191 6002 1477 137 5846 5423 106 820 263 12273 10689 131 287 11014 103 16161 30123 2227 7713 131 9461 17553 30110 10689 5674 8058 422 10094 422 13156 137 16480 1865 1590 241 1434 451 1090 179 2350 168 238 527 205 106 502 579 2410 3597 2358 241 6018 168 453 137 776 3445 205 12498 9654 263 535 15009 30116 267 5007 2350 205 453 191 111 8001 579 3587 137 6002 1477 6028 267 2760 487 7908 5692 205 106 5067 7312 3089 2358 241 501 147 2743 111 10687 131 6002 1477 5846 4198 137 5423 205 2116 579 111 12288 2505 5207 1681 147 9265 6002 1477 5846 4198 220 17384 214 111 1243 131 2882 28696 422 5545 8057 131 6002 1477 147 2599 3594 422 2870 615 137 4703 131 111 4489 1900 422 6028 191 111 1175 3970 131 13509 30113 781 7112 30126 7843 137 5021 2072 263 111 1874 579 5947 3117 205 121 106 2928 582 7675 1071 165 111 5521 2033 131 5404 422 4640 3236 2279 147 1242 111 2900 131 106 19323 429 147 14413 3458 13245 137 14649 190 111 6091 205 2592 208 1352 973 579 111 2790 131 5947 579 615 5846 3236 137 1391 12657 547 9714 2015 165 1060 147 15835 5691 137 899 3996 205 238 650 469 2279 147 8974 5846 188 106 3130 168 11170 106 3835 131 6733 1111 190 6002 1477 121 7675 5672 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] association of small ##holder dairy farmers management and milk ##ing practices with bacterial quality of milk in mb ##ey ##a , tanzania [SEP] ##el for drying ud ##der and tea ##t . none of the respondents used pre milk ##ing , post milk ##ing dip ##ping or dry cow therapy . milk from cows kept in bar ##ns made from concrete floor had ( p < 0 . 00 ##1 ) lower tb ##c and tsc . frequency of cleaning of dairy bar ##n ( p < 0 . 05 ) influenced the tb ##c , tcc and tsc . lack of fore milk ##ing ( p < 0 . 05 ) associated with higher tb ##c . furthermore , water source ( p < 0 . 05 ) influenced tb ##c and tcc . milk ##ing practices which includes washing of hands , ud ##der and tea ##t , dry of tea ##ts using individual tow ##el per cow and followed by fore milk ##ing yielded ( p < 0 . 05 ) lower bacterial count than other practices . similarly , cow bar ##ns whose floor ##s were made from concrete and cleaned twice or more daily had clean cow which produce milk with lower bacterial counts than cows kept on floor ##s made from timber and soil . the results reveal the need for training farmers in good agricultural practices . this would contribute to achieving better quality milk , and ensure the sustainability of the sector in the study area [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] association of small ##holder dairy farmers management and milk ##ing practices with bacterial quality of milk in mb ##ey ##a , tanzania [SEP] ##el for drying ud ##der and tea ##t . none of the respondents used pre milk ##ing , post milk ##ing dip ##ping or dry cow therapy . milk from cows kept in bar ##ns made from concrete floor had ( p < 0 . 00 ##1 ) lower tb ##c and tsc . frequency of cleaning of dairy bar ##n ( p < 0 . 05 ) influenced the tb ##c , tcc and tsc . lack of fore milk ##ing ( p < 0 . 05 ) associated with higher tb ##c . furthermore , water source ( p < 0 . 05 ) influenced tb ##c and tcc . milk ##ing practices which includes washing of hands , ud ##der and tea ##t , dry of tea ##ts using individual tow ##el per cow and followed by fore milk ##ing yielded ( p < 0 . 05 ) lower bacterial count than other practices . similarly , cow bar ##ns whose floor ##s were made from concrete and cleaned twice or more daily had clean cow which produce milk with lower bacterial counts than cows kept on floor ##s made from timber and soil . the results reveal the need for training farmers in good agricultural practices . this would contribute to achieving better quality milk , and ensure the sustainability of the sector in the study area [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 2274 131 952 13643 14874 12288 1837 137 7435 140 5423 190 4667 1671 131 7435 121 6369 9257 30110 422 27281 103 154 168 15211 8722 343 137 15297 30108 205 4580 131 111 6634 501 382 7435 140 422 1422 7435 140 6332 5354 234 5965 8506 2223 205 7435 263 15627 6890 121 2261 1290 1827 263 9209 10298 883 145 118 962 244 205 3641 30130 546 1268 6226 30116 137 26800 205 1610 131 17376 131 14874 2261 30111 145 118 962 244 205 10764 546 6452 111 6226 30116 422 28331 137 26800 205 2596 131 2312 7435 140 145 118 962 244 205 10764 546 1111 190 1001 6226 30116 205 2403 422 1506 1908 145 118 962 244 205 10764 546 6452 6226 30116 137 28331 205 7435 140 5423 334 3753 6907 131 12053 422 8722 343 137 15297 30108 422 5965 131 15297 203 487 1161 10755 154 309 8506 137 2594 214 2312 7435 140 9378 145 118 962 244 205 10764 546 1268 4667 2232 506 494 5423 205 3453 422 8506 2261 1290 3489 10298 30113 267 1827 263 9209 137 23445 6279 234 475 4122 883 7113 8506 334 3299 7435 190 1268 4667 7274 506 15627 6890 191 10298 30113 1827 263 28094 137 3115 205 111 545 2303 111 965 168 2208 12288 121 1846 7675 5423 205 238 989 4362 147 9153 1883 1671 7435 422 137 4207 111 13591 131 111 5672 121 111 527 1590 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 2274 131 952 13643 14874 12288 1837 137 7435 140 5423 190 4667 1671 131 7435 121 6369 9257 30110 422 27281 103 154 168 15211 8722 343 137 15297 30108 205 4580 131 111 6634 501 382 7435 140 422 1422 7435 140 6332 5354 234 5965 8506 2223 205 7435 263 15627 6890 121 2261 1290 1827 263 9209 10298 883 145 118 962 244 205 3641 30130 546 1268 6226 30116 137 26800 205 1610 131 17376 131 14874 2261 30111 145 118 962 244 205 10764 546 6452 111 6226 30116 422 28331 137 26800 205 2596 131 2312 7435 140 145 118 962 244 205 10764 546 1111 190 1001 6226 30116 205 2403 422 1506 1908 145 118 962 244 205 10764 546 6452 6226 30116 137 28331 205 7435 140 5423 334 3753 6907 131 12053 422 8722 343 137 15297 30108 422 5965 131 15297 203 487 1161 10755 154 309 8506 137 2594 214 2312 7435 140 9378 145 118 962 244 205 10764 546 1268 4667 2232 506 494 5423 205 3453 422 8506 2261 1290 3489 10298 30113 267 1827 263 9209 137 23445 6279 234 475 4122 883 7113 8506 334 3299 7435 190 1268 4667 7274 506 15627 6890 191 10298 30113 1827 263 28094 137 3115 205 111 545 2303 111 965 168 2208 12288 121 1846 7675 5423 205 238 989 4362 147 9153 1883 1671 7435 422 137 4207 111 13591 131 111 5672 121 111 527 1590 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] assessing maize farmers ' adaptation strategies to climate change and variability in ghana [SEP] this study examined the adaptation strategies of maize farmers to climate change and variability in the eastern region of ghana using primary data collected from 150 maize farming households by the administration of structured questionnaires . the results of the multin ##omial logit regression revealed that rainfall perception , access to credit , and farming experience significantly influenced the adoption of recommended agricultural practices , whereas the adoption of soil - related strategies is influenced by gender and rainfall perception . farming experience and rainfall perception influenced the adoption of improved varieties strategies . this study highlights the need for the development of water resources for maize production in the context of the changing climate . in this respect , the crucial roles of the ghana irrigation development authority , the agricultural extension division of the ministry of food and agriculture , and other international organizations such as the japan international cooperation agency and the world bank regarding the development of irrigation facilities and the associated capacity building of the farmers are very important . finally , the formation of water user associations for the small ##holder farmers regarding the usage and maintenance of the irrigation facilities would be a step in the right direction [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] assessing maize farmers ' adaptation strategies to climate change and variability in ghana [SEP] this study examined the adaptation strategies of maize farmers to climate change and variability in the eastern region of ghana using primary data collected from 150 maize farming households by the administration of structured questionnaires . the results of the multin ##omial logit regression revealed that rainfall perception , access to credit , and farming experience significantly influenced the adoption of recommended agricultural practices , whereas the adoption of soil - related strategies is influenced by gender and rainfall perception . farming experience and rainfall perception influenced the adoption of improved varieties strategies . this study highlights the need for the development of water resources for maize production in the context of the changing climate . in this respect , the crucial roles of the ghana irrigation development authority , the agricultural extension division of the ministry of food and agriculture , and other international organizations such as the japan international cooperation agency and the world bank regarding the development of irrigation facilities and the associated capacity building of the farmers are very important . finally , the formation of water user associations for the small ##holder farmers regarding the usage and maintenance of the irrigation facilities would be a step in the right direction [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 7836 13156 12288 2505 5846 3236 147 6002 1477 137 4638 121 26274 103 238 527 2959 111 5846 3236 131 13156 12288 147 6002 1477 137 4638 121 111 10309 1025 131 26274 487 1916 453 2760 263 5413 13156 19323 9654 214 111 3762 131 7908 11682 205 111 545 131 111 18231 4025 25565 3089 2861 198 12660 6028 422 1899 147 8388 422 137 19323 2899 1357 6452 111 9714 131 5805 7675 5423 422 2255 111 9714 131 3115 579 1482 3236 165 6452 214 4703 137 12660 6028 205 19323 2899 137 12660 6028 6452 111 9714 131 3007 13600 3236 205 238 527 11878 111 965 168 111 1120 131 1506 2965 168 13156 1865 121 111 2220 131 111 5468 6002 205 121 238 2646 422 111 5784 5370 131 111 26274 15055 1120 11226 422 111 7675 3840 5277 131 111 9795 131 2599 137 9879 422 137 494 2565 6938 555 188 111 4838 2565 9226 8118 137 111 2399 5108 3560 111 1120 131 15055 7845 137 111 1111 2900 4736 131 111 12288 220 1248 1060 205 2285 422 111 2256 131 1506 1824 6225 168 111 952 13643 12288 3560 111 6323 137 5365 131 111 15055 7845 989 195 106 1371 121 111 2083 3041 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 7836 13156 12288 2505 5846 3236 147 6002 1477 137 4638 121 26274 103 238 527 2959 111 5846 3236 131 13156 12288 147 6002 1477 137 4638 121 111 10309 1025 131 26274 487 1916 453 2760 263 5413 13156 19323 9654 214 111 3762 131 7908 11682 205 111 545 131 111 18231 4025 25565 3089 2861 198 12660 6028 422 1899 147 8388 422 137 19323 2899 1357 6452 111 9714 131 5805 7675 5423 422 2255 111 9714 131 3115 579 1482 3236 165 6452 214 4703 137 12660 6028 205 19323 2899 137 12660 6028 6452 111 9714 131 3007 13600 3236 205 238 527 11878 111 965 168 111 1120 131 1506 2965 168 13156 1865 121 111 2220 131 111 5468 6002 205 121 238 2646 422 111 5784 5370 131 111 26274 15055 1120 11226 422 111 7675 3840 5277 131 111 9795 131 2599 137 9879 422 137 494 2565 6938 555 188 111 4838 2565 9226 8118 137 111 2399 5108 3560 111 1120 131 15055 7845 137 111 1111 2900 4736 131 111 12288 220 1248 1060 205 2285 422 111 2256 131 1506 1824 6225 168 111 952 13643 12288 3560 111 6323 137 5365 131 111 15055 7845 989 195 106 1371 121 111 2083 3041 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used for model gpu 0\n",
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from study_type_multi\\model.ckpt-2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from study_type_multi\\model.ckpt-2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared train from end\n",
      "Train on 20252 samples, validate on 888 samples\n",
      "Epoch 1/5\n",
      "20252/20252 [==============================] - 2s 114us/sample - loss: 0.0786 - val_loss: 0.0632\n",
      "Epoch 2/5\n",
      "20252/20252 [==============================] - 2s 95us/sample - loss: 0.0647 - val_loss: 0.0556\n",
      "Epoch 3/5\n",
      "20252/20252 [==============================] - 2s 94us/sample - loss: 0.0611 - val_loss: 0.0597\n",
      "Epoch 4/5\n",
      "20252/20252 [==============================] - 2s 95us/sample - loss: 0.0577 - val_loss: 0.0518\n",
      "Epoch 5/5\n",
      "20252/20252 [==============================] - 2s 100us/sample - loss: 0.0556 - val_loss: 0.0540\n"
     ]
    }
   ],
   "source": [
    "_study_type_labeler = study_type_labeler.StudyTypeLabeler(\n",
    "      \"study_type_multi\",\n",
    "      max_seq_length = 256, batch_size = 16,\n",
    "      gpu_device_num_hub = 0, gpu_device_num = 0,\n",
    "      model_folder = \"../tmp/scibert_scivocab_uncased\",\n",
    "      label_list = list(range(len(study_labels))),\n",
    "      meta_model_folder = \"study_type_multi_meta\",\n",
    "      use_one_layer=True,\n",
    "      keep_prob=0.8,\n",
    "      epochs_num=5,\n",
    "      multilabel=True\n",
    "      )\n",
    "#_study_type_labeler.train_model(train, test, use_tails=True)\n",
    "_study_type_labeler.train_meta_model(train, test, use_tail=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../tmp/scibert_scivocab_uncased\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'study_type_multi', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000022449773898>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'study_type_multi', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000022449773898>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config is done\n",
      "INFO:tensorflow:Writing example 0 of 20252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 20252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] direct - use value of small ##holder crop production in a semi - arid rural south african village . [SEP] the monetary value of natural resources used by rural communities for subs ##istence is important when addressing issues affecting the live ##li ##hood ##s of imp ##over ##ished rural households . there is therefore the need to attribute monetary values to non - market ##ed products from small ##holder production systems in order to reliably account for resource availability and usage to further sound policy decisions . the objective of this paper is to present an empirical analysis of the direct - use and trade ##d values of crop production by households , and to discuss the implications for policy development . the study was undertaken in combination with an evaluation of other live ##li ##hood sectors in thorn ##dale , a semi - arid rural village in the lim ##pop ##o province of south africa . the net direct - use value of crops was estimated at $ 44 ##3 . 4 per household per annu ##m across the village . maize ( ze ##a may ##s ) , water ##mel ##on ( citr ##ull ##us vulgaris ) , pean ##uts ( arach ##is hypo ##ga ##ea ) and common bean ##s ( phase ##olus vulgaris ) contributed over 90 % to the total direct - use value of crops . maize alone contributed 40 % of this value per household at an estimated $ 65 ##2 / ha . marketing of resources [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] direct - use value of small ##holder crop production in a semi - arid rural south african village . [SEP] the monetary value of natural resources used by rural communities for subs ##istence is important when addressing issues affecting the live ##li ##hood ##s of imp ##over ##ished rural households . there is therefore the need to attribute monetary values to non - market ##ed products from small ##holder production systems in order to reliably account for resource availability and usage to further sound policy decisions . the objective of this paper is to present an empirical analysis of the direct - use and trade ##d values of crop production by households , and to discuss the implications for policy development . the study was undertaken in combination with an evaluation of other live ##li ##hood sectors in thorn ##dale , a semi - arid rural village in the lim ##pop ##o province of south africa . the net direct - use value of crops was estimated at $ 44 ##3 . 4 per household per annu ##m across the village . maize ( ze ##a may ##s ) , water ##mel ##on ( citr ##ull ##us vulgaris ) , pean ##uts ( arach ##is hypo ##ga ##ea ) and common bean ##s ( phase ##olus vulgaris ) contributed over 90 % to the total direct - use value of crops . maize alone contributed 40 % of this value per household at an estimated $ 65 ##2 / ha . marketing of resources [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 1381 579 626 973 131 952 13643 8501 1865 121 106 6317 579 29111 7046 4430 7608 17719 205 103 111 13029 973 131 2404 2965 501 214 7046 5904 168 1763 28674 165 1060 603 11210 3293 7586 111 6489 3206 4402 30113 131 1082 1132 4064 7046 9654 205 461 165 1104 111 965 147 7053 13029 988 147 699 579 2753 119 2800 263 952 13643 1865 1078 121 993 147 14767 2670 168 3955 5232 137 6323 147 911 4976 2951 5207 205 111 3201 131 238 1203 165 147 709 130 4808 669 131 111 1381 579 626 137 4931 30118 988 131 8501 1865 214 9654 422 137 147 3978 111 5214 168 2951 1120 205 111 527 241 11058 121 2702 190 130 2166 131 494 6489 3206 4402 11329 121 29104 28871 422 106 6317 579 29111 7046 17719 121 111 950 9851 30112 11768 131 4430 7611 205 111 3657 1381 579 626 973 131 12760 241 2595 235 3250 3894 30138 205 286 309 5430 309 19389 30119 2186 111 17719 205 13156 145 3889 30110 552 30113 546 422 1506 12952 110 145 11887 1435 153 27039 546 422 27501 16402 145 22153 129 19877 2784 11901 546 137 1495 23022 30113 145 1481 24815 27039 546 6753 573 3289 1863 147 111 1114 1381 579 626 973 131 12760 205 13156 3957 6753 1921 1863 131 238 973 309 5430 235 130 2595 3250 4747 30132 1352 325 205 10644 131 2965 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 1381 579 626 973 131 952 13643 8501 1865 121 106 6317 579 29111 7046 4430 7608 17719 205 103 111 13029 973 131 2404 2965 501 214 7046 5904 168 1763 28674 165 1060 603 11210 3293 7586 111 6489 3206 4402 30113 131 1082 1132 4064 7046 9654 205 461 165 1104 111 965 147 7053 13029 988 147 699 579 2753 119 2800 263 952 13643 1865 1078 121 993 147 14767 2670 168 3955 5232 137 6323 147 911 4976 2951 5207 205 111 3201 131 238 1203 165 147 709 130 4808 669 131 111 1381 579 626 137 4931 30118 988 131 8501 1865 214 9654 422 137 147 3978 111 5214 168 2951 1120 205 111 527 241 11058 121 2702 190 130 2166 131 494 6489 3206 4402 11329 121 29104 28871 422 106 6317 579 29111 7046 17719 121 111 950 9851 30112 11768 131 4430 7611 205 111 3657 1381 579 626 973 131 12760 241 2595 235 3250 3894 30138 205 286 309 5430 309 19389 30119 2186 111 17719 205 13156 145 3889 30110 552 30113 546 422 1506 12952 110 145 11887 1435 153 27039 546 422 27501 16402 145 22153 129 19877 2784 11901 546 137 1495 23022 30113 145 1481 24815 27039 546 6753 573 3289 1863 147 111 1114 1381 579 626 973 131 12760 205 13156 3957 6753 1921 1863 131 238 973 309 5430 235 130 2595 3250 4747 30132 1352 325 205 10644 131 2965 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] pattern of live ##li ##hood and household food security among rural dwell ##ers : case of women past ##oral ##ists in o ##yo state of nigeria [SEP] the study of the pattern of live ##li ##hood and household food security among rural dwell ##ers case of women past ##oral ##ist was carried out in o ##yo state , nigeria . data were obtained from 100 women . the women are pur ##pos ##ive ##ly sampled such that their husband ##s were past ##oral ##ists or that they are involved in past ##oral farming . structured and validated interview schedule was used to elicit necessary information through interviews . the data were analyzed with appropriate statistical tools . the majority of past ##oral women was still young and was between the ages of 21 - 30 years . they therefore had strength to face the rig ##ors and tasks involved in maintaining household food security . also they were actively involved in income generating activities such as se ##wing , we ##aving mats and trading apart from dairy work . these activities enabled the women past ##oral ##ists in o ##yo state to function effectively in ensuring that their households were food secured . the respondents were engaged in livestock farming with varying numbers of different domestic animals including duc ##ks , chicken and goats . about 59 % kept duc ##ks , 54 % had chicken while 58 % had goats . majority of them ( 78 % ) processed daily products such as [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] pattern of live ##li ##hood and household food security among rural dwell ##ers : case of women past ##oral ##ists in o ##yo state of nigeria [SEP] the study of the pattern of live ##li ##hood and household food security among rural dwell ##ers case of women past ##oral ##ist was carried out in o ##yo state , nigeria . data were obtained from 100 women . the women are pur ##pos ##ive ##ly sampled such that their husband ##s were past ##oral ##ists or that they are involved in past ##oral farming . structured and validated interview schedule was used to elicit necessary information through interviews . the data were analyzed with appropriate statistical tools . the majority of past ##oral women was still young and was between the ages of 21 - 30 years . they therefore had strength to face the rig ##ors and tasks involved in maintaining household food security . also they were actively involved in income generating activities such as se ##wing , we ##aving mats and trading apart from dairy work . these activities enabled the women past ##oral ##ists in o ##yo state to function effectively in ensuring that their households were food secured . the respondents were engaged in livestock farming with varying numbers of different domestic animals including duc ##ks , chicken and goats . about 59 % kept duc ##ks , 54 % had chicken while 58 % had goats . majority of them ( 78 % ) processed daily products such as [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 1377 131 6489 3206 4402 137 5430 2599 3594 1247 7046 23694 270 862 820 131 2007 3648 5870 1340 121 116 13926 1098 131 17964 103 111 527 131 111 1377 131 6489 3206 4402 137 5430 2599 3594 1247 7046 23694 270 820 131 2007 3648 5870 383 241 3065 556 121 116 13926 1098 422 17964 205 453 267 1151 263 1287 2007 205 111 2007 220 1434 451 1090 179 7757 555 198 547 20936 30113 267 3648 5870 1340 234 198 698 220 2296 121 3648 5870 19323 205 7908 137 7905 4489 8279 241 501 147 15945 2538 776 833 8572 205 111 453 267 2549 190 2826 2397 3674 205 111 4185 131 3648 5870 2007 241 2077 3182 137 241 467 111 8714 131 2337 579 1339 1320 205 698 1104 883 3432 147 3942 111 5431 386 137 3574 2296 121 7732 5430 2599 3594 205 469 698 267 11785 2296 121 5021 6527 2687 555 188 262 17718 422 185 5756 18588 137 13028 8394 263 14874 697 205 407 2687 10378 111 2007 3648 5870 1340 121 116 13926 1098 147 662 5419 121 12730 198 547 9654 267 2599 26048 205 111 6634 267 13030 121 18262 19323 190 5543 3067 131 643 8070 2863 1471 11006 784 422 14935 137 27762 205 1011 5275 1863 6890 11006 784 422 4881 1863 883 14935 969 4878 1863 883 27762 205 4185 131 1445 145 7898 1863 546 6609 4122 2800 555 188 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 1377 131 6489 3206 4402 137 5430 2599 3594 1247 7046 23694 270 862 820 131 2007 3648 5870 1340 121 116 13926 1098 131 17964 103 111 527 131 111 1377 131 6489 3206 4402 137 5430 2599 3594 1247 7046 23694 270 820 131 2007 3648 5870 383 241 3065 556 121 116 13926 1098 422 17964 205 453 267 1151 263 1287 2007 205 111 2007 220 1434 451 1090 179 7757 555 198 547 20936 30113 267 3648 5870 1340 234 198 698 220 2296 121 3648 5870 19323 205 7908 137 7905 4489 8279 241 501 147 15945 2538 776 833 8572 205 111 453 267 2549 190 2826 2397 3674 205 111 4185 131 3648 5870 2007 241 2077 3182 137 241 467 111 8714 131 2337 579 1339 1320 205 698 1104 883 3432 147 3942 111 5431 386 137 3574 2296 121 7732 5430 2599 3594 205 469 698 267 11785 2296 121 5021 6527 2687 555 188 262 17718 422 185 5756 18588 137 13028 8394 263 14874 697 205 407 2687 10378 111 2007 3648 5870 1340 121 116 13926 1098 147 662 5419 121 12730 198 547 9654 267 2599 26048 205 111 6634 267 13030 121 18262 19323 190 5543 3067 131 643 8070 2863 1471 11006 784 422 14935 137 27762 205 1011 5275 1863 6890 11006 784 422 4881 1863 883 14935 969 4878 1863 883 27762 205 4185 131 1445 145 7898 1863 546 6609 4122 2800 555 188 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] proceedings of the regional symposium on sustaining small - scale vegetable production and marketing systems for food and nutrition security ( sea ##ve ##g ##201 ##4 ) , 25 - 27 february 2014 , bang ##ko ##k , thailand . [SEP] this proceedings contains papers dealing with various aspects of vegetable production , supply and demand in the region , with particular emphasis on cross - sector ##al issues . these papers are presented in five technical sessions to review small - scale vegetable farming , processing and marketing , to encourage improvement in the sustainability of vegetable production , promotion of vegetable consumption for better nutrition and health , to examine policy and development aspects , to attract industry profitability and competitiveness , and to enhance the performance of the vegetable industry in southeast asia . session 1 deal with small - scale farming systems , discussing topics on small holding ##s and farming status , contribution of farm families in food production , measures to improve small - scale farming systems as well as home , school and other public facility garden ##s . session 2 ( from seed to harvest ) covers topics on germ ##plasm , breeding , seed and plant health , integrated crop management with a focus on good agricultural practices , low - input / organic vegetable production , and ase ##an indigenous vegetables . topics described in session 3 ( from harvest to table ) include vegetable post ##har ##vest technologies , post ##har ##vest loss [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] proceedings of the regional symposium on sustaining small - scale vegetable production and marketing systems for food and nutrition security ( sea ##ve ##g ##201 ##4 ) , 25 - 27 february 2014 , bang ##ko ##k , thailand . [SEP] this proceedings contains papers dealing with various aspects of vegetable production , supply and demand in the region , with particular emphasis on cross - sector ##al issues . these papers are presented in five technical sessions to review small - scale vegetable farming , processing and marketing , to encourage improvement in the sustainability of vegetable production , promotion of vegetable consumption for better nutrition and health , to examine policy and development aspects , to attract industry profitability and competitiveness , and to enhance the performance of the vegetable industry in southeast asia . session 1 deal with small - scale farming systems , discussing topics on small holding ##s and farming status , contribution of farm families in food production , measures to improve small - scale farming systems as well as home , school and other public facility garden ##s . session 2 ( from seed to harvest ) covers topics on germ ##plasm , breeding , seed and plant health , integrated crop management with a focus on good agricultural practices , low - input / organic vegetable production , and ase ##an indigenous vegetables . topics described in session 3 ( from harvest to table ) include vegetable post ##har ##vest technologies , post ##har ##vest loss [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 5643 131 111 5447 10824 191 29855 952 579 2211 20288 1865 137 10644 1078 168 2599 137 8556 3594 145 6615 175 30123 4967 30140 546 422 1552 579 2532 6497 4521 422 14092 5635 30135 422 18493 205 103 238 5643 3017 5995 10449 190 1711 3905 131 20288 1865 422 4494 137 3880 121 111 1025 422 190 1349 9581 191 2057 579 5672 120 3293 205 407 5995 220 1632 121 2539 4147 7656 147 1579 952 579 2211 20288 19323 422 2307 137 10644 422 147 12902 3523 121 111 13591 131 20288 1865 422 11185 131 20288 3337 168 1883 8556 137 947 422 147 4423 2951 137 1120 3905 422 147 16619 4569 22640 137 23035 422 137 147 3681 111 1150 131 111 20288 4569 121 21106 10705 205 4205 158 5752 190 952 579 2211 19323 1078 422 16875 8746 191 952 11803 30113 137 19323 2726 422 4068 131 5947 5498 121 2599 1865 422 2554 147 1658 952 579 2211 19323 1078 188 804 188 3417 422 2694 137 494 1771 8516 25314 30113 205 4205 170 145 263 3678 147 10719 546 9774 8746 191 2927 25169 422 10156 422 3678 137 2529 947 422 4240 8501 1837 190 106 1790 191 1846 7675 5423 422 629 579 1653 1352 4756 20288 1865 422 137 19807 133 16508 17477 205 8746 1356 121 4205 239 145 263 10719 147 1020 546 2212 20288 1422 8509 22722 4198 422 1422 8509 22722 1738 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 5643 131 111 5447 10824 191 29855 952 579 2211 20288 1865 137 10644 1078 168 2599 137 8556 3594 145 6615 175 30123 4967 30140 546 422 1552 579 2532 6497 4521 422 14092 5635 30135 422 18493 205 103 238 5643 3017 5995 10449 190 1711 3905 131 20288 1865 422 4494 137 3880 121 111 1025 422 190 1349 9581 191 2057 579 5672 120 3293 205 407 5995 220 1632 121 2539 4147 7656 147 1579 952 579 2211 20288 19323 422 2307 137 10644 422 147 12902 3523 121 111 13591 131 20288 1865 422 11185 131 20288 3337 168 1883 8556 137 947 422 147 4423 2951 137 1120 3905 422 147 16619 4569 22640 137 23035 422 137 147 3681 111 1150 131 111 20288 4569 121 21106 10705 205 4205 158 5752 190 952 579 2211 19323 1078 422 16875 8746 191 952 11803 30113 137 19323 2726 422 4068 131 5947 5498 121 2599 1865 422 2554 147 1658 952 579 2211 19323 1078 188 804 188 3417 422 2694 137 494 1771 8516 25314 30113 205 4205 170 145 263 3678 147 10719 546 9774 8746 191 2927 25169 422 10156 422 3678 137 2529 947 422 4240 8501 1837 190 106 1790 191 1846 7675 5423 422 629 579 1653 1352 4756 20288 1865 422 137 19807 133 16508 17477 205 8746 1356 121 4205 239 145 263 10719 147 1020 546 2212 20288 1422 8509 22722 4198 422 1422 8509 22722 1738 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] succin ##ylated di ##osc ##ore ##a cay ##enen ##sis starch : effect of reaction parameters and character ##isation [SEP] starch was extracted from di ##osc ##ore ##a cay ##enen ##sis ( yield , 87 . 2 % ) and it was subjected to succin ##ylation under different reaction conditions . the succin ##yl amount and the degree of substitution ( ds ) in starch increased as the concentration of succin ##ic anhydr ##ide , time and temperature increased . optimal ds ( 0 . 217 ) and % succin ##yl content of 11 . 82 could be achieved at starch : catalyst ( pyridine ) ratio of 1 : 1 , succin ##ic anhydr ##ide concentration of 4 % w / w at 120 degrees c and 4 ##h of reaction . succin ##ylation did not alter starch crystall ##inity remarkably . native starch granules appeared oval , round and spherical with sizes in the range 153 ##1 mu m ( length ) and 102 ##0 mu m ( width ) . both swelling capacity and solubility improved after succin ##ylation . succin ##ylation increased paste clarity and it was dependent of the ds and storage time for the paste . past ##ing temperature was reduced after modification and as the ds increased among the starch derivatives . the peak viscosity of the native starch was lower than the viscos ##ities of all the modified starch ##es . succin ##ylation reduced set ##back values but increased breakdown . percentage syn ##ere ##sis was reduced [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] succin ##ylated di ##osc ##ore ##a cay ##enen ##sis starch : effect of reaction parameters and character ##isation [SEP] starch was extracted from di ##osc ##ore ##a cay ##enen ##sis ( yield , 87 . 2 % ) and it was subjected to succin ##ylation under different reaction conditions . the succin ##yl amount and the degree of substitution ( ds ) in starch increased as the concentration of succin ##ic anhydr ##ide , time and temperature increased . optimal ds ( 0 . 217 ) and % succin ##yl content of 11 . 82 could be achieved at starch : catalyst ( pyridine ) ratio of 1 : 1 , succin ##ic anhydr ##ide concentration of 4 % w / w at 120 degrees c and 4 ##h of reaction . succin ##ylation did not alter starch crystall ##inity remarkably . native starch granules appeared oval , round and spherical with sizes in the range 153 ##1 mu m ( length ) and 102 ##0 mu m ( width ) . both swelling capacity and solubility improved after succin ##ylation . succin ##ylation increased paste clarity and it was dependent of the ds and storage time for the paste . past ##ing temperature was reduced after modification and as the ds increased among the starch derivatives . the peak viscosity of the native starch was lower than the viscos ##ities of all the modified starch ##es . succin ##ylation reduced set ##back values but increased breakdown . percentage syn ##ere ##sis was reduced [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 21510 8471 346 26624 257 30110 29422 11298 4554 13881 862 907 131 2426 1496 137 954 3502 103 13881 241 4271 263 346 26624 257 30110 29422 11298 4554 145 2210 422 8468 205 170 1863 546 137 256 241 7178 147 21510 4218 604 643 2426 1245 205 111 21510 654 2407 137 111 2460 131 8029 145 4415 546 121 13881 1175 188 111 1840 131 21510 141 26911 390 422 532 137 1633 1175 205 2409 4415 145 244 205 17982 546 137 1863 21510 654 2118 131 1021 205 8707 968 195 3178 235 13881 862 8432 145 26656 546 1857 131 158 862 158 422 21510 141 26911 390 1840 131 286 1863 124 1352 124 235 5897 5634 115 137 286 30117 131 2426 205 21510 4218 1544 302 4222 13881 6592 4096 13688 205 6227 13881 16040 5844 22473 422 5194 137 10048 190 5155 121 111 1493 18308 30130 7229 127 145 1755 546 137 11480 30131 7229 127 145 4897 546 205 655 13648 2900 137 13601 3007 647 21510 4218 205 21510 4218 1175 17519 16585 137 256 241 3644 131 111 4415 137 4197 532 168 111 17519 205 3648 140 1633 241 1797 647 5945 137 188 111 4415 1175 1247 111 13881 6823 205 111 2391 11212 131 111 6227 13881 241 1268 506 111 10716 1003 131 355 111 3793 13881 123 205 21510 4218 1797 610 2686 988 563 1175 12236 205 3592 977 8602 4554 241 1797 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 21510 8471 346 26624 257 30110 29422 11298 4554 13881 862 907 131 2426 1496 137 954 3502 103 13881 241 4271 263 346 26624 257 30110 29422 11298 4554 145 2210 422 8468 205 170 1863 546 137 256 241 7178 147 21510 4218 604 643 2426 1245 205 111 21510 654 2407 137 111 2460 131 8029 145 4415 546 121 13881 1175 188 111 1840 131 21510 141 26911 390 422 532 137 1633 1175 205 2409 4415 145 244 205 17982 546 137 1863 21510 654 2118 131 1021 205 8707 968 195 3178 235 13881 862 8432 145 26656 546 1857 131 158 862 158 422 21510 141 26911 390 1840 131 286 1863 124 1352 124 235 5897 5634 115 137 286 30117 131 2426 205 21510 4218 1544 302 4222 13881 6592 4096 13688 205 6227 13881 16040 5844 22473 422 5194 137 10048 190 5155 121 111 1493 18308 30130 7229 127 145 1755 546 137 11480 30131 7229 127 145 4897 546 205 655 13648 2900 137 13601 3007 647 21510 4218 205 21510 4218 1175 17519 16585 137 256 241 3644 131 111 4415 137 4197 532 168 111 17519 205 3648 140 1633 241 1797 647 5945 137 188 111 4415 1175 1247 111 13881 6823 205 111 2391 11212 131 111 6227 13881 241 1268 506 111 10716 1003 131 355 111 3793 13881 123 205 21510 4218 1797 610 2686 988 563 1175 12236 205 3592 977 8602 4554 241 1797 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] experiences with the use of gli ##ric ##idia sep ##ium as animal fo ##dd ##er in northeast brazil . experi ##encia ##s com o us ##o da gli ##ric ##idia sep ##ium na ali ##ment ##aca ##o animal no nord ##este brasil ##ei ##ro [SEP] after a short introduction discussing the value of sil ##vo ##pas ##toral systems and outl ##ining the characteristics of gli ##ric ##idia sep ##ium , the paper describes the use of the species in brazil with emphasis on the northeast ##ern region . the various sil ##vo ##pas ##toral and agro ##sil ##vo ##pas ##toral systems used are described and data presented on the productivity ( biomass production and meat and milk production ) of selected systems [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] experiences with the use of gli ##ric ##idia sep ##ium as animal fo ##dd ##er in northeast brazil . experi ##encia ##s com o us ##o da gli ##ric ##idia sep ##ium na ali ##ment ##aca ##o animal no nord ##este brasil ##ei ##ro [SEP] after a short introduction discussing the value of sil ##vo ##pas ##toral systems and outl ##ining the characteristics of gli ##ric ##idia sep ##ium , the paper describes the use of the species in brazil with emphasis on the northeast ##ern region . the various sil ##vo ##pas ##toral and agro ##sil ##vo ##pas ##toral systems used are described and data presented on the productivity ( biomass production and meat and milk production ) of selected systems [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 6274 190 111 626 131 9301 782 17935 9892 888 188 3561 2941 6529 114 121 24994 7689 205 1852 14129 30113 209 116 227 30112 2762 9301 782 17935 9892 888 2431 14360 225 11266 30112 3561 425 16206 14930 25238 6422 136 103 647 106 2001 2067 16875 111 973 131 2622 9496 17268 15944 1078 137 8734 2620 111 2087 131 9301 782 17935 9892 888 422 111 1203 5223 111 626 131 111 1578 121 7689 190 9581 191 111 24994 1841 1025 205 111 1711 2622 9496 17268 15944 137 22362 11424 9496 17268 15944 1078 501 220 1356 137 453 1632 191 111 7381 145 7537 1865 137 12782 137 7435 1865 546 131 2350 1078 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 6274 190 111 626 131 9301 782 17935 9892 888 188 3561 2941 6529 114 121 24994 7689 205 1852 14129 30113 209 116 227 30112 2762 9301 782 17935 9892 888 2431 14360 225 11266 30112 3561 425 16206 14930 25238 6422 136 103 647 106 2001 2067 16875 111 973 131 2622 9496 17268 15944 1078 137 8734 2620 111 2087 131 9301 782 17935 9892 888 422 111 1203 5223 111 626 131 111 1578 121 7689 190 9581 191 111 24994 1841 1025 205 111 1711 2622 9496 17268 15944 137 22362 11424 9496 17268 15944 1078 501 220 1356 137 453 1632 191 111 7381 145 7537 1865 137 12782 137 7435 1865 546 131 2350 1078 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 10000 of 20252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 10000 of 20252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 20000 of 20252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 20000 of 20252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used for model gpu 0\n",
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from study_type_multi\\model.ckpt-2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from study_type_multi\\model.ckpt-2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 46030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 46030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] direct - use value of small ##holder crop production in a semi - arid rural south african village . [SEP] the monetary value of natural resources used by rural communities for subs ##istence is important when addressing issues affecting the live ##li ##hood ##s of imp ##over ##ished rural households . there is therefore the need to attribute monetary values to non - market ##ed products from small ##holder production systems in order to reliably account for resource availability and usage to further sound policy decisions . the objective of this paper is to present an empirical analysis of the direct - use and trade ##d values of crop production by households , and to discuss the implications for policy development . the study was undertaken in combination with an evaluation of other live ##li ##hood sectors in thorn ##dale , a semi - arid rural village in the lim ##pop ##o province of south africa . the net direct - use value of crops was estimated at $ 44 ##3 . 4 per household per annu ##m across the village . maize ( ze ##a may ##s ) , water ##mel ##on ( citr ##ull ##us vulgaris ) , pean ##uts ( arach ##is hypo ##ga ##ea ) and common bean ##s ( phase ##olus vulgaris ) contributed over 90 % to the total direct - use value of crops . maize alone contributed 40 % of this value per household at an estimated $ 65 ##2 / ha . marketing of resources [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] direct - use value of small ##holder crop production in a semi - arid rural south african village . [SEP] the monetary value of natural resources used by rural communities for subs ##istence is important when addressing issues affecting the live ##li ##hood ##s of imp ##over ##ished rural households . there is therefore the need to attribute monetary values to non - market ##ed products from small ##holder production systems in order to reliably account for resource availability and usage to further sound policy decisions . the objective of this paper is to present an empirical analysis of the direct - use and trade ##d values of crop production by households , and to discuss the implications for policy development . the study was undertaken in combination with an evaluation of other live ##li ##hood sectors in thorn ##dale , a semi - arid rural village in the lim ##pop ##o province of south africa . the net direct - use value of crops was estimated at $ 44 ##3 . 4 per household per annu ##m across the village . maize ( ze ##a may ##s ) , water ##mel ##on ( citr ##ull ##us vulgaris ) , pean ##uts ( arach ##is hypo ##ga ##ea ) and common bean ##s ( phase ##olus vulgaris ) contributed over 90 % to the total direct - use value of crops . maize alone contributed 40 % of this value per household at an estimated $ 65 ##2 / ha . marketing of resources [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 1381 579 626 973 131 952 13643 8501 1865 121 106 6317 579 29111 7046 4430 7608 17719 205 103 111 13029 973 131 2404 2965 501 214 7046 5904 168 1763 28674 165 1060 603 11210 3293 7586 111 6489 3206 4402 30113 131 1082 1132 4064 7046 9654 205 461 165 1104 111 965 147 7053 13029 988 147 699 579 2753 119 2800 263 952 13643 1865 1078 121 993 147 14767 2670 168 3955 5232 137 6323 147 911 4976 2951 5207 205 111 3201 131 238 1203 165 147 709 130 4808 669 131 111 1381 579 626 137 4931 30118 988 131 8501 1865 214 9654 422 137 147 3978 111 5214 168 2951 1120 205 111 527 241 11058 121 2702 190 130 2166 131 494 6489 3206 4402 11329 121 29104 28871 422 106 6317 579 29111 7046 17719 121 111 950 9851 30112 11768 131 4430 7611 205 111 3657 1381 579 626 973 131 12760 241 2595 235 3250 3894 30138 205 286 309 5430 309 19389 30119 2186 111 17719 205 13156 145 3889 30110 552 30113 546 422 1506 12952 110 145 11887 1435 153 27039 546 422 27501 16402 145 22153 129 19877 2784 11901 546 137 1495 23022 30113 145 1481 24815 27039 546 6753 573 3289 1863 147 111 1114 1381 579 626 973 131 12760 205 13156 3957 6753 1921 1863 131 238 973 309 5430 235 130 2595 3250 4747 30132 1352 325 205 10644 131 2965 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 1381 579 626 973 131 952 13643 8501 1865 121 106 6317 579 29111 7046 4430 7608 17719 205 103 111 13029 973 131 2404 2965 501 214 7046 5904 168 1763 28674 165 1060 603 11210 3293 7586 111 6489 3206 4402 30113 131 1082 1132 4064 7046 9654 205 461 165 1104 111 965 147 7053 13029 988 147 699 579 2753 119 2800 263 952 13643 1865 1078 121 993 147 14767 2670 168 3955 5232 137 6323 147 911 4976 2951 5207 205 111 3201 131 238 1203 165 147 709 130 4808 669 131 111 1381 579 626 137 4931 30118 988 131 8501 1865 214 9654 422 137 147 3978 111 5214 168 2951 1120 205 111 527 241 11058 121 2702 190 130 2166 131 494 6489 3206 4402 11329 121 29104 28871 422 106 6317 579 29111 7046 17719 121 111 950 9851 30112 11768 131 4430 7611 205 111 3657 1381 579 626 973 131 12760 241 2595 235 3250 3894 30138 205 286 309 5430 309 19389 30119 2186 111 17719 205 13156 145 3889 30110 552 30113 546 422 1506 12952 110 145 11887 1435 153 27039 546 422 27501 16402 145 22153 129 19877 2784 11901 546 137 1495 23022 30113 145 1481 24815 27039 546 6753 573 3289 1863 147 111 1114 1381 579 626 973 131 12760 205 13156 3957 6753 1921 1863 131 238 973 309 5430 235 130 2595 3250 4747 30132 1352 325 205 10644 131 2965 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] direct - use value of small ##holder crop production in a semi - arid rural south african village . [SEP] maize ( ze ##a may ##s ) , water ##mel ##on ( citr ##ull ##us vulgaris ) , pean ##uts ( arach ##is hypo ##ga ##ea ) and common bean ##s ( phase ##olus vulgaris ) contributed over 90 % to the total direct - use value of crops . maize alone contributed 40 % of this value per household at an estimated $ 65 ##2 / ha . marketing of resources was not a common practice , limited to only maize and pean ##uts . farming was basically a rain - fed - mixed crop ##ping system with low production inputs . farmer support services , human capital development and tenure security were major areas identified for policy development . [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] direct - use value of small ##holder crop production in a semi - arid rural south african village . [SEP] maize ( ze ##a may ##s ) , water ##mel ##on ( citr ##ull ##us vulgaris ) , pean ##uts ( arach ##is hypo ##ga ##ea ) and common bean ##s ( phase ##olus vulgaris ) contributed over 90 % to the total direct - use value of crops . maize alone contributed 40 % of this value per household at an estimated $ 65 ##2 / ha . marketing of resources was not a common practice , limited to only maize and pean ##uts . farming was basically a rain - fed - mixed crop ##ping system with low production inputs . farmer support services , human capital development and tenure security were major areas identified for policy development . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 1381 579 626 973 131 952 13643 8501 1865 121 106 6317 579 29111 7046 4430 7608 17719 205 103 13156 145 3889 30110 552 30113 546 422 1506 12952 110 145 11887 1435 153 27039 546 422 27501 16402 145 22153 129 19877 2784 11901 546 137 1495 23022 30113 145 1481 24815 27039 546 6753 573 3289 1863 147 111 1114 1381 579 626 973 131 12760 205 13156 3957 6753 1921 1863 131 238 973 309 5430 235 130 2595 3250 4747 30132 1352 325 205 10644 131 2965 241 302 106 1495 2794 422 2379 147 617 13156 137 27501 16402 205 19323 241 13277 106 7112 579 4140 579 4055 8501 5354 429 190 629 1865 5671 205 24830 1385 2522 422 1168 5153 1120 137 27949 3594 267 1626 2326 1887 168 2951 1120 205 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 1381 579 626 973 131 952 13643 8501 1865 121 106 6317 579 29111 7046 4430 7608 17719 205 103 13156 145 3889 30110 552 30113 546 422 1506 12952 110 145 11887 1435 153 27039 546 422 27501 16402 145 22153 129 19877 2784 11901 546 137 1495 23022 30113 145 1481 24815 27039 546 6753 573 3289 1863 147 111 1114 1381 579 626 973 131 12760 205 13156 3957 6753 1921 1863 131 238 973 309 5430 235 130 2595 3250 4747 30132 1352 325 205 10644 131 2965 241 302 106 1495 2794 422 2379 147 617 13156 137 27501 16402 205 19323 241 13277 106 7112 579 4140 579 4055 8501 5354 429 190 629 1865 5671 205 24830 1385 2522 422 1168 5153 1120 137 27949 3594 267 1626 2326 1887 168 2951 1120 205 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] pattern of live ##li ##hood and household food security among rural dwell ##ers : case of women past ##oral ##ists in o ##yo state of nigeria [SEP] the study of the pattern of live ##li ##hood and household food security among rural dwell ##ers case of women past ##oral ##ist was carried out in o ##yo state , nigeria . data were obtained from 100 women . the women are pur ##pos ##ive ##ly sampled such that their husband ##s were past ##oral ##ists or that they are involved in past ##oral farming . structured and validated interview schedule was used to elicit necessary information through interviews . the data were analyzed with appropriate statistical tools . the majority of past ##oral women was still young and was between the ages of 21 - 30 years . they therefore had strength to face the rig ##ors and tasks involved in maintaining household food security . also they were actively involved in income generating activities such as se ##wing , we ##aving mats and trading apart from dairy work . these activities enabled the women past ##oral ##ists in o ##yo state to function effectively in ensuring that their households were food secured . the respondents were engaged in livestock farming with varying numbers of different domestic animals including duc ##ks , chicken and goats . about 59 % kept duc ##ks , 54 % had chicken while 58 % had goats . majority of them ( 78 % ) processed daily products such as [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] pattern of live ##li ##hood and household food security among rural dwell ##ers : case of women past ##oral ##ists in o ##yo state of nigeria [SEP] the study of the pattern of live ##li ##hood and household food security among rural dwell ##ers case of women past ##oral ##ist was carried out in o ##yo state , nigeria . data were obtained from 100 women . the women are pur ##pos ##ive ##ly sampled such that their husband ##s were past ##oral ##ists or that they are involved in past ##oral farming . structured and validated interview schedule was used to elicit necessary information through interviews . the data were analyzed with appropriate statistical tools . the majority of past ##oral women was still young and was between the ages of 21 - 30 years . they therefore had strength to face the rig ##ors and tasks involved in maintaining household food security . also they were actively involved in income generating activities such as se ##wing , we ##aving mats and trading apart from dairy work . these activities enabled the women past ##oral ##ists in o ##yo state to function effectively in ensuring that their households were food secured . the respondents were engaged in livestock farming with varying numbers of different domestic animals including duc ##ks , chicken and goats . about 59 % kept duc ##ks , 54 % had chicken while 58 % had goats . majority of them ( 78 % ) processed daily products such as [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 1377 131 6489 3206 4402 137 5430 2599 3594 1247 7046 23694 270 862 820 131 2007 3648 5870 1340 121 116 13926 1098 131 17964 103 111 527 131 111 1377 131 6489 3206 4402 137 5430 2599 3594 1247 7046 23694 270 820 131 2007 3648 5870 383 241 3065 556 121 116 13926 1098 422 17964 205 453 267 1151 263 1287 2007 205 111 2007 220 1434 451 1090 179 7757 555 198 547 20936 30113 267 3648 5870 1340 234 198 698 220 2296 121 3648 5870 19323 205 7908 137 7905 4489 8279 241 501 147 15945 2538 776 833 8572 205 111 453 267 2549 190 2826 2397 3674 205 111 4185 131 3648 5870 2007 241 2077 3182 137 241 467 111 8714 131 2337 579 1339 1320 205 698 1104 883 3432 147 3942 111 5431 386 137 3574 2296 121 7732 5430 2599 3594 205 469 698 267 11785 2296 121 5021 6527 2687 555 188 262 17718 422 185 5756 18588 137 13028 8394 263 14874 697 205 407 2687 10378 111 2007 3648 5870 1340 121 116 13926 1098 147 662 5419 121 12730 198 547 9654 267 2599 26048 205 111 6634 267 13030 121 18262 19323 190 5543 3067 131 643 8070 2863 1471 11006 784 422 14935 137 27762 205 1011 5275 1863 6890 11006 784 422 4881 1863 883 14935 969 4878 1863 883 27762 205 4185 131 1445 145 7898 1863 546 6609 4122 2800 555 188 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 1377 131 6489 3206 4402 137 5430 2599 3594 1247 7046 23694 270 862 820 131 2007 3648 5870 1340 121 116 13926 1098 131 17964 103 111 527 131 111 1377 131 6489 3206 4402 137 5430 2599 3594 1247 7046 23694 270 820 131 2007 3648 5870 383 241 3065 556 121 116 13926 1098 422 17964 205 453 267 1151 263 1287 2007 205 111 2007 220 1434 451 1090 179 7757 555 198 547 20936 30113 267 3648 5870 1340 234 198 698 220 2296 121 3648 5870 19323 205 7908 137 7905 4489 8279 241 501 147 15945 2538 776 833 8572 205 111 453 267 2549 190 2826 2397 3674 205 111 4185 131 3648 5870 2007 241 2077 3182 137 241 467 111 8714 131 2337 579 1339 1320 205 698 1104 883 3432 147 3942 111 5431 386 137 3574 2296 121 7732 5430 2599 3594 205 469 698 267 11785 2296 121 5021 6527 2687 555 188 262 17718 422 185 5756 18588 137 13028 8394 263 14874 697 205 407 2687 10378 111 2007 3648 5870 1340 121 116 13926 1098 147 662 5419 121 12730 198 547 9654 267 2599 26048 205 111 6634 267 13030 121 18262 19323 190 5543 3067 131 643 8070 2863 1471 11006 784 422 14935 137 27762 205 1011 5275 1863 6890 11006 784 422 4881 1863 883 14935 969 4878 1863 883 27762 205 4185 131 1445 145 7898 1863 546 6609 4122 2800 555 188 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] pattern of live ##li ##hood and household food security among rural dwell ##ers : case of women past ##oral ##ists in o ##yo state of nigeria [SEP] the women past ##oral ##ists in o ##yo state to function effectively in ensuring that their households were food secured . the respondents were engaged in livestock farming with varying numbers of different domestic animals including duc ##ks , chicken and goats . about 59 % kept duc ##ks , 54 % had chicken while 58 % had goats . majority of them ( 78 % ) processed daily products such as \" war ##a \" , \" non ##o \" and butter . over 90 % of the respondents processed their products by boiling , while 69 % used salt to process . they spent all the money realized to supplement family feeding by buying various food items needed for their families . the x ##2 showed that there was no significant difference between the marital status and numbers of wi ##ves , home structure and providing household food security . based on the findings , it was recommended that storage facilities should be provided for the women past ##oral ##ists so as to cur ##b was ##tage of their farm produce and other peri ##sha ##ble commodities . they should also be given access to credit facilities so as to improve and increase the quantity and quality of their products . [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] pattern of live ##li ##hood and household food security among rural dwell ##ers : case of women past ##oral ##ists in o ##yo state of nigeria [SEP] the women past ##oral ##ists in o ##yo state to function effectively in ensuring that their households were food secured . the respondents were engaged in livestock farming with varying numbers of different domestic animals including duc ##ks , chicken and goats . about 59 % kept duc ##ks , 54 % had chicken while 58 % had goats . majority of them ( 78 % ) processed daily products such as \" war ##a \" , \" non ##o \" and butter . over 90 % of the respondents processed their products by boiling , while 69 % used salt to process . they spent all the money realized to supplement family feeding by buying various food items needed for their families . the x ##2 showed that there was no significant difference between the marital status and numbers of wi ##ves , home structure and providing household food security . based on the findings , it was recommended that storage facilities should be provided for the women past ##oral ##ists so as to cur ##b was ##tage of their farm produce and other peri ##sha ##ble commodities . they should also be given access to credit facilities so as to improve and increase the quantity and quality of their products . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 1377 131 6489 3206 4402 137 5430 2599 3594 1247 7046 23694 270 862 820 131 2007 3648 5870 1340 121 116 13926 1098 131 17964 103 111 2007 3648 5870 1340 121 116 13926 1098 147 662 5419 121 12730 198 547 9654 267 2599 26048 205 111 6634 267 13030 121 18262 19323 190 5543 3067 131 643 8070 2863 1471 11006 784 422 14935 137 27762 205 1011 5275 1863 6890 11006 784 422 4881 1863 883 14935 969 4878 1863 883 27762 205 4185 131 1445 145 7898 1863 546 6609 4122 2800 555 188 1554 4170 30110 1554 422 1554 699 30112 1554 137 18024 205 573 3289 1863 131 111 6634 6609 547 2800 214 24456 422 969 7751 1863 501 7030 147 624 205 698 9181 355 111 9668 8742 147 2979 2370 6983 214 26237 1711 2599 3945 2764 168 547 5498 205 111 412 30132 1367 198 461 241 425 684 1673 467 111 18039 2726 137 3067 131 3208 1046 422 3417 1187 137 3910 5430 2599 3594 205 791 191 111 2116 422 256 241 5805 198 4197 7845 1055 195 1966 168 111 2007 3648 5870 1340 564 188 147 718 30125 241 14647 131 547 5947 3299 137 494 9447 22269 503 29407 205 698 1055 469 195 906 1899 147 8388 7845 564 188 147 1658 137 1242 111 6725 137 1671 131 547 2800 205 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 1377 131 6489 3206 4402 137 5430 2599 3594 1247 7046 23694 270 862 820 131 2007 3648 5870 1340 121 116 13926 1098 131 17964 103 111 2007 3648 5870 1340 121 116 13926 1098 147 662 5419 121 12730 198 547 9654 267 2599 26048 205 111 6634 267 13030 121 18262 19323 190 5543 3067 131 643 8070 2863 1471 11006 784 422 14935 137 27762 205 1011 5275 1863 6890 11006 784 422 4881 1863 883 14935 969 4878 1863 883 27762 205 4185 131 1445 145 7898 1863 546 6609 4122 2800 555 188 1554 4170 30110 1554 422 1554 699 30112 1554 137 18024 205 573 3289 1863 131 111 6634 6609 547 2800 214 24456 422 969 7751 1863 501 7030 147 624 205 698 9181 355 111 9668 8742 147 2979 2370 6983 214 26237 1711 2599 3945 2764 168 547 5498 205 111 412 30132 1367 198 461 241 425 684 1673 467 111 18039 2726 137 3067 131 3208 1046 422 3417 1187 137 3910 5430 2599 3594 205 791 191 111 2116 422 256 241 5805 198 4197 7845 1055 195 1966 168 111 2007 3648 5870 1340 564 188 147 718 30125 241 14647 131 547 5947 3299 137 494 9447 22269 503 29407 205 698 1055 469 195 906 1899 147 8388 7845 564 188 147 1658 137 1242 111 6725 137 1671 131 547 2800 205 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] pattern of live ##li ##hood and household food security among rural dwell ##ers : case of women past ##oral ##ists in o ##yo state of nigeria [SEP] facilities should be provided for the women past ##oral ##ists so as to cur ##b was ##tage of their farm produce and other peri ##sha ##ble commodities . they should also be given access to credit facilities so as to improve and increase the quantity and quality of their products . [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] pattern of live ##li ##hood and household food security among rural dwell ##ers : case of women past ##oral ##ists in o ##yo state of nigeria [SEP] facilities should be provided for the women past ##oral ##ists so as to cur ##b was ##tage of their farm produce and other peri ##sha ##ble commodities . they should also be given access to credit facilities so as to improve and increase the quantity and quality of their products . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 1377 131 6489 3206 4402 137 5430 2599 3594 1247 7046 23694 270 862 820 131 2007 3648 5870 1340 121 116 13926 1098 131 17964 103 7845 1055 195 1966 168 111 2007 3648 5870 1340 564 188 147 718 30125 241 14647 131 547 5947 3299 137 494 9447 22269 503 29407 205 698 1055 469 195 906 1899 147 8388 7845 564 188 147 1658 137 1242 111 6725 137 1671 131 547 2800 205 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 1377 131 6489 3206 4402 137 5430 2599 3594 1247 7046 23694 270 862 820 131 2007 3648 5870 1340 121 116 13926 1098 131 17964 103 7845 1055 195 1966 168 111 2007 3648 5870 1340 564 188 147 718 30125 241 14647 131 547 5947 3299 137 494 9447 22269 503 29407 205 698 1055 469 195 906 1899 147 8388 7845 564 188 147 1658 137 1242 111 6725 137 1671 131 547 2800 205 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 10000 of 46030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 10000 of 46030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 20000 of 46030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 20000 of 46030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 30000 of 46030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 30000 of 46030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 40000 of 46030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 40000 of 46030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used for model gpu 0\n",
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from study_type_multi\\model.ckpt-2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from study_type_multi\\model.ckpt-2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 20252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 20252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] direct - use value of small ##holder crop production in a semi - arid rural south african village . [SEP] production systems in order to reliably account for resource availability and usage to further sound policy decisions . the objective of this paper is to present an empirical analysis of the direct - use and trade ##d values of crop production by households , and to discuss the implications for policy development . the study was undertaken in combination with an evaluation of other live ##li ##hood sectors in thorn ##dale , a semi - arid rural village in the lim ##pop ##o province of south africa . the net direct - use value of crops was estimated at $ 44 ##3 . 4 per household per annu ##m across the village . maize ( ze ##a may ##s ) , water ##mel ##on ( citr ##ull ##us vulgaris ) , pean ##uts ( arach ##is hypo ##ga ##ea ) and common bean ##s ( phase ##olus vulgaris ) contributed over 90 % to the total direct - use value of crops . maize alone contributed 40 % of this value per household at an estimated $ 65 ##2 / ha . marketing of resources was not a common practice , limited to only maize and pean ##uts . farming was basically a rain - fed - mixed crop ##ping system with low production inputs . farmer support services , human capital development and tenure security were major areas identified for policy development . [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] direct - use value of small ##holder crop production in a semi - arid rural south african village . [SEP] production systems in order to reliably account for resource availability and usage to further sound policy decisions . the objective of this paper is to present an empirical analysis of the direct - use and trade ##d values of crop production by households , and to discuss the implications for policy development . the study was undertaken in combination with an evaluation of other live ##li ##hood sectors in thorn ##dale , a semi - arid rural village in the lim ##pop ##o province of south africa . the net direct - use value of crops was estimated at $ 44 ##3 . 4 per household per annu ##m across the village . maize ( ze ##a may ##s ) , water ##mel ##on ( citr ##ull ##us vulgaris ) , pean ##uts ( arach ##is hypo ##ga ##ea ) and common bean ##s ( phase ##olus vulgaris ) contributed over 90 % to the total direct - use value of crops . maize alone contributed 40 % of this value per household at an estimated $ 65 ##2 / ha . marketing of resources was not a common practice , limited to only maize and pean ##uts . farming was basically a rain - fed - mixed crop ##ping system with low production inputs . farmer support services , human capital development and tenure security were major areas identified for policy development . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 1381 579 626 973 131 952 13643 8501 1865 121 106 6317 579 29111 7046 4430 7608 17719 205 103 1865 1078 121 993 147 14767 2670 168 3955 5232 137 6323 147 911 4976 2951 5207 205 111 3201 131 238 1203 165 147 709 130 4808 669 131 111 1381 579 626 137 4931 30118 988 131 8501 1865 214 9654 422 137 147 3978 111 5214 168 2951 1120 205 111 527 241 11058 121 2702 190 130 2166 131 494 6489 3206 4402 11329 121 29104 28871 422 106 6317 579 29111 7046 17719 121 111 950 9851 30112 11768 131 4430 7611 205 111 3657 1381 579 626 973 131 12760 241 2595 235 3250 3894 30138 205 286 309 5430 309 19389 30119 2186 111 17719 205 13156 145 3889 30110 552 30113 546 422 1506 12952 110 145 11887 1435 153 27039 546 422 27501 16402 145 22153 129 19877 2784 11901 546 137 1495 23022 30113 145 1481 24815 27039 546 6753 573 3289 1863 147 111 1114 1381 579 626 973 131 12760 205 13156 3957 6753 1921 1863 131 238 973 309 5430 235 130 2595 3250 4747 30132 1352 325 205 10644 131 2965 241 302 106 1495 2794 422 2379 147 617 13156 137 27501 16402 205 19323 241 13277 106 7112 579 4140 579 4055 8501 5354 429 190 629 1865 5671 205 24830 1385 2522 422 1168 5153 1120 137 27949 3594 267 1626 2326 1887 168 2951 1120 205 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 1381 579 626 973 131 952 13643 8501 1865 121 106 6317 579 29111 7046 4430 7608 17719 205 103 1865 1078 121 993 147 14767 2670 168 3955 5232 137 6323 147 911 4976 2951 5207 205 111 3201 131 238 1203 165 147 709 130 4808 669 131 111 1381 579 626 137 4931 30118 988 131 8501 1865 214 9654 422 137 147 3978 111 5214 168 2951 1120 205 111 527 241 11058 121 2702 190 130 2166 131 494 6489 3206 4402 11329 121 29104 28871 422 106 6317 579 29111 7046 17719 121 111 950 9851 30112 11768 131 4430 7611 205 111 3657 1381 579 626 973 131 12760 241 2595 235 3250 3894 30138 205 286 309 5430 309 19389 30119 2186 111 17719 205 13156 145 3889 30110 552 30113 546 422 1506 12952 110 145 11887 1435 153 27039 546 422 27501 16402 145 22153 129 19877 2784 11901 546 137 1495 23022 30113 145 1481 24815 27039 546 6753 573 3289 1863 147 111 1114 1381 579 626 973 131 12760 205 13156 3957 6753 1921 1863 131 238 973 309 5430 235 130 2595 3250 4747 30132 1352 325 205 10644 131 2965 241 302 106 1495 2794 422 2379 147 617 13156 137 27501 16402 205 19323 241 13277 106 7112 579 4140 579 4055 8501 5354 429 190 629 1865 5671 205 24830 1385 2522 422 1168 5153 1120 137 27949 3594 267 1626 2326 1887 168 2951 1120 205 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] pattern of live ##li ##hood and household food security among rural dwell ##ers : case of women past ##oral ##ists in o ##yo state of nigeria [SEP] , we ##aving mats and trading apart from dairy work . these activities enabled the women past ##oral ##ists in o ##yo state to function effectively in ensuring that their households were food secured . the respondents were engaged in livestock farming with varying numbers of different domestic animals including duc ##ks , chicken and goats . about 59 % kept duc ##ks , 54 % had chicken while 58 % had goats . majority of them ( 78 % ) processed daily products such as \" war ##a \" , \" non ##o \" and butter . over 90 % of the respondents processed their products by boiling , while 69 % used salt to process . they spent all the money realized to supplement family feeding by buying various food items needed for their families . the x ##2 showed that there was no significant difference between the marital status and numbers of wi ##ves , home structure and providing household food security . based on the findings , it was recommended that storage facilities should be provided for the women past ##oral ##ists so as to cur ##b was ##tage of their farm produce and other peri ##sha ##ble commodities . they should also be given access to credit facilities so as to improve and increase the quantity and quality of their products . [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] pattern of live ##li ##hood and household food security among rural dwell ##ers : case of women past ##oral ##ists in o ##yo state of nigeria [SEP] , we ##aving mats and trading apart from dairy work . these activities enabled the women past ##oral ##ists in o ##yo state to function effectively in ensuring that their households were food secured . the respondents were engaged in livestock farming with varying numbers of different domestic animals including duc ##ks , chicken and goats . about 59 % kept duc ##ks , 54 % had chicken while 58 % had goats . majority of them ( 78 % ) processed daily products such as \" war ##a \" , \" non ##o \" and butter . over 90 % of the respondents processed their products by boiling , while 69 % used salt to process . they spent all the money realized to supplement family feeding by buying various food items needed for their families . the x ##2 showed that there was no significant difference between the marital status and numbers of wi ##ves , home structure and providing household food security . based on the findings , it was recommended that storage facilities should be provided for the women past ##oral ##ists so as to cur ##b was ##tage of their farm produce and other peri ##sha ##ble commodities . they should also be given access to credit facilities so as to improve and increase the quantity and quality of their products . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 1377 131 6489 3206 4402 137 5430 2599 3594 1247 7046 23694 270 862 820 131 2007 3648 5870 1340 121 116 13926 1098 131 17964 103 422 185 5756 18588 137 13028 8394 263 14874 697 205 407 2687 10378 111 2007 3648 5870 1340 121 116 13926 1098 147 662 5419 121 12730 198 547 9654 267 2599 26048 205 111 6634 267 13030 121 18262 19323 190 5543 3067 131 643 8070 2863 1471 11006 784 422 14935 137 27762 205 1011 5275 1863 6890 11006 784 422 4881 1863 883 14935 969 4878 1863 883 27762 205 4185 131 1445 145 7898 1863 546 6609 4122 2800 555 188 1554 4170 30110 1554 422 1554 699 30112 1554 137 18024 205 573 3289 1863 131 111 6634 6609 547 2800 214 24456 422 969 7751 1863 501 7030 147 624 205 698 9181 355 111 9668 8742 147 2979 2370 6983 214 26237 1711 2599 3945 2764 168 547 5498 205 111 412 30132 1367 198 461 241 425 684 1673 467 111 18039 2726 137 3067 131 3208 1046 422 3417 1187 137 3910 5430 2599 3594 205 791 191 111 2116 422 256 241 5805 198 4197 7845 1055 195 1966 168 111 2007 3648 5870 1340 564 188 147 718 30125 241 14647 131 547 5947 3299 137 494 9447 22269 503 29407 205 698 1055 469 195 906 1899 147 8388 7845 564 188 147 1658 137 1242 111 6725 137 1671 131 547 2800 205 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 1377 131 6489 3206 4402 137 5430 2599 3594 1247 7046 23694 270 862 820 131 2007 3648 5870 1340 121 116 13926 1098 131 17964 103 422 185 5756 18588 137 13028 8394 263 14874 697 205 407 2687 10378 111 2007 3648 5870 1340 121 116 13926 1098 147 662 5419 121 12730 198 547 9654 267 2599 26048 205 111 6634 267 13030 121 18262 19323 190 5543 3067 131 643 8070 2863 1471 11006 784 422 14935 137 27762 205 1011 5275 1863 6890 11006 784 422 4881 1863 883 14935 969 4878 1863 883 27762 205 4185 131 1445 145 7898 1863 546 6609 4122 2800 555 188 1554 4170 30110 1554 422 1554 699 30112 1554 137 18024 205 573 3289 1863 131 111 6634 6609 547 2800 214 24456 422 969 7751 1863 501 7030 147 624 205 698 9181 355 111 9668 8742 147 2979 2370 6983 214 26237 1711 2599 3945 2764 168 547 5498 205 111 412 30132 1367 198 461 241 425 684 1673 467 111 18039 2726 137 3067 131 3208 1046 422 3417 1187 137 3910 5430 2599 3594 205 791 191 111 2116 422 256 241 5805 198 4197 7845 1055 195 1966 168 111 2007 3648 5870 1340 564 188 147 718 30125 241 14647 131 547 5947 3299 137 494 9447 22269 503 29407 205 698 1055 469 195 906 1899 147 8388 7845 564 188 147 1658 137 1242 111 6725 137 1671 131 547 2800 205 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] proceedings of the regional symposium on sustaining small - scale vegetable production and marketing systems for food and nutrition security ( sea ##ve ##g ##201 ##4 ) , 25 - 27 february 2014 , bang ##ko ##k , thailand . [SEP] school and other public facility garden ##s . session 2 ( from seed to harvest ) covers topics on germ ##plasm , breeding , seed and plant health , integrated crop management with a focus on good agricultural practices , low - input / organic vegetable production , and ase ##an indigenous vegetables . topics described in session 3 ( from harvest to table ) include vegetable post ##har ##vest technologies , post ##har ##vest loss prevention , good manufacturing practices , cold chain management , vegetable economics and marketing , supply chain model simulation , food safety , health and nutrition aspects of vegetables , food consumption and nutrition in the 21 ##st century , and measures to improve the role of vegetables in nutrition . session 4 deal with cross - sector ##al issues , such as contributions of vegetable research and development to addressing cross - sector ##al issues such as climate change , gender equity , ecological san ##itation , urban and regional planning , urban and peri - urban hor ##tic ##ulture , disaster risk management and micro ##fin ##ancing . session 5 ( enabling policies ) examines present policies and programmes in the sector and suggests changes related to issues mentioned in sessions 1 to 4 . [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] proceedings of the regional symposium on sustaining small - scale vegetable production and marketing systems for food and nutrition security ( sea ##ve ##g ##201 ##4 ) , 25 - 27 february 2014 , bang ##ko ##k , thailand . [SEP] school and other public facility garden ##s . session 2 ( from seed to harvest ) covers topics on germ ##plasm , breeding , seed and plant health , integrated crop management with a focus on good agricultural practices , low - input / organic vegetable production , and ase ##an indigenous vegetables . topics described in session 3 ( from harvest to table ) include vegetable post ##har ##vest technologies , post ##har ##vest loss prevention , good manufacturing practices , cold chain management , vegetable economics and marketing , supply chain model simulation , food safety , health and nutrition aspects of vegetables , food consumption and nutrition in the 21 ##st century , and measures to improve the role of vegetables in nutrition . session 4 deal with cross - sector ##al issues , such as contributions of vegetable research and development to addressing cross - sector ##al issues such as climate change , gender equity , ecological san ##itation , urban and regional planning , urban and peri - urban hor ##tic ##ulture , disaster risk management and micro ##fin ##ancing . session 5 ( enabling policies ) examines present policies and programmes in the sector and suggests changes related to issues mentioned in sessions 1 to 4 . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 5643 131 111 5447 10824 191 29855 952 579 2211 20288 1865 137 10644 1078 168 2599 137 8556 3594 145 6615 175 30123 4967 30140 546 422 1552 579 2532 6497 4521 422 14092 5635 30135 422 18493 205 103 2694 137 494 1771 8516 25314 30113 205 4205 170 145 263 3678 147 10719 546 9774 8746 191 2927 25169 422 10156 422 3678 137 2529 947 422 4240 8501 1837 190 106 1790 191 1846 7675 5423 422 629 579 1653 1352 4756 20288 1865 422 137 19807 133 16508 17477 205 8746 1356 121 4205 239 145 263 10719 147 1020 546 2212 20288 1422 8509 22722 4198 422 1422 8509 22722 1738 5200 422 1846 7887 5423 422 6663 3590 1837 422 20288 9324 137 10644 422 4494 3590 437 2257 422 2599 4104 422 947 137 8556 3905 131 17477 422 2599 3337 137 8556 121 111 2337 187 8862 422 137 2554 147 1658 111 1447 131 17477 121 8556 205 4205 286 5752 190 2057 579 5672 120 3293 422 555 188 4954 131 20288 849 137 1120 147 11210 2057 579 5672 120 3293 555 188 6002 1477 422 4703 12696 422 8384 5754 2202 422 5156 137 5447 4608 422 5156 137 9447 579 5156 3624 204 6221 422 17068 1265 1837 137 1234 10914 6019 205 4205 305 145 10619 5691 546 15817 709 5691 137 13557 121 111 5672 137 3082 1334 1482 147 3293 4382 121 7656 158 147 286 205 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 5643 131 111 5447 10824 191 29855 952 579 2211 20288 1865 137 10644 1078 168 2599 137 8556 3594 145 6615 175 30123 4967 30140 546 422 1552 579 2532 6497 4521 422 14092 5635 30135 422 18493 205 103 2694 137 494 1771 8516 25314 30113 205 4205 170 145 263 3678 147 10719 546 9774 8746 191 2927 25169 422 10156 422 3678 137 2529 947 422 4240 8501 1837 190 106 1790 191 1846 7675 5423 422 629 579 1653 1352 4756 20288 1865 422 137 19807 133 16508 17477 205 8746 1356 121 4205 239 145 263 10719 147 1020 546 2212 20288 1422 8509 22722 4198 422 1422 8509 22722 1738 5200 422 1846 7887 5423 422 6663 3590 1837 422 20288 9324 137 10644 422 4494 3590 437 2257 422 2599 4104 422 947 137 8556 3905 131 17477 422 2599 3337 137 8556 121 111 2337 187 8862 422 137 2554 147 1658 111 1447 131 17477 121 8556 205 4205 286 5752 190 2057 579 5672 120 3293 422 555 188 4954 131 20288 849 137 1120 147 11210 2057 579 5672 120 3293 555 188 6002 1477 422 4703 12696 422 8384 5754 2202 422 5156 137 5447 4608 422 5156 137 9447 579 5156 3624 204 6221 422 17068 1265 1837 137 1234 10914 6019 205 4205 305 145 10619 5691 546 15817 709 5691 137 13557 121 111 5672 137 3082 1334 1482 147 3293 4382 121 7656 158 147 286 205 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] succin ##ylated di ##osc ##ore ##a cay ##enen ##sis starch : effect of reaction parameters and character ##isation [SEP] in starch increased as the concentration of succin ##ic anhydr ##ide , time and temperature increased . optimal ds ( 0 . 217 ) and % succin ##yl content of 11 . 82 could be achieved at starch : catalyst ( pyridine ) ratio of 1 : 1 , succin ##ic anhydr ##ide concentration of 4 % w / w at 120 degrees c and 4 ##h of reaction . succin ##ylation did not alter starch crystall ##inity remarkably . native starch granules appeared oval , round and spherical with sizes in the range 153 ##1 mu m ( length ) and 102 ##0 mu m ( width ) . both swelling capacity and solubility improved after succin ##ylation . succin ##ylation increased paste clarity and it was dependent of the ds and storage time for the paste . past ##ing temperature was reduced after modification and as the ds increased among the starch derivatives . the peak viscosity of the native starch was lower than the viscos ##ities of all the modified starch ##es . succin ##ylation reduced set ##back values but increased breakdown . percentage syn ##ere ##sis was reduced after succin ##ylation and progressive increases in percentage syn ##ere ##sis were observed as the number of cycles of freezing ##tha ##wing increased . in vitro enzyme digest ##ibility reduced after succin ##ylation and progressively as the ds of the modified starch increased [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] succin ##ylated di ##osc ##ore ##a cay ##enen ##sis starch : effect of reaction parameters and character ##isation [SEP] in starch increased as the concentration of succin ##ic anhydr ##ide , time and temperature increased . optimal ds ( 0 . 217 ) and % succin ##yl content of 11 . 82 could be achieved at starch : catalyst ( pyridine ) ratio of 1 : 1 , succin ##ic anhydr ##ide concentration of 4 % w / w at 120 degrees c and 4 ##h of reaction . succin ##ylation did not alter starch crystall ##inity remarkably . native starch granules appeared oval , round and spherical with sizes in the range 153 ##1 mu m ( length ) and 102 ##0 mu m ( width ) . both swelling capacity and solubility improved after succin ##ylation . succin ##ylation increased paste clarity and it was dependent of the ds and storage time for the paste . past ##ing temperature was reduced after modification and as the ds increased among the starch derivatives . the peak viscosity of the native starch was lower than the viscos ##ities of all the modified starch ##es . succin ##ylation reduced set ##back values but increased breakdown . percentage syn ##ere ##sis was reduced after succin ##ylation and progressive increases in percentage syn ##ere ##sis were observed as the number of cycles of freezing ##tha ##wing increased . in vitro enzyme digest ##ibility reduced after succin ##ylation and progressively as the ds of the modified starch increased [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 21510 8471 346 26624 257 30110 29422 11298 4554 13881 862 907 131 2426 1496 137 954 3502 103 121 13881 1175 188 111 1840 131 21510 141 26911 390 422 532 137 1633 1175 205 2409 4415 145 244 205 17982 546 137 1863 21510 654 2118 131 1021 205 8707 968 195 3178 235 13881 862 8432 145 26656 546 1857 131 158 862 158 422 21510 141 26911 390 1840 131 286 1863 124 1352 124 235 5897 5634 115 137 286 30117 131 2426 205 21510 4218 1544 302 4222 13881 6592 4096 13688 205 6227 13881 16040 5844 22473 422 5194 137 10048 190 5155 121 111 1493 18308 30130 7229 127 145 1755 546 137 11480 30131 7229 127 145 4897 546 205 655 13648 2900 137 13601 3007 647 21510 4218 205 21510 4218 1175 17519 16585 137 256 241 3644 131 111 4415 137 4197 532 168 111 17519 205 3648 140 1633 241 1797 647 5945 137 188 111 4415 1175 1247 111 13881 6823 205 111 2391 11212 131 111 6227 13881 241 1268 506 111 10716 1003 131 355 111 3793 13881 123 205 21510 4218 1797 610 2686 988 563 1175 12236 205 3592 977 8602 4554 241 1797 647 21510 4218 137 8381 2348 121 3592 977 8602 4554 267 1058 188 111 649 131 5119 131 17247 29191 17718 1175 205 121 3335 3823 21525 3554 1797 647 21510 4218 137 15116 188 111 4415 131 111 3793 13881 1175 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 21510 8471 346 26624 257 30110 29422 11298 4554 13881 862 907 131 2426 1496 137 954 3502 103 121 13881 1175 188 111 1840 131 21510 141 26911 390 422 532 137 1633 1175 205 2409 4415 145 244 205 17982 546 137 1863 21510 654 2118 131 1021 205 8707 968 195 3178 235 13881 862 8432 145 26656 546 1857 131 158 862 158 422 21510 141 26911 390 1840 131 286 1863 124 1352 124 235 5897 5634 115 137 286 30117 131 2426 205 21510 4218 1544 302 4222 13881 6592 4096 13688 205 6227 13881 16040 5844 22473 422 5194 137 10048 190 5155 121 111 1493 18308 30130 7229 127 145 1755 546 137 11480 30131 7229 127 145 4897 546 205 655 13648 2900 137 13601 3007 647 21510 4218 205 21510 4218 1175 17519 16585 137 256 241 3644 131 111 4415 137 4197 532 168 111 17519 205 3648 140 1633 241 1797 647 5945 137 188 111 4415 1175 1247 111 13881 6823 205 111 2391 11212 131 111 6227 13881 241 1268 506 111 10716 1003 131 355 111 3793 13881 123 205 21510 4218 1797 610 2686 988 563 1175 12236 205 3592 977 8602 4554 241 1797 647 21510 4218 137 8381 2348 121 3592 977 8602 4554 267 1058 188 111 649 131 5119 131 17247 29191 17718 1175 205 121 3335 3823 21525 3554 1797 647 21510 4218 137 15116 188 111 4415 131 111 3793 13881 1175 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] experiences with the use of gli ##ric ##idia sep ##ium as animal fo ##dd ##er in northeast brazil . experi ##encia ##s com o us ##o da gli ##ric ##idia sep ##ium na ali ##ment ##aca ##o animal no nord ##este brasil ##ei ##ro [SEP] after a short introduction discussing the value of sil ##vo ##pas ##toral systems and outl ##ining the characteristics of gli ##ric ##idia sep ##ium , the paper describes the use of the species in brazil with emphasis on the northeast ##ern region . the various sil ##vo ##pas ##toral and agro ##sil ##vo ##pas ##toral systems used are described and data presented on the productivity ( biomass production and meat and milk production ) of selected systems [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] experiences with the use of gli ##ric ##idia sep ##ium as animal fo ##dd ##er in northeast brazil . experi ##encia ##s com o us ##o da gli ##ric ##idia sep ##ium na ali ##ment ##aca ##o animal no nord ##este brasil ##ei ##ro [SEP] after a short introduction discussing the value of sil ##vo ##pas ##toral systems and outl ##ining the characteristics of gli ##ric ##idia sep ##ium , the paper describes the use of the species in brazil with emphasis on the northeast ##ern region . the various sil ##vo ##pas ##toral and agro ##sil ##vo ##pas ##toral systems used are described and data presented on the productivity ( biomass production and meat and milk production ) of selected systems [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 6274 190 111 626 131 9301 782 17935 9892 888 188 3561 2941 6529 114 121 24994 7689 205 1852 14129 30113 209 116 227 30112 2762 9301 782 17935 9892 888 2431 14360 225 11266 30112 3561 425 16206 14930 25238 6422 136 103 647 106 2001 2067 16875 111 973 131 2622 9496 17268 15944 1078 137 8734 2620 111 2087 131 9301 782 17935 9892 888 422 111 1203 5223 111 626 131 111 1578 121 7689 190 9581 191 111 24994 1841 1025 205 111 1711 2622 9496 17268 15944 137 22362 11424 9496 17268 15944 1078 501 220 1356 137 453 1632 191 111 7381 145 7537 1865 137 12782 137 7435 1865 546 131 2350 1078 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 6274 190 111 626 131 9301 782 17935 9892 888 188 3561 2941 6529 114 121 24994 7689 205 1852 14129 30113 209 116 227 30112 2762 9301 782 17935 9892 888 2431 14360 225 11266 30112 3561 425 16206 14930 25238 6422 136 103 647 106 2001 2067 16875 111 973 131 2622 9496 17268 15944 1078 137 8734 2620 111 2087 131 9301 782 17935 9892 888 422 111 1203 5223 111 626 131 111 1578 121 7689 190 9581 191 111 24994 1841 1025 205 111 1711 2622 9496 17268 15944 137 22362 11424 9496 17268 15944 1078 501 220 1356 137 453 1632 191 111 7381 145 7537 1865 137 12782 137 7435 1865 546 131 2350 1078 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 10000 of 20252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 10000 of 20252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 20000 of 20252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 20000 of 20252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used for model gpu 0\n",
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from study_type_multi\\model.ckpt-2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from study_type_multi\\model.ckpt-2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] impact of promoting sustainable agriculture in born ##o ( pros ##ab ) program on adoption of improved crop varieties in born ##o state of nigeria [SEP] this paper evaluates the impact of the intervention of promoting sustainable agriculture in born ##o ( pros ##ab ) project on the live ##li ##hood ##s of farming households in born ##o state , nigeria . specifically , the paper identifies and provides information on farmers ' adoption of improved crop varieties introduced by pros ##ab ; measures their adoption rates , and analyzes the factors that affect the probability and intensity of adoption of the crop varieties . a multi - stage sampling procedure was used to select respondents in three agro ##ec ##ological zones ( ae ##zs ) - the northern guinea sav ##anna ( ng ##s ) , southern guinea sav ##anna ( sg ##s ) , and the sah ##elian sav ##anna ( ss ) , and a structured questionnaire was used to collect data on their socioeconomic characteristics and adoption parameters in the 2009 period . data were analyzed using descriptive statistics and the to ##bit regression techniques . the paper demonstrates how adoption and use decisions were significantly influenced by education , membership in associations , and yield in the ng ##s ; yield and labor in the sg ##s , and age and labor in the ss , implying that these variables , and particularly yield , are important for consideration by development agencies who are interested in improving agricultural production [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] impact of promoting sustainable agriculture in born ##o ( pros ##ab ) program on adoption of improved crop varieties in born ##o state of nigeria [SEP] this paper evaluates the impact of the intervention of promoting sustainable agriculture in born ##o ( pros ##ab ) project on the live ##li ##hood ##s of farming households in born ##o state , nigeria . specifically , the paper identifies and provides information on farmers ' adoption of improved crop varieties introduced by pros ##ab ; measures their adoption rates , and analyzes the factors that affect the probability and intensity of adoption of the crop varieties . a multi - stage sampling procedure was used to select respondents in three agro ##ec ##ological zones ( ae ##zs ) - the northern guinea sav ##anna ( ng ##s ) , southern guinea sav ##anna ( sg ##s ) , and the sah ##elian sav ##anna ( ss ) , and a structured questionnaire was used to collect data on their socioeconomic characteristics and adoption parameters in the 2009 period . data were analyzed using descriptive statistics and the to ##bit regression techniques . the paper demonstrates how adoption and use decisions were significantly influenced by education , membership in associations , and yield in the ng ##s ; yield and labor in the sg ##s , and age and labor in the ss , implying that these variables , and particularly yield , are important for consideration by development agencies who are interested in improving agricultural production [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 2141 131 10302 10886 9879 121 9856 30112 145 12000 201 546 1618 191 9714 131 3007 8501 13600 121 9856 30112 1098 131 17964 103 238 1203 18224 111 2141 131 111 3832 131 10302 10886 9879 121 9856 30112 145 12000 201 546 2575 191 111 6489 3206 4402 30113 131 19323 9654 121 9856 30112 1098 422 17964 205 3697 422 111 1203 12369 137 2315 776 191 12288 2505 9714 131 3007 8501 13600 3376 214 12000 201 1814 2554 547 9714 1975 422 137 21127 111 1391 198 2606 111 2064 137 3165 131 9714 131 111 8501 13600 205 106 869 579 2410 3597 2272 241 501 147 5470 6634 121 874 22362 155 872 10166 145 8674 22882 546 579 111 7926 16616 7110 25215 145 3312 30113 546 422 7663 16616 7110 25215 145 10410 30113 546 422 137 111 16905 14005 7110 25215 145 3712 546 422 137 106 7908 5692 241 501 147 9921 453 191 547 13044 2087 137 9714 1496 121 111 4758 1275 205 453 267 2549 487 10363 4530 137 111 147 2386 3089 2190 205 111 1203 7499 539 9714 137 626 5207 267 1357 6452 214 2870 422 9610 121 6225 422 137 2210 121 111 3312 30113 1814 2210 137 2193 121 111 10410 30113 422 137 1407 137 2193 121 111 3712 422 14466 198 407 1762 422 137 3220 2210 422 220 1060 168 5976 214 1120 11934 975 220 7301 121 6017 7675 1865 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 2141 131 10302 10886 9879 121 9856 30112 145 12000 201 546 1618 191 9714 131 3007 8501 13600 121 9856 30112 1098 131 17964 103 238 1203 18224 111 2141 131 111 3832 131 10302 10886 9879 121 9856 30112 145 12000 201 546 2575 191 111 6489 3206 4402 30113 131 19323 9654 121 9856 30112 1098 422 17964 205 3697 422 111 1203 12369 137 2315 776 191 12288 2505 9714 131 3007 8501 13600 3376 214 12000 201 1814 2554 547 9714 1975 422 137 21127 111 1391 198 2606 111 2064 137 3165 131 9714 131 111 8501 13600 205 106 869 579 2410 3597 2272 241 501 147 5470 6634 121 874 22362 155 872 10166 145 8674 22882 546 579 111 7926 16616 7110 25215 145 3312 30113 546 422 7663 16616 7110 25215 145 10410 30113 546 422 137 111 16905 14005 7110 25215 145 3712 546 422 137 106 7908 5692 241 501 147 9921 453 191 547 13044 2087 137 9714 1496 121 111 4758 1275 205 453 267 2549 487 10363 4530 137 111 147 2386 3089 2190 205 111 1203 7499 539 9714 137 626 5207 267 1357 6452 214 2870 422 9610 121 6225 422 137 2210 121 111 3312 30113 1814 2210 137 2193 121 111 10410 30113 422 137 1407 137 2193 121 111 3712 422 14466 198 407 1762 422 137 3220 2210 422 220 1060 168 5976 214 1120 11934 975 220 7301 121 6017 7675 1865 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] exploring indigenous soil knowledge in go ##ure ##l yo ##ba , sen ##eg ##al : two years as a peace corp ##s volunteer [SEP] through my participation in the master ##s international program at the university of california , davis ( uc ##d ) and during my service as a sustainable agriculture peace corp ##s volunteer in the village of go ##ure ##l yo ##ba , sen ##eg ##al , i explored the nature of indigenous soil knowledge to better understand how farmers living at a subs ##istence level recognize soil resources and whether this knowledge is used to make crop management decisions . a two - part investigation was conducted , first , in the village where farmers were interviewed and second , back in the usa where soils were analysed . village ##rs recognize seven main types of soil . these are fa ##hr ##o , bal ##ee ##ri , ken ##yer ##i , seen ##o , ka ##hran - ka ##hran , wen ##do ##u , and ka ##hr ##el , which are differentiated by colour , texture , drainage class , and vegetation . the first four are agricultural soils , and the village ##rs know the crops best suited to each soil . the soils of go ##ure ##l yo ##ba were found to have low fertility with average total n ( for samples tested , n = 16 ) of 0 . 25 g kg ##1 . average total c and extract ##able p was 2 . 9 [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] exploring indigenous soil knowledge in go ##ure ##l yo ##ba , sen ##eg ##al : two years as a peace corp ##s volunteer [SEP] through my participation in the master ##s international program at the university of california , davis ( uc ##d ) and during my service as a sustainable agriculture peace corp ##s volunteer in the village of go ##ure ##l yo ##ba , sen ##eg ##al , i explored the nature of indigenous soil knowledge to better understand how farmers living at a subs ##istence level recognize soil resources and whether this knowledge is used to make crop management decisions . a two - part investigation was conducted , first , in the village where farmers were interviewed and second , back in the usa where soils were analysed . village ##rs recognize seven main types of soil . these are fa ##hr ##o , bal ##ee ##ri , ken ##yer ##i , seen ##o , ka ##hran - ka ##hran , wen ##do ##u , and ka ##hr ##el , which are differentiated by colour , texture , drainage class , and vegetation . the first four are agricultural soils , and the village ##rs know the crops best suited to each soil . the soils of go ##ure ##l yo ##ba were found to have low fertility with average total n ( for samples tested , n = 16 ) of 0 . 25 g kg ##1 . average total c and extract ##able p was 2 . 9 [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 12038 16508 3115 1767 121 796 221 30115 1778 3331 422 1311 4980 120 862 502 1320 188 106 20244 7241 30113 23069 103 833 1536 6163 121 111 7945 30113 2565 1618 235 111 1224 131 6739 422 12763 145 6359 30118 546 137 781 1536 2289 188 106 10886 9879 20244 7241 30113 23069 121 111 17719 131 796 221 30115 1778 3331 422 1311 4980 120 422 259 7293 111 2540 131 16508 3115 1767 147 1883 2008 539 12288 5404 235 106 1763 28674 615 8974 3115 2965 137 1681 238 1767 165 501 147 2113 8501 1837 5207 205 106 502 579 1188 4534 241 2728 422 705 422 121 111 17719 582 12288 267 17985 137 971 422 1542 121 111 2394 582 10511 267 6814 205 17719 2771 8974 5152 936 1910 131 3115 205 407 220 2016 2506 30112 422 2679 1900 212 422 10248 6883 30109 422 2187 30112 422 5656 25178 579 5656 25178 422 11592 2904 30120 422 137 5656 2506 154 422 334 220 9923 214 9389 422 8690 422 12808 844 422 137 9723 205 111 705 1379 220 7675 10511 422 137 111 17719 2771 871 111 12760 2172 13022 147 535 3115 205 111 10511 131 796 221 30115 1778 3331 267 797 147 360 629 12508 190 1568 1114 146 145 168 1488 2733 422 146 275 1107 546 131 244 205 1552 159 5036 30130 205 1568 1114 115 137 4070 318 118 241 170 205 514 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 12038 16508 3115 1767 121 796 221 30115 1778 3331 422 1311 4980 120 862 502 1320 188 106 20244 7241 30113 23069 103 833 1536 6163 121 111 7945 30113 2565 1618 235 111 1224 131 6739 422 12763 145 6359 30118 546 137 781 1536 2289 188 106 10886 9879 20244 7241 30113 23069 121 111 17719 131 796 221 30115 1778 3331 422 1311 4980 120 422 259 7293 111 2540 131 16508 3115 1767 147 1883 2008 539 12288 5404 235 106 1763 28674 615 8974 3115 2965 137 1681 238 1767 165 501 147 2113 8501 1837 5207 205 106 502 579 1188 4534 241 2728 422 705 422 121 111 17719 582 12288 267 17985 137 971 422 1542 121 111 2394 582 10511 267 6814 205 17719 2771 8974 5152 936 1910 131 3115 205 407 220 2016 2506 30112 422 2679 1900 212 422 10248 6883 30109 422 2187 30112 422 5656 25178 579 5656 25178 422 11592 2904 30120 422 137 5656 2506 154 422 334 220 9923 214 9389 422 8690 422 12808 844 422 137 9723 205 111 705 1379 220 7675 10511 422 137 111 17719 2771 871 111 12760 2172 13022 147 535 3115 205 111 10511 131 796 221 30115 1778 3331 267 797 147 360 629 12508 190 1568 1114 146 145 168 1488 2733 422 146 275 1107 546 131 244 205 1552 159 5036 30130 205 1568 1114 115 137 4070 318 118 241 170 205 514 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] farm households ' perception on climate change and adaptation practices a case from mountain district of ne ##pal [SEP] purpose - this paper aims to assess the farming community ' s perception on important parameters of climate change and identify major practices and technologies adopted to mitigate the impacts of climate change and their determinants in mountain district of ne ##pal . being an agr ##arian economy and dependent on mon ##soon rain , the impact of climate change on agricultural productivity and production has been experienced . different adaptation strategies have been adopted by the communities to cope with the consequences of climate change . design / methodology / approach - four village development committees ( vd ##cs ) situated in the buffer zone of lang ##tan ##g national park of ras ##uw ##a district representing rice , wheat , maize and potato production area was pur ##pos ##ive ##ly selected for this study . a two - stage sampling technique was adopted for data and information collection . thirty households from each vd ##c were randomly selected . data on the socio - economic and climate change perception were collected using structured questionnaire . a binary logistic regression technique was used to identify the determinants of climate change adaptation technologies and practices . findings - the farmers ' decisions whether to adopt climate change adaptation technologies are governed by the size of land ##holding , perceived threat of climate change to food security , education level and gender of the interview ##ee [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] farm households ' perception on climate change and adaptation practices a case from mountain district of ne ##pal [SEP] purpose - this paper aims to assess the farming community ' s perception on important parameters of climate change and identify major practices and technologies adopted to mitigate the impacts of climate change and their determinants in mountain district of ne ##pal . being an agr ##arian economy and dependent on mon ##soon rain , the impact of climate change on agricultural productivity and production has been experienced . different adaptation strategies have been adopted by the communities to cope with the consequences of climate change . design / methodology / approach - four village development committees ( vd ##cs ) situated in the buffer zone of lang ##tan ##g national park of ras ##uw ##a district representing rice , wheat , maize and potato production area was pur ##pos ##ive ##ly selected for this study . a two - stage sampling technique was adopted for data and information collection . thirty households from each vd ##c were randomly selected . data on the socio - economic and climate change perception were collected using structured questionnaire . a binary logistic regression technique was used to identify the determinants of climate change adaptation technologies and practices . findings - the farmers ' decisions whether to adopt climate change adaptation technologies are governed by the size of land ##holding , perceived threat of climate change to food security , education level and gender of the interview ##ee [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 5947 9654 2505 6028 191 6002 1477 137 5846 5423 106 820 263 12273 10689 131 287 11014 103 3559 579 238 1203 7415 147 1285 111 19323 2928 2505 112 6028 191 1060 1496 131 6002 1477 137 2743 1626 5423 137 4198 6018 147 16735 111 7681 131 6002 1477 137 547 10687 121 12273 10689 131 287 11014 205 1558 130 4545 9371 7425 137 3644 191 775 27484 7112 422 111 2141 131 6002 1477 191 7675 7381 137 1865 434 528 5770 205 643 5846 3236 360 528 6018 214 111 5904 147 14649 190 111 6091 131 6002 1477 205 899 1352 4964 1352 1139 579 1379 17719 1120 23073 145 15009 1647 546 15657 121 111 3520 5493 131 1973 16161 30123 2227 7713 131 9461 17553 30110 10689 5674 8058 422 10094 422 13156 137 16480 1865 1590 241 1434 451 1090 179 2350 168 238 527 205 106 502 579 2410 3597 2358 241 6018 168 453 137 776 3445 205 12498 9654 263 535 15009 30116 267 5007 2350 205 453 191 111 8001 579 3587 137 6002 1477 6028 267 2760 487 7908 5692 205 106 5067 7312 3089 2358 241 501 147 2743 111 10687 131 6002 1477 5846 4198 137 5423 205 2116 579 111 12288 2505 5207 1681 147 9265 6002 1477 5846 4198 220 17384 214 111 1243 131 2882 28696 422 5545 8057 131 6002 1477 147 2599 3594 422 2870 615 137 4703 131 111 4489 1900 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 5947 9654 2505 6028 191 6002 1477 137 5846 5423 106 820 263 12273 10689 131 287 11014 103 3559 579 238 1203 7415 147 1285 111 19323 2928 2505 112 6028 191 1060 1496 131 6002 1477 137 2743 1626 5423 137 4198 6018 147 16735 111 7681 131 6002 1477 137 547 10687 121 12273 10689 131 287 11014 205 1558 130 4545 9371 7425 137 3644 191 775 27484 7112 422 111 2141 131 6002 1477 191 7675 7381 137 1865 434 528 5770 205 643 5846 3236 360 528 6018 214 111 5904 147 14649 190 111 6091 131 6002 1477 205 899 1352 4964 1352 1139 579 1379 17719 1120 23073 145 15009 1647 546 15657 121 111 3520 5493 131 1973 16161 30123 2227 7713 131 9461 17553 30110 10689 5674 8058 422 10094 422 13156 137 16480 1865 1590 241 1434 451 1090 179 2350 168 238 527 205 106 502 579 2410 3597 2358 241 6018 168 453 137 776 3445 205 12498 9654 263 535 15009 30116 267 5007 2350 205 453 191 111 8001 579 3587 137 6002 1477 6028 267 2760 487 7908 5692 205 106 5067 7312 3089 2358 241 501 147 2743 111 10687 131 6002 1477 5846 4198 137 5423 205 2116 579 111 12288 2505 5207 1681 147 9265 6002 1477 5846 4198 220 17384 214 111 1243 131 2882 28696 422 5545 8057 131 6002 1477 147 2599 3594 422 2870 615 137 4703 131 111 4489 1900 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] association of small ##holder dairy farmers management and milk ##ing practices with bacterial quality of milk in mb ##ey ##a , tanzania [SEP] the study determine the association between some of management ##s and milk ##ing practices with bacterial counts in dairy herd among small holder dairy farmers of mb ##ey ##a and mb ##oz ##i districts of mb ##ey ##a , tanzania . a cross - sectional study was conducted with the aim of assessing housing structures , condition and management , milk ##ing procedure and milk handling in the study area . a total of 192 raw milk samples were collected from farmers herd ##s with at least one lact ##ating dairy cow . samples were tested for total bacteria count ( tb ##c ) , total coli ##form counts ( tcc ) and total staphylococcus counts ( tsc ) using standard procedures . all respondents practiced hand milk ##ing and 96 . 9 % of the respondents washed hands before milk ##ing . about 57 . 8 % of the respondents used same tow ##el to dry the ud ##der of all milk ##ing cows in the herd per milk ##ing time . furthermore , 42 . 2 % of the respondents used individual tow ##el for drying ud ##der and tea ##t . none of the respondents used pre milk ##ing , post milk ##ing dip ##ping or dry cow therapy . milk from cows kept in bar ##ns made from concrete floor had ( p < 0 . 00 [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] association of small ##holder dairy farmers management and milk ##ing practices with bacterial quality of milk in mb ##ey ##a , tanzania [SEP] the study determine the association between some of management ##s and milk ##ing practices with bacterial counts in dairy herd among small holder dairy farmers of mb ##ey ##a and mb ##oz ##i districts of mb ##ey ##a , tanzania . a cross - sectional study was conducted with the aim of assessing housing structures , condition and management , milk ##ing procedure and milk handling in the study area . a total of 192 raw milk samples were collected from farmers herd ##s with at least one lact ##ating dairy cow . samples were tested for total bacteria count ( tb ##c ) , total coli ##form counts ( tcc ) and total staphylococcus counts ( tsc ) using standard procedures . all respondents practiced hand milk ##ing and 96 . 9 % of the respondents washed hands before milk ##ing . about 57 . 8 % of the respondents used same tow ##el to dry the ud ##der of all milk ##ing cows in the herd per milk ##ing time . furthermore , 42 . 2 % of the respondents used individual tow ##el for drying ud ##der and tea ##t . none of the respondents used pre milk ##ing , post milk ##ing dip ##ping or dry cow therapy . milk from cows kept in bar ##ns made from concrete floor had ( p < 0 . 00 [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 2274 131 952 13643 14874 12288 1837 137 7435 140 5423 190 4667 1671 131 7435 121 6369 9257 30110 422 27281 103 111 527 2100 111 2274 467 693 131 1837 30113 137 7435 140 5423 190 4667 7274 121 14874 24593 1247 952 19347 14874 12288 131 6369 9257 30110 137 6369 9627 30109 17618 131 6369 9257 30110 422 27281 205 106 2057 579 27554 527 241 2728 190 111 2579 131 7836 10792 2484 422 803 137 1837 422 7435 140 2272 137 7435 8731 121 111 527 1590 205 106 1114 131 13054 6908 7435 1488 267 2760 263 12288 24593 30113 190 235 1823 482 6779 560 14874 8506 205 1488 267 2733 168 1114 4738 2232 145 6226 30116 546 422 1114 5703 321 7274 145 28331 546 137 1114 16808 7274 145 26800 546 487 1235 3622 205 355 6634 28371 1500 7435 140 137 7380 205 514 1863 131 111 6634 6069 12053 1548 7435 140 205 1011 5309 205 493 1863 131 111 6634 501 855 10755 154 147 5965 111 8722 343 131 355 7435 140 15627 121 111 24593 309 7435 140 532 205 2403 422 4637 205 170 1863 131 111 6634 501 1161 10755 154 168 15211 8722 343 137 15297 30108 205 4580 131 111 6634 501 382 7435 140 422 1422 7435 140 6332 5354 234 5965 8506 2223 205 7435 263 15627 6890 121 2261 1290 1827 263 9209 10298 883 145 118 962 244 205 3641 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 2274 131 952 13643 14874 12288 1837 137 7435 140 5423 190 4667 1671 131 7435 121 6369 9257 30110 422 27281 103 111 527 2100 111 2274 467 693 131 1837 30113 137 7435 140 5423 190 4667 7274 121 14874 24593 1247 952 19347 14874 12288 131 6369 9257 30110 137 6369 9627 30109 17618 131 6369 9257 30110 422 27281 205 106 2057 579 27554 527 241 2728 190 111 2579 131 7836 10792 2484 422 803 137 1837 422 7435 140 2272 137 7435 8731 121 111 527 1590 205 106 1114 131 13054 6908 7435 1488 267 2760 263 12288 24593 30113 190 235 1823 482 6779 560 14874 8506 205 1488 267 2733 168 1114 4738 2232 145 6226 30116 546 422 1114 5703 321 7274 145 28331 546 137 1114 16808 7274 145 26800 546 487 1235 3622 205 355 6634 28371 1500 7435 140 137 7380 205 514 1863 131 111 6634 6069 12053 1548 7435 140 205 1011 5309 205 493 1863 131 111 6634 501 855 10755 154 147 5965 111 8722 343 131 355 7435 140 15627 121 111 24593 309 7435 140 532 205 2403 422 4637 205 170 1863 131 111 6634 501 1161 10755 154 168 15211 8722 343 137 15297 30108 205 4580 131 111 6634 501 382 7435 140 422 1422 7435 140 6332 5354 234 5965 8506 2223 205 7435 263 15627 6890 121 2261 1290 1827 263 9209 10298 883 145 118 962 244 205 3641 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] assessing maize farmers ' adaptation strategies to climate change and variability in ghana [SEP] this study examined the adaptation strategies of maize farmers to climate change and variability in the eastern region of ghana using primary data collected from 150 maize farming households by the administration of structured questionnaires . the results of the multin ##omial logit regression revealed that rainfall perception , access to credit , and farming experience significantly influenced the adoption of recommended agricultural practices , whereas the adoption of soil - related strategies is influenced by gender and rainfall perception . farming experience and rainfall perception influenced the adoption of improved varieties strategies . this study highlights the need for the development of water resources for maize production in the context of the changing climate . in this respect , the crucial roles of the ghana irrigation development authority , the agricultural extension division of the ministry of food and agriculture , and other international organizations such as the japan international cooperation agency and the world bank regarding the development of irrigation facilities and the associated capacity building of the farmers are very important . finally , the formation of water user associations for the small ##holder farmers regarding the usage and maintenance of the irrigation facilities would be a step in the right direction [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] assessing maize farmers ' adaptation strategies to climate change and variability in ghana [SEP] this study examined the adaptation strategies of maize farmers to climate change and variability in the eastern region of ghana using primary data collected from 150 maize farming households by the administration of structured questionnaires . the results of the multin ##omial logit regression revealed that rainfall perception , access to credit , and farming experience significantly influenced the adoption of recommended agricultural practices , whereas the adoption of soil - related strategies is influenced by gender and rainfall perception . farming experience and rainfall perception influenced the adoption of improved varieties strategies . this study highlights the need for the development of water resources for maize production in the context of the changing climate . in this respect , the crucial roles of the ghana irrigation development authority , the agricultural extension division of the ministry of food and agriculture , and other international organizations such as the japan international cooperation agency and the world bank regarding the development of irrigation facilities and the associated capacity building of the farmers are very important . finally , the formation of water user associations for the small ##holder farmers regarding the usage and maintenance of the irrigation facilities would be a step in the right direction [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 7836 13156 12288 2505 5846 3236 147 6002 1477 137 4638 121 26274 103 238 527 2959 111 5846 3236 131 13156 12288 147 6002 1477 137 4638 121 111 10309 1025 131 26274 487 1916 453 2760 263 5413 13156 19323 9654 214 111 3762 131 7908 11682 205 111 545 131 111 18231 4025 25565 3089 2861 198 12660 6028 422 1899 147 8388 422 137 19323 2899 1357 6452 111 9714 131 5805 7675 5423 422 2255 111 9714 131 3115 579 1482 3236 165 6452 214 4703 137 12660 6028 205 19323 2899 137 12660 6028 6452 111 9714 131 3007 13600 3236 205 238 527 11878 111 965 168 111 1120 131 1506 2965 168 13156 1865 121 111 2220 131 111 5468 6002 205 121 238 2646 422 111 5784 5370 131 111 26274 15055 1120 11226 422 111 7675 3840 5277 131 111 9795 131 2599 137 9879 422 137 494 2565 6938 555 188 111 4838 2565 9226 8118 137 111 2399 5108 3560 111 1120 131 15055 7845 137 111 1111 2900 4736 131 111 12288 220 1248 1060 205 2285 422 111 2256 131 1506 1824 6225 168 111 952 13643 12288 3560 111 6323 137 5365 131 111 15055 7845 989 195 106 1371 121 111 2083 3041 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 7836 13156 12288 2505 5846 3236 147 6002 1477 137 4638 121 26274 103 238 527 2959 111 5846 3236 131 13156 12288 147 6002 1477 137 4638 121 111 10309 1025 131 26274 487 1916 453 2760 263 5413 13156 19323 9654 214 111 3762 131 7908 11682 205 111 545 131 111 18231 4025 25565 3089 2861 198 12660 6028 422 1899 147 8388 422 137 19323 2899 1357 6452 111 9714 131 5805 7675 5423 422 2255 111 9714 131 3115 579 1482 3236 165 6452 214 4703 137 12660 6028 205 19323 2899 137 12660 6028 6452 111 9714 131 3007 13600 3236 205 238 527 11878 111 965 168 111 1120 131 1506 2965 168 13156 1865 121 111 2220 131 111 5468 6002 205 121 238 2646 422 111 5784 5370 131 111 26274 15055 1120 11226 422 111 7675 3840 5277 131 111 9795 131 2599 137 9879 422 137 494 2565 6938 555 188 111 4838 2565 9226 8118 137 111 2399 5108 3560 111 1120 131 15055 7845 137 111 1111 2900 4736 131 111 12288 220 1248 1060 205 2285 422 111 2256 131 1506 1824 6225 168 111 952 13643 12288 3560 111 6323 137 5365 131 111 15055 7845 989 195 106 1371 121 111 2083 3041 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used for model gpu 0\n",
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from study_type_multi\\model.ckpt-2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from study_type_multi\\model.ckpt-2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 1952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 1952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] impact of promoting sustainable agriculture in born ##o ( pros ##ab ) program on adoption of improved crop varieties in born ##o state of nigeria [SEP] this paper evaluates the impact of the intervention of promoting sustainable agriculture in born ##o ( pros ##ab ) project on the live ##li ##hood ##s of farming households in born ##o state , nigeria . specifically , the paper identifies and provides information on farmers ' adoption of improved crop varieties introduced by pros ##ab ; measures their adoption rates , and analyzes the factors that affect the probability and intensity of adoption of the crop varieties . a multi - stage sampling procedure was used to select respondents in three agro ##ec ##ological zones ( ae ##zs ) - the northern guinea sav ##anna ( ng ##s ) , southern guinea sav ##anna ( sg ##s ) , and the sah ##elian sav ##anna ( ss ) , and a structured questionnaire was used to collect data on their socioeconomic characteristics and adoption parameters in the 2009 period . data were analyzed using descriptive statistics and the to ##bit regression techniques . the paper demonstrates how adoption and use decisions were significantly influenced by education , membership in associations , and yield in the ng ##s ; yield and labor in the sg ##s , and age and labor in the ss , implying that these variables , and particularly yield , are important for consideration by development agencies who are interested in improving agricultural production [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] impact of promoting sustainable agriculture in born ##o ( pros ##ab ) program on adoption of improved crop varieties in born ##o state of nigeria [SEP] this paper evaluates the impact of the intervention of promoting sustainable agriculture in born ##o ( pros ##ab ) project on the live ##li ##hood ##s of farming households in born ##o state , nigeria . specifically , the paper identifies and provides information on farmers ' adoption of improved crop varieties introduced by pros ##ab ; measures their adoption rates , and analyzes the factors that affect the probability and intensity of adoption of the crop varieties . a multi - stage sampling procedure was used to select respondents in three agro ##ec ##ological zones ( ae ##zs ) - the northern guinea sav ##anna ( ng ##s ) , southern guinea sav ##anna ( sg ##s ) , and the sah ##elian sav ##anna ( ss ) , and a structured questionnaire was used to collect data on their socioeconomic characteristics and adoption parameters in the 2009 period . data were analyzed using descriptive statistics and the to ##bit regression techniques . the paper demonstrates how adoption and use decisions were significantly influenced by education , membership in associations , and yield in the ng ##s ; yield and labor in the sg ##s , and age and labor in the ss , implying that these variables , and particularly yield , are important for consideration by development agencies who are interested in improving agricultural production [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 2141 131 10302 10886 9879 121 9856 30112 145 12000 201 546 1618 191 9714 131 3007 8501 13600 121 9856 30112 1098 131 17964 103 238 1203 18224 111 2141 131 111 3832 131 10302 10886 9879 121 9856 30112 145 12000 201 546 2575 191 111 6489 3206 4402 30113 131 19323 9654 121 9856 30112 1098 422 17964 205 3697 422 111 1203 12369 137 2315 776 191 12288 2505 9714 131 3007 8501 13600 3376 214 12000 201 1814 2554 547 9714 1975 422 137 21127 111 1391 198 2606 111 2064 137 3165 131 9714 131 111 8501 13600 205 106 869 579 2410 3597 2272 241 501 147 5470 6634 121 874 22362 155 872 10166 145 8674 22882 546 579 111 7926 16616 7110 25215 145 3312 30113 546 422 7663 16616 7110 25215 145 10410 30113 546 422 137 111 16905 14005 7110 25215 145 3712 546 422 137 106 7908 5692 241 501 147 9921 453 191 547 13044 2087 137 9714 1496 121 111 4758 1275 205 453 267 2549 487 10363 4530 137 111 147 2386 3089 2190 205 111 1203 7499 539 9714 137 626 5207 267 1357 6452 214 2870 422 9610 121 6225 422 137 2210 121 111 3312 30113 1814 2210 137 2193 121 111 10410 30113 422 137 1407 137 2193 121 111 3712 422 14466 198 407 1762 422 137 3220 2210 422 220 1060 168 5976 214 1120 11934 975 220 7301 121 6017 7675 1865 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 2141 131 10302 10886 9879 121 9856 30112 145 12000 201 546 1618 191 9714 131 3007 8501 13600 121 9856 30112 1098 131 17964 103 238 1203 18224 111 2141 131 111 3832 131 10302 10886 9879 121 9856 30112 145 12000 201 546 2575 191 111 6489 3206 4402 30113 131 19323 9654 121 9856 30112 1098 422 17964 205 3697 422 111 1203 12369 137 2315 776 191 12288 2505 9714 131 3007 8501 13600 3376 214 12000 201 1814 2554 547 9714 1975 422 137 21127 111 1391 198 2606 111 2064 137 3165 131 9714 131 111 8501 13600 205 106 869 579 2410 3597 2272 241 501 147 5470 6634 121 874 22362 155 872 10166 145 8674 22882 546 579 111 7926 16616 7110 25215 145 3312 30113 546 422 7663 16616 7110 25215 145 10410 30113 546 422 137 111 16905 14005 7110 25215 145 3712 546 422 137 106 7908 5692 241 501 147 9921 453 191 547 13044 2087 137 9714 1496 121 111 4758 1275 205 453 267 2549 487 10363 4530 137 111 147 2386 3089 2190 205 111 1203 7499 539 9714 137 626 5207 267 1357 6452 214 2870 422 9610 121 6225 422 137 2210 121 111 3312 30113 1814 2210 137 2193 121 111 10410 30113 422 137 1407 137 2193 121 111 3712 422 14466 198 407 1762 422 137 3220 2210 422 220 1060 168 5976 214 1120 11934 975 220 7301 121 6017 7675 1865 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] impact of promoting sustainable agriculture in born ##o ( pros ##ab ) program on adoption of improved crop varieties in born ##o state of nigeria [SEP] and use decisions were significantly influenced by education , membership in associations , and yield in the ng ##s ; yield and labor in the sg ##s , and age and labor in the ss , implying that these variables , and particularly yield , are important for consideration by development agencies who are interested in improving agricultural production and the live ##li ##hood ##s of the people [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] impact of promoting sustainable agriculture in born ##o ( pros ##ab ) program on adoption of improved crop varieties in born ##o state of nigeria [SEP] and use decisions were significantly influenced by education , membership in associations , and yield in the ng ##s ; yield and labor in the sg ##s , and age and labor in the ss , implying that these variables , and particularly yield , are important for consideration by development agencies who are interested in improving agricultural production and the live ##li ##hood ##s of the people [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 2141 131 10302 10886 9879 121 9856 30112 145 12000 201 546 1618 191 9714 131 3007 8501 13600 121 9856 30112 1098 131 17964 103 137 626 5207 267 1357 6452 214 2870 422 9610 121 6225 422 137 2210 121 111 3312 30113 1814 2210 137 2193 121 111 10410 30113 422 137 1407 137 2193 121 111 3712 422 14466 198 407 1762 422 137 3220 2210 422 220 1060 168 5976 214 1120 11934 975 220 7301 121 6017 7675 1865 137 111 6489 3206 4402 30113 131 111 2325 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 2141 131 10302 10886 9879 121 9856 30112 145 12000 201 546 1618 191 9714 131 3007 8501 13600 121 9856 30112 1098 131 17964 103 137 626 5207 267 1357 6452 214 2870 422 9610 121 6225 422 137 2210 121 111 3312 30113 1814 2210 137 2193 121 111 10410 30113 422 137 1407 137 2193 121 111 3712 422 14466 198 407 1762 422 137 3220 2210 422 220 1060 168 5976 214 1120 11934 975 220 7301 121 6017 7675 1865 137 111 6489 3206 4402 30113 131 111 2325 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] exploring indigenous soil knowledge in go ##ure ##l yo ##ba , sen ##eg ##al : two years as a peace corp ##s volunteer [SEP] through my participation in the master ##s international program at the university of california , davis ( uc ##d ) and during my service as a sustainable agriculture peace corp ##s volunteer in the village of go ##ure ##l yo ##ba , sen ##eg ##al , i explored the nature of indigenous soil knowledge to better understand how farmers living at a subs ##istence level recognize soil resources and whether this knowledge is used to make crop management decisions . a two - part investigation was conducted , first , in the village where farmers were interviewed and second , back in the usa where soils were analysed . village ##rs recognize seven main types of soil . these are fa ##hr ##o , bal ##ee ##ri , ken ##yer ##i , seen ##o , ka ##hran - ka ##hran , wen ##do ##u , and ka ##hr ##el , which are differentiated by colour , texture , drainage class , and vegetation . the first four are agricultural soils , and the village ##rs know the crops best suited to each soil . the soils of go ##ure ##l yo ##ba were found to have low fertility with average total n ( for samples tested , n = 16 ) of 0 . 25 g kg ##1 . average total c and extract ##able p was 2 . 9 [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] exploring indigenous soil knowledge in go ##ure ##l yo ##ba , sen ##eg ##al : two years as a peace corp ##s volunteer [SEP] through my participation in the master ##s international program at the university of california , davis ( uc ##d ) and during my service as a sustainable agriculture peace corp ##s volunteer in the village of go ##ure ##l yo ##ba , sen ##eg ##al , i explored the nature of indigenous soil knowledge to better understand how farmers living at a subs ##istence level recognize soil resources and whether this knowledge is used to make crop management decisions . a two - part investigation was conducted , first , in the village where farmers were interviewed and second , back in the usa where soils were analysed . village ##rs recognize seven main types of soil . these are fa ##hr ##o , bal ##ee ##ri , ken ##yer ##i , seen ##o , ka ##hran - ka ##hran , wen ##do ##u , and ka ##hr ##el , which are differentiated by colour , texture , drainage class , and vegetation . the first four are agricultural soils , and the village ##rs know the crops best suited to each soil . the soils of go ##ure ##l yo ##ba were found to have low fertility with average total n ( for samples tested , n = 16 ) of 0 . 25 g kg ##1 . average total c and extract ##able p was 2 . 9 [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 12038 16508 3115 1767 121 796 221 30115 1778 3331 422 1311 4980 120 862 502 1320 188 106 20244 7241 30113 23069 103 833 1536 6163 121 111 7945 30113 2565 1618 235 111 1224 131 6739 422 12763 145 6359 30118 546 137 781 1536 2289 188 106 10886 9879 20244 7241 30113 23069 121 111 17719 131 796 221 30115 1778 3331 422 1311 4980 120 422 259 7293 111 2540 131 16508 3115 1767 147 1883 2008 539 12288 5404 235 106 1763 28674 615 8974 3115 2965 137 1681 238 1767 165 501 147 2113 8501 1837 5207 205 106 502 579 1188 4534 241 2728 422 705 422 121 111 17719 582 12288 267 17985 137 971 422 1542 121 111 2394 582 10511 267 6814 205 17719 2771 8974 5152 936 1910 131 3115 205 407 220 2016 2506 30112 422 2679 1900 212 422 10248 6883 30109 422 2187 30112 422 5656 25178 579 5656 25178 422 11592 2904 30120 422 137 5656 2506 154 422 334 220 9923 214 9389 422 8690 422 12808 844 422 137 9723 205 111 705 1379 220 7675 10511 422 137 111 17719 2771 871 111 12760 2172 13022 147 535 3115 205 111 10511 131 796 221 30115 1778 3331 267 797 147 360 629 12508 190 1568 1114 146 145 168 1488 2733 422 146 275 1107 546 131 244 205 1552 159 5036 30130 205 1568 1114 115 137 4070 318 118 241 170 205 514 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 12038 16508 3115 1767 121 796 221 30115 1778 3331 422 1311 4980 120 862 502 1320 188 106 20244 7241 30113 23069 103 833 1536 6163 121 111 7945 30113 2565 1618 235 111 1224 131 6739 422 12763 145 6359 30118 546 137 781 1536 2289 188 106 10886 9879 20244 7241 30113 23069 121 111 17719 131 796 221 30115 1778 3331 422 1311 4980 120 422 259 7293 111 2540 131 16508 3115 1767 147 1883 2008 539 12288 5404 235 106 1763 28674 615 8974 3115 2965 137 1681 238 1767 165 501 147 2113 8501 1837 5207 205 106 502 579 1188 4534 241 2728 422 705 422 121 111 17719 582 12288 267 17985 137 971 422 1542 121 111 2394 582 10511 267 6814 205 17719 2771 8974 5152 936 1910 131 3115 205 407 220 2016 2506 30112 422 2679 1900 212 422 10248 6883 30109 422 2187 30112 422 5656 25178 579 5656 25178 422 11592 2904 30120 422 137 5656 2506 154 422 334 220 9923 214 9389 422 8690 422 12808 844 422 137 9723 205 111 705 1379 220 7675 10511 422 137 111 17719 2771 871 111 12760 2172 13022 147 535 3115 205 111 10511 131 796 221 30115 1778 3331 267 797 147 360 629 12508 190 1568 1114 146 145 168 1488 2733 422 146 275 1107 546 131 244 205 1552 159 5036 30130 205 1568 1114 115 137 4070 318 118 241 170 205 514 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] exploring indigenous soil knowledge in go ##ure ##l yo ##ba , sen ##eg ##al : two years as a peace corp ##s volunteer [SEP] suited to each soil . the soils of go ##ure ##l yo ##ba were found to have low fertility with average total n ( for samples tested , n = 16 ) of 0 . 25 g kg ##1 . average total c and extract ##able p was 2 . 9 g kg ##1 and 2 . 6 kg ha ##1 , respectively . farmers in go ##ure ##l yo ##ba have good working knowledge of their soil resources , but factors such as risk management behaviours and climate change prevent them from always using this knowledge to make the best crop management decisions . simple steps such as concentr ##ating manure along crop rows are recommended to build on what they already understand about soils to improve crop performance [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] exploring indigenous soil knowledge in go ##ure ##l yo ##ba , sen ##eg ##al : two years as a peace corp ##s volunteer [SEP] suited to each soil . the soils of go ##ure ##l yo ##ba were found to have low fertility with average total n ( for samples tested , n = 16 ) of 0 . 25 g kg ##1 . average total c and extract ##able p was 2 . 9 g kg ##1 and 2 . 6 kg ha ##1 , respectively . farmers in go ##ure ##l yo ##ba have good working knowledge of their soil resources , but factors such as risk management behaviours and climate change prevent them from always using this knowledge to make the best crop management decisions . simple steps such as concentr ##ating manure along crop rows are recommended to build on what they already understand about soils to improve crop performance [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 12038 16508 3115 1767 121 796 221 30115 1778 3331 422 1311 4980 120 862 502 1320 188 106 20244 7241 30113 23069 103 13022 147 535 3115 205 111 10511 131 796 221 30115 1778 3331 267 797 147 360 629 12508 190 1568 1114 146 145 168 1488 2733 422 146 275 1107 546 131 244 205 1552 159 5036 30130 205 1568 1114 115 137 4070 318 118 241 170 205 514 159 5036 30130 137 170 205 370 5036 325 30130 422 1222 205 12288 121 796 221 30115 1778 3331 360 1846 3630 1767 131 547 3115 2965 422 563 1391 555 188 1265 1837 13765 137 6002 1477 3363 1445 263 3259 487 238 1767 147 2113 111 2172 8501 1837 5207 205 2177 3565 555 188 1112 560 26096 2252 8501 10392 220 5805 147 2875 191 1792 698 3440 2008 1011 10511 147 1658 8501 1150 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 12038 16508 3115 1767 121 796 221 30115 1778 3331 422 1311 4980 120 862 502 1320 188 106 20244 7241 30113 23069 103 13022 147 535 3115 205 111 10511 131 796 221 30115 1778 3331 267 797 147 360 629 12508 190 1568 1114 146 145 168 1488 2733 422 146 275 1107 546 131 244 205 1552 159 5036 30130 205 1568 1114 115 137 4070 318 118 241 170 205 514 159 5036 30130 137 170 205 370 5036 325 30130 422 1222 205 12288 121 796 221 30115 1778 3331 360 1846 3630 1767 131 547 3115 2965 422 563 1391 555 188 1265 1837 13765 137 6002 1477 3363 1445 263 3259 487 238 1767 147 2113 111 2172 8501 1837 5207 205 2177 3565 555 188 1112 560 26096 2252 8501 10392 220 5805 147 2875 191 1792 698 3440 2008 1011 10511 147 1658 8501 1150 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] farm households ' perception on climate change and adaptation practices a case from mountain district of ne ##pal [SEP] purpose - this paper aims to assess the farming community ' s perception on important parameters of climate change and identify major practices and technologies adopted to mitigate the impacts of climate change and their determinants in mountain district of ne ##pal . being an agr ##arian economy and dependent on mon ##soon rain , the impact of climate change on agricultural productivity and production has been experienced . different adaptation strategies have been adopted by the communities to cope with the consequences of climate change . design / methodology / approach - four village development committees ( vd ##cs ) situated in the buffer zone of lang ##tan ##g national park of ras ##uw ##a district representing rice , wheat , maize and potato production area was pur ##pos ##ive ##ly selected for this study . a two - stage sampling technique was adopted for data and information collection . thirty households from each vd ##c were randomly selected . data on the socio - economic and climate change perception were collected using structured questionnaire . a binary logistic regression technique was used to identify the determinants of climate change adaptation technologies and practices . findings - the farmers ' decisions whether to adopt climate change adaptation technologies are governed by the size of land ##holding , perceived threat of climate change to food security , education level and gender of the interview ##ee [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] farm households ' perception on climate change and adaptation practices a case from mountain district of ne ##pal [SEP] purpose - this paper aims to assess the farming community ' s perception on important parameters of climate change and identify major practices and technologies adopted to mitigate the impacts of climate change and their determinants in mountain district of ne ##pal . being an agr ##arian economy and dependent on mon ##soon rain , the impact of climate change on agricultural productivity and production has been experienced . different adaptation strategies have been adopted by the communities to cope with the consequences of climate change . design / methodology / approach - four village development committees ( vd ##cs ) situated in the buffer zone of lang ##tan ##g national park of ras ##uw ##a district representing rice , wheat , maize and potato production area was pur ##pos ##ive ##ly selected for this study . a two - stage sampling technique was adopted for data and information collection . thirty households from each vd ##c were randomly selected . data on the socio - economic and climate change perception were collected using structured questionnaire . a binary logistic regression technique was used to identify the determinants of climate change adaptation technologies and practices . findings - the farmers ' decisions whether to adopt climate change adaptation technologies are governed by the size of land ##holding , perceived threat of climate change to food security , education level and gender of the interview ##ee [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 5947 9654 2505 6028 191 6002 1477 137 5846 5423 106 820 263 12273 10689 131 287 11014 103 3559 579 238 1203 7415 147 1285 111 19323 2928 2505 112 6028 191 1060 1496 131 6002 1477 137 2743 1626 5423 137 4198 6018 147 16735 111 7681 131 6002 1477 137 547 10687 121 12273 10689 131 287 11014 205 1558 130 4545 9371 7425 137 3644 191 775 27484 7112 422 111 2141 131 6002 1477 191 7675 7381 137 1865 434 528 5770 205 643 5846 3236 360 528 6018 214 111 5904 147 14649 190 111 6091 131 6002 1477 205 899 1352 4964 1352 1139 579 1379 17719 1120 23073 145 15009 1647 546 15657 121 111 3520 5493 131 1973 16161 30123 2227 7713 131 9461 17553 30110 10689 5674 8058 422 10094 422 13156 137 16480 1865 1590 241 1434 451 1090 179 2350 168 238 527 205 106 502 579 2410 3597 2358 241 6018 168 453 137 776 3445 205 12498 9654 263 535 15009 30116 267 5007 2350 205 453 191 111 8001 579 3587 137 6002 1477 6028 267 2760 487 7908 5692 205 106 5067 7312 3089 2358 241 501 147 2743 111 10687 131 6002 1477 5846 4198 137 5423 205 2116 579 111 12288 2505 5207 1681 147 9265 6002 1477 5846 4198 220 17384 214 111 1243 131 2882 28696 422 5545 8057 131 6002 1477 147 2599 3594 422 2870 615 137 4703 131 111 4489 1900 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 5947 9654 2505 6028 191 6002 1477 137 5846 5423 106 820 263 12273 10689 131 287 11014 103 3559 579 238 1203 7415 147 1285 111 19323 2928 2505 112 6028 191 1060 1496 131 6002 1477 137 2743 1626 5423 137 4198 6018 147 16735 111 7681 131 6002 1477 137 547 10687 121 12273 10689 131 287 11014 205 1558 130 4545 9371 7425 137 3644 191 775 27484 7112 422 111 2141 131 6002 1477 191 7675 7381 137 1865 434 528 5770 205 643 5846 3236 360 528 6018 214 111 5904 147 14649 190 111 6091 131 6002 1477 205 899 1352 4964 1352 1139 579 1379 17719 1120 23073 145 15009 1647 546 15657 121 111 3520 5493 131 1973 16161 30123 2227 7713 131 9461 17553 30110 10689 5674 8058 422 10094 422 13156 137 16480 1865 1590 241 1434 451 1090 179 2350 168 238 527 205 106 502 579 2410 3597 2358 241 6018 168 453 137 776 3445 205 12498 9654 263 535 15009 30116 267 5007 2350 205 453 191 111 8001 579 3587 137 6002 1477 6028 267 2760 487 7908 5692 205 106 5067 7312 3089 2358 241 501 147 2743 111 10687 131 6002 1477 5846 4198 137 5423 205 2116 579 111 12288 2505 5207 1681 147 9265 6002 1477 5846 4198 220 17384 214 111 1243 131 2882 28696 422 5545 8057 131 6002 1477 147 2599 3594 422 2870 615 137 4703 131 111 4489 1900 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used for model gpu 0\n",
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from study_type_multi\\model.ckpt-2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from study_type_multi\\model.ckpt-2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] impact of promoting sustainable agriculture in born ##o ( pros ##ab ) program on adoption of improved crop varieties in born ##o state of nigeria [SEP] promoting sustainable agriculture in born ##o ( pros ##ab ) project on the live ##li ##hood ##s of farming households in born ##o state , nigeria . specifically , the paper identifies and provides information on farmers ' adoption of improved crop varieties introduced by pros ##ab ; measures their adoption rates , and analyzes the factors that affect the probability and intensity of adoption of the crop varieties . a multi - stage sampling procedure was used to select respondents in three agro ##ec ##ological zones ( ae ##zs ) - the northern guinea sav ##anna ( ng ##s ) , southern guinea sav ##anna ( sg ##s ) , and the sah ##elian sav ##anna ( ss ) , and a structured questionnaire was used to collect data on their socioeconomic characteristics and adoption parameters in the 2009 period . data were analyzed using descriptive statistics and the to ##bit regression techniques . the paper demonstrates how adoption and use decisions were significantly influenced by education , membership in associations , and yield in the ng ##s ; yield and labor in the sg ##s , and age and labor in the ss , implying that these variables , and particularly yield , are important for consideration by development agencies who are interested in improving agricultural production and the live ##li ##hood ##s of the people [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] impact of promoting sustainable agriculture in born ##o ( pros ##ab ) program on adoption of improved crop varieties in born ##o state of nigeria [SEP] promoting sustainable agriculture in born ##o ( pros ##ab ) project on the live ##li ##hood ##s of farming households in born ##o state , nigeria . specifically , the paper identifies and provides information on farmers ' adoption of improved crop varieties introduced by pros ##ab ; measures their adoption rates , and analyzes the factors that affect the probability and intensity of adoption of the crop varieties . a multi - stage sampling procedure was used to select respondents in three agro ##ec ##ological zones ( ae ##zs ) - the northern guinea sav ##anna ( ng ##s ) , southern guinea sav ##anna ( sg ##s ) , and the sah ##elian sav ##anna ( ss ) , and a structured questionnaire was used to collect data on their socioeconomic characteristics and adoption parameters in the 2009 period . data were analyzed using descriptive statistics and the to ##bit regression techniques . the paper demonstrates how adoption and use decisions were significantly influenced by education , membership in associations , and yield in the ng ##s ; yield and labor in the sg ##s , and age and labor in the ss , implying that these variables , and particularly yield , are important for consideration by development agencies who are interested in improving agricultural production and the live ##li ##hood ##s of the people [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 2141 131 10302 10886 9879 121 9856 30112 145 12000 201 546 1618 191 9714 131 3007 8501 13600 121 9856 30112 1098 131 17964 103 10302 10886 9879 121 9856 30112 145 12000 201 546 2575 191 111 6489 3206 4402 30113 131 19323 9654 121 9856 30112 1098 422 17964 205 3697 422 111 1203 12369 137 2315 776 191 12288 2505 9714 131 3007 8501 13600 3376 214 12000 201 1814 2554 547 9714 1975 422 137 21127 111 1391 198 2606 111 2064 137 3165 131 9714 131 111 8501 13600 205 106 869 579 2410 3597 2272 241 501 147 5470 6634 121 874 22362 155 872 10166 145 8674 22882 546 579 111 7926 16616 7110 25215 145 3312 30113 546 422 7663 16616 7110 25215 145 10410 30113 546 422 137 111 16905 14005 7110 25215 145 3712 546 422 137 106 7908 5692 241 501 147 9921 453 191 547 13044 2087 137 9714 1496 121 111 4758 1275 205 453 267 2549 487 10363 4530 137 111 147 2386 3089 2190 205 111 1203 7499 539 9714 137 626 5207 267 1357 6452 214 2870 422 9610 121 6225 422 137 2210 121 111 3312 30113 1814 2210 137 2193 121 111 10410 30113 422 137 1407 137 2193 121 111 3712 422 14466 198 407 1762 422 137 3220 2210 422 220 1060 168 5976 214 1120 11934 975 220 7301 121 6017 7675 1865 137 111 6489 3206 4402 30113 131 111 2325 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 2141 131 10302 10886 9879 121 9856 30112 145 12000 201 546 1618 191 9714 131 3007 8501 13600 121 9856 30112 1098 131 17964 103 10302 10886 9879 121 9856 30112 145 12000 201 546 2575 191 111 6489 3206 4402 30113 131 19323 9654 121 9856 30112 1098 422 17964 205 3697 422 111 1203 12369 137 2315 776 191 12288 2505 9714 131 3007 8501 13600 3376 214 12000 201 1814 2554 547 9714 1975 422 137 21127 111 1391 198 2606 111 2064 137 3165 131 9714 131 111 8501 13600 205 106 869 579 2410 3597 2272 241 501 147 5470 6634 121 874 22362 155 872 10166 145 8674 22882 546 579 111 7926 16616 7110 25215 145 3312 30113 546 422 7663 16616 7110 25215 145 10410 30113 546 422 137 111 16905 14005 7110 25215 145 3712 546 422 137 106 7908 5692 241 501 147 9921 453 191 547 13044 2087 137 9714 1496 121 111 4758 1275 205 453 267 2549 487 10363 4530 137 111 147 2386 3089 2190 205 111 1203 7499 539 9714 137 626 5207 267 1357 6452 214 2870 422 9610 121 6225 422 137 2210 121 111 3312 30113 1814 2210 137 2193 121 111 10410 30113 422 137 1407 137 2193 121 111 3712 422 14466 198 407 1762 422 137 3220 2210 422 220 1060 168 5976 214 1120 11934 975 220 7301 121 6017 7675 1865 137 111 6489 3206 4402 30113 131 111 2325 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] exploring indigenous soil knowledge in go ##ure ##l yo ##ba , sen ##eg ##al : two years as a peace corp ##s volunteer [SEP] . a two - part investigation was conducted , first , in the village where farmers were interviewed and second , back in the usa where soils were analysed . village ##rs recognize seven main types of soil . these are fa ##hr ##o , bal ##ee ##ri , ken ##yer ##i , seen ##o , ka ##hran - ka ##hran , wen ##do ##u , and ka ##hr ##el , which are differentiated by colour , texture , drainage class , and vegetation . the first four are agricultural soils , and the village ##rs know the crops best suited to each soil . the soils of go ##ure ##l yo ##ba were found to have low fertility with average total n ( for samples tested , n = 16 ) of 0 . 25 g kg ##1 . average total c and extract ##able p was 2 . 9 g kg ##1 and 2 . 6 kg ha ##1 , respectively . farmers in go ##ure ##l yo ##ba have good working knowledge of their soil resources , but factors such as risk management behaviours and climate change prevent them from always using this knowledge to make the best crop management decisions . simple steps such as concentr ##ating manure along crop rows are recommended to build on what they already understand about soils to improve crop performance [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] exploring indigenous soil knowledge in go ##ure ##l yo ##ba , sen ##eg ##al : two years as a peace corp ##s volunteer [SEP] . a two - part investigation was conducted , first , in the village where farmers were interviewed and second , back in the usa where soils were analysed . village ##rs recognize seven main types of soil . these are fa ##hr ##o , bal ##ee ##ri , ken ##yer ##i , seen ##o , ka ##hran - ka ##hran , wen ##do ##u , and ka ##hr ##el , which are differentiated by colour , texture , drainage class , and vegetation . the first four are agricultural soils , and the village ##rs know the crops best suited to each soil . the soils of go ##ure ##l yo ##ba were found to have low fertility with average total n ( for samples tested , n = 16 ) of 0 . 25 g kg ##1 . average total c and extract ##able p was 2 . 9 g kg ##1 and 2 . 6 kg ha ##1 , respectively . farmers in go ##ure ##l yo ##ba have good working knowledge of their soil resources , but factors such as risk management behaviours and climate change prevent them from always using this knowledge to make the best crop management decisions . simple steps such as concentr ##ating manure along crop rows are recommended to build on what they already understand about soils to improve crop performance [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 12038 16508 3115 1767 121 796 221 30115 1778 3331 422 1311 4980 120 862 502 1320 188 106 20244 7241 30113 23069 103 205 106 502 579 1188 4534 241 2728 422 705 422 121 111 17719 582 12288 267 17985 137 971 422 1542 121 111 2394 582 10511 267 6814 205 17719 2771 8974 5152 936 1910 131 3115 205 407 220 2016 2506 30112 422 2679 1900 212 422 10248 6883 30109 422 2187 30112 422 5656 25178 579 5656 25178 422 11592 2904 30120 422 137 5656 2506 154 422 334 220 9923 214 9389 422 8690 422 12808 844 422 137 9723 205 111 705 1379 220 7675 10511 422 137 111 17719 2771 871 111 12760 2172 13022 147 535 3115 205 111 10511 131 796 221 30115 1778 3331 267 797 147 360 629 12508 190 1568 1114 146 145 168 1488 2733 422 146 275 1107 546 131 244 205 1552 159 5036 30130 205 1568 1114 115 137 4070 318 118 241 170 205 514 159 5036 30130 137 170 205 370 5036 325 30130 422 1222 205 12288 121 796 221 30115 1778 3331 360 1846 3630 1767 131 547 3115 2965 422 563 1391 555 188 1265 1837 13765 137 6002 1477 3363 1445 263 3259 487 238 1767 147 2113 111 2172 8501 1837 5207 205 2177 3565 555 188 1112 560 26096 2252 8501 10392 220 5805 147 2875 191 1792 698 3440 2008 1011 10511 147 1658 8501 1150 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 12038 16508 3115 1767 121 796 221 30115 1778 3331 422 1311 4980 120 862 502 1320 188 106 20244 7241 30113 23069 103 205 106 502 579 1188 4534 241 2728 422 705 422 121 111 17719 582 12288 267 17985 137 971 422 1542 121 111 2394 582 10511 267 6814 205 17719 2771 8974 5152 936 1910 131 3115 205 407 220 2016 2506 30112 422 2679 1900 212 422 10248 6883 30109 422 2187 30112 422 5656 25178 579 5656 25178 422 11592 2904 30120 422 137 5656 2506 154 422 334 220 9923 214 9389 422 8690 422 12808 844 422 137 9723 205 111 705 1379 220 7675 10511 422 137 111 17719 2771 871 111 12760 2172 13022 147 535 3115 205 111 10511 131 796 221 30115 1778 3331 267 797 147 360 629 12508 190 1568 1114 146 145 168 1488 2733 422 146 275 1107 546 131 244 205 1552 159 5036 30130 205 1568 1114 115 137 4070 318 118 241 170 205 514 159 5036 30130 137 170 205 370 5036 325 30130 422 1222 205 12288 121 796 221 30115 1778 3331 360 1846 3630 1767 131 547 3115 2965 422 563 1391 555 188 1265 1837 13765 137 6002 1477 3363 1445 263 3259 487 238 1767 147 2113 111 2172 8501 1837 5207 205 2177 3565 555 188 1112 560 26096 2252 8501 10392 220 5805 147 2875 191 1792 698 3440 2008 1011 10511 147 1658 8501 1150 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] farm households ' perception on climate change and adaptation practices a case from mountain district of ne ##pal [SEP] ##tan ##g national park of ras ##uw ##a district representing rice , wheat , maize and potato production area was pur ##pos ##ive ##ly selected for this study . a two - stage sampling technique was adopted for data and information collection . thirty households from each vd ##c were randomly selected . data on the socio - economic and climate change perception were collected using structured questionnaire . a binary logistic regression technique was used to identify the determinants of climate change adaptation technologies and practices . findings - the farmers ' decisions whether to adopt climate change adaptation technologies are governed by the size of land ##holding , perceived threat of climate change to food security , education level and gender of the interview ##ee , perception on the increased incidence of drought ##s during rain ##y season and income received from the off - farm sources . in a community where agricultural activity is the dominant means of living , adaptive strategies help to increase the capacity of a farming system to survive external shocks and cope with the consequences . original ##ity / value - the assessment of farm - level adaptation strategies and factors influencing their adoption decision is important to formulate policies and design programs . this will also help to recognize adaptation as a tool for managing a variety of risks associated with climate change in agricultural sector [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] farm households ' perception on climate change and adaptation practices a case from mountain district of ne ##pal [SEP] ##tan ##g national park of ras ##uw ##a district representing rice , wheat , maize and potato production area was pur ##pos ##ive ##ly selected for this study . a two - stage sampling technique was adopted for data and information collection . thirty households from each vd ##c were randomly selected . data on the socio - economic and climate change perception were collected using structured questionnaire . a binary logistic regression technique was used to identify the determinants of climate change adaptation technologies and practices . findings - the farmers ' decisions whether to adopt climate change adaptation technologies are governed by the size of land ##holding , perceived threat of climate change to food security , education level and gender of the interview ##ee , perception on the increased incidence of drought ##s during rain ##y season and income received from the off - farm sources . in a community where agricultural activity is the dominant means of living , adaptive strategies help to increase the capacity of a farming system to survive external shocks and cope with the consequences . original ##ity / value - the assessment of farm - level adaptation strategies and factors influencing their adoption decision is important to formulate policies and design programs . this will also help to recognize adaptation as a tool for managing a variety of risks associated with climate change in agricultural sector [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 5947 9654 2505 6028 191 6002 1477 137 5846 5423 106 820 263 12273 10689 131 287 11014 103 16161 30123 2227 7713 131 9461 17553 30110 10689 5674 8058 422 10094 422 13156 137 16480 1865 1590 241 1434 451 1090 179 2350 168 238 527 205 106 502 579 2410 3597 2358 241 6018 168 453 137 776 3445 205 12498 9654 263 535 15009 30116 267 5007 2350 205 453 191 111 8001 579 3587 137 6002 1477 6028 267 2760 487 7908 5692 205 106 5067 7312 3089 2358 241 501 147 2743 111 10687 131 6002 1477 5846 4198 137 5423 205 2116 579 111 12288 2505 5207 1681 147 9265 6002 1477 5846 4198 220 17384 214 111 1243 131 2882 28696 422 5545 8057 131 6002 1477 147 2599 3594 422 2870 615 137 4703 131 111 4489 1900 422 6028 191 111 1175 3970 131 13509 30113 781 7112 30126 7843 137 5021 2072 263 111 1874 579 5947 3117 205 121 106 2928 582 7675 1071 165 111 5521 2033 131 5404 422 4640 3236 2279 147 1242 111 2900 131 106 19323 429 147 14413 3458 13245 137 14649 190 111 6091 205 2592 208 1352 973 579 111 2790 131 5947 579 615 5846 3236 137 1391 12657 547 9714 2015 165 1060 147 15835 5691 137 899 3996 205 238 650 469 2279 147 8974 5846 188 106 3130 168 11170 106 3835 131 6733 1111 190 6002 1477 121 7675 5672 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 5947 9654 2505 6028 191 6002 1477 137 5846 5423 106 820 263 12273 10689 131 287 11014 103 16161 30123 2227 7713 131 9461 17553 30110 10689 5674 8058 422 10094 422 13156 137 16480 1865 1590 241 1434 451 1090 179 2350 168 238 527 205 106 502 579 2410 3597 2358 241 6018 168 453 137 776 3445 205 12498 9654 263 535 15009 30116 267 5007 2350 205 453 191 111 8001 579 3587 137 6002 1477 6028 267 2760 487 7908 5692 205 106 5067 7312 3089 2358 241 501 147 2743 111 10687 131 6002 1477 5846 4198 137 5423 205 2116 579 111 12288 2505 5207 1681 147 9265 6002 1477 5846 4198 220 17384 214 111 1243 131 2882 28696 422 5545 8057 131 6002 1477 147 2599 3594 422 2870 615 137 4703 131 111 4489 1900 422 6028 191 111 1175 3970 131 13509 30113 781 7112 30126 7843 137 5021 2072 263 111 1874 579 5947 3117 205 121 106 2928 582 7675 1071 165 111 5521 2033 131 5404 422 4640 3236 2279 147 1242 111 2900 131 106 19323 429 147 14413 3458 13245 137 14649 190 111 6091 205 2592 208 1352 973 579 111 2790 131 5947 579 615 5846 3236 137 1391 12657 547 9714 2015 165 1060 147 15835 5691 137 899 3996 205 238 650 469 2279 147 8974 5846 188 106 3130 168 11170 106 3835 131 6733 1111 190 6002 1477 121 7675 5672 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] association of small ##holder dairy farmers management and milk ##ing practices with bacterial quality of milk in mb ##ey ##a , tanzania [SEP] ##el for drying ud ##der and tea ##t . none of the respondents used pre milk ##ing , post milk ##ing dip ##ping or dry cow therapy . milk from cows kept in bar ##ns made from concrete floor had ( p < 0 . 00 ##1 ) lower tb ##c and tsc . frequency of cleaning of dairy bar ##n ( p < 0 . 05 ) influenced the tb ##c , tcc and tsc . lack of fore milk ##ing ( p < 0 . 05 ) associated with higher tb ##c . furthermore , water source ( p < 0 . 05 ) influenced tb ##c and tcc . milk ##ing practices which includes washing of hands , ud ##der and tea ##t , dry of tea ##ts using individual tow ##el per cow and followed by fore milk ##ing yielded ( p < 0 . 05 ) lower bacterial count than other practices . similarly , cow bar ##ns whose floor ##s were made from concrete and cleaned twice or more daily had clean cow which produce milk with lower bacterial counts than cows kept on floor ##s made from timber and soil . the results reveal the need for training farmers in good agricultural practices . this would contribute to achieving better quality milk , and ensure the sustainability of the sector in the study area [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] association of small ##holder dairy farmers management and milk ##ing practices with bacterial quality of milk in mb ##ey ##a , tanzania [SEP] ##el for drying ud ##der and tea ##t . none of the respondents used pre milk ##ing , post milk ##ing dip ##ping or dry cow therapy . milk from cows kept in bar ##ns made from concrete floor had ( p < 0 . 00 ##1 ) lower tb ##c and tsc . frequency of cleaning of dairy bar ##n ( p < 0 . 05 ) influenced the tb ##c , tcc and tsc . lack of fore milk ##ing ( p < 0 . 05 ) associated with higher tb ##c . furthermore , water source ( p < 0 . 05 ) influenced tb ##c and tcc . milk ##ing practices which includes washing of hands , ud ##der and tea ##t , dry of tea ##ts using individual tow ##el per cow and followed by fore milk ##ing yielded ( p < 0 . 05 ) lower bacterial count than other practices . similarly , cow bar ##ns whose floor ##s were made from concrete and cleaned twice or more daily had clean cow which produce milk with lower bacterial counts than cows kept on floor ##s made from timber and soil . the results reveal the need for training farmers in good agricultural practices . this would contribute to achieving better quality milk , and ensure the sustainability of the sector in the study area [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 2274 131 952 13643 14874 12288 1837 137 7435 140 5423 190 4667 1671 131 7435 121 6369 9257 30110 422 27281 103 154 168 15211 8722 343 137 15297 30108 205 4580 131 111 6634 501 382 7435 140 422 1422 7435 140 6332 5354 234 5965 8506 2223 205 7435 263 15627 6890 121 2261 1290 1827 263 9209 10298 883 145 118 962 244 205 3641 30130 546 1268 6226 30116 137 26800 205 1610 131 17376 131 14874 2261 30111 145 118 962 244 205 10764 546 6452 111 6226 30116 422 28331 137 26800 205 2596 131 2312 7435 140 145 118 962 244 205 10764 546 1111 190 1001 6226 30116 205 2403 422 1506 1908 145 118 962 244 205 10764 546 6452 6226 30116 137 28331 205 7435 140 5423 334 3753 6907 131 12053 422 8722 343 137 15297 30108 422 5965 131 15297 203 487 1161 10755 154 309 8506 137 2594 214 2312 7435 140 9378 145 118 962 244 205 10764 546 1268 4667 2232 506 494 5423 205 3453 422 8506 2261 1290 3489 10298 30113 267 1827 263 9209 137 23445 6279 234 475 4122 883 7113 8506 334 3299 7435 190 1268 4667 7274 506 15627 6890 191 10298 30113 1827 263 28094 137 3115 205 111 545 2303 111 965 168 2208 12288 121 1846 7675 5423 205 238 989 4362 147 9153 1883 1671 7435 422 137 4207 111 13591 131 111 5672 121 111 527 1590 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 2274 131 952 13643 14874 12288 1837 137 7435 140 5423 190 4667 1671 131 7435 121 6369 9257 30110 422 27281 103 154 168 15211 8722 343 137 15297 30108 205 4580 131 111 6634 501 382 7435 140 422 1422 7435 140 6332 5354 234 5965 8506 2223 205 7435 263 15627 6890 121 2261 1290 1827 263 9209 10298 883 145 118 962 244 205 3641 30130 546 1268 6226 30116 137 26800 205 1610 131 17376 131 14874 2261 30111 145 118 962 244 205 10764 546 6452 111 6226 30116 422 28331 137 26800 205 2596 131 2312 7435 140 145 118 962 244 205 10764 546 1111 190 1001 6226 30116 205 2403 422 1506 1908 145 118 962 244 205 10764 546 6452 6226 30116 137 28331 205 7435 140 5423 334 3753 6907 131 12053 422 8722 343 137 15297 30108 422 5965 131 15297 203 487 1161 10755 154 309 8506 137 2594 214 2312 7435 140 9378 145 118 962 244 205 10764 546 1268 4667 2232 506 494 5423 205 3453 422 8506 2261 1290 3489 10298 30113 267 1827 263 9209 137 23445 6279 234 475 4122 883 7113 8506 334 3299 7435 190 1268 4667 7274 506 15627 6890 191 10298 30113 1827 263 28094 137 3115 205 111 545 2303 111 965 168 2208 12288 121 1846 7675 5423 205 238 989 4362 147 9153 1883 1671 7435 422 137 4207 111 13591 131 111 5672 121 111 527 1590 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] assessing maize farmers ' adaptation strategies to climate change and variability in ghana [SEP] this study examined the adaptation strategies of maize farmers to climate change and variability in the eastern region of ghana using primary data collected from 150 maize farming households by the administration of structured questionnaires . the results of the multin ##omial logit regression revealed that rainfall perception , access to credit , and farming experience significantly influenced the adoption of recommended agricultural practices , whereas the adoption of soil - related strategies is influenced by gender and rainfall perception . farming experience and rainfall perception influenced the adoption of improved varieties strategies . this study highlights the need for the development of water resources for maize production in the context of the changing climate . in this respect , the crucial roles of the ghana irrigation development authority , the agricultural extension division of the ministry of food and agriculture , and other international organizations such as the japan international cooperation agency and the world bank regarding the development of irrigation facilities and the associated capacity building of the farmers are very important . finally , the formation of water user associations for the small ##holder farmers regarding the usage and maintenance of the irrigation facilities would be a step in the right direction [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] assessing maize farmers ' adaptation strategies to climate change and variability in ghana [SEP] this study examined the adaptation strategies of maize farmers to climate change and variability in the eastern region of ghana using primary data collected from 150 maize farming households by the administration of structured questionnaires . the results of the multin ##omial logit regression revealed that rainfall perception , access to credit , and farming experience significantly influenced the adoption of recommended agricultural practices , whereas the adoption of soil - related strategies is influenced by gender and rainfall perception . farming experience and rainfall perception influenced the adoption of improved varieties strategies . this study highlights the need for the development of water resources for maize production in the context of the changing climate . in this respect , the crucial roles of the ghana irrigation development authority , the agricultural extension division of the ministry of food and agriculture , and other international organizations such as the japan international cooperation agency and the world bank regarding the development of irrigation facilities and the associated capacity building of the farmers are very important . finally , the formation of water user associations for the small ##holder farmers regarding the usage and maintenance of the irrigation facilities would be a step in the right direction [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 7836 13156 12288 2505 5846 3236 147 6002 1477 137 4638 121 26274 103 238 527 2959 111 5846 3236 131 13156 12288 147 6002 1477 137 4638 121 111 10309 1025 131 26274 487 1916 453 2760 263 5413 13156 19323 9654 214 111 3762 131 7908 11682 205 111 545 131 111 18231 4025 25565 3089 2861 198 12660 6028 422 1899 147 8388 422 137 19323 2899 1357 6452 111 9714 131 5805 7675 5423 422 2255 111 9714 131 3115 579 1482 3236 165 6452 214 4703 137 12660 6028 205 19323 2899 137 12660 6028 6452 111 9714 131 3007 13600 3236 205 238 527 11878 111 965 168 111 1120 131 1506 2965 168 13156 1865 121 111 2220 131 111 5468 6002 205 121 238 2646 422 111 5784 5370 131 111 26274 15055 1120 11226 422 111 7675 3840 5277 131 111 9795 131 2599 137 9879 422 137 494 2565 6938 555 188 111 4838 2565 9226 8118 137 111 2399 5108 3560 111 1120 131 15055 7845 137 111 1111 2900 4736 131 111 12288 220 1248 1060 205 2285 422 111 2256 131 1506 1824 6225 168 111 952 13643 12288 3560 111 6323 137 5365 131 111 15055 7845 989 195 106 1371 121 111 2083 3041 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 7836 13156 12288 2505 5846 3236 147 6002 1477 137 4638 121 26274 103 238 527 2959 111 5846 3236 131 13156 12288 147 6002 1477 137 4638 121 111 10309 1025 131 26274 487 1916 453 2760 263 5413 13156 19323 9654 214 111 3762 131 7908 11682 205 111 545 131 111 18231 4025 25565 3089 2861 198 12660 6028 422 1899 147 8388 422 137 19323 2899 1357 6452 111 9714 131 5805 7675 5423 422 2255 111 9714 131 3115 579 1482 3236 165 6452 214 4703 137 12660 6028 205 19323 2899 137 12660 6028 6452 111 9714 131 3007 13600 3236 205 238 527 11878 111 965 168 111 1120 131 1506 2965 168 13156 1865 121 111 2220 131 111 5468 6002 205 121 238 2646 422 111 5784 5370 131 111 26274 15055 1120 11226 422 111 7675 3840 5277 131 111 9795 131 2599 137 9879 422 137 494 2565 6938 555 188 111 4838 2565 9226 8118 137 111 2399 5108 3560 111 1120 131 15055 7845 137 111 1111 2900 4736 131 111 12288 220 1248 1060 205 2285 422 111 2256 131 1506 1824 6225 168 111 952 13643 12288 3560 111 6323 137 5365 131 111 15055 7845 989 195 106 1371 121 111 2083 3041 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used for model gpu 0\n",
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from study_type_multi\\model.ckpt-2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from study_type_multi\\model.ckpt-2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20252 samples, validate on 888 samples\n",
      "Epoch 1/4\n",
      "20252/20252 [==============================] - 4s 214us/sample - loss: 0.0734 - val_loss: 0.0499\n",
      "Epoch 2/4\n",
      "20252/20252 [==============================] - 4s 201us/sample - loss: 0.0605 - val_loss: 0.0544\n",
      "Epoch 3/4\n",
      "20252/20252 [==============================] - 4s 195us/sample - loss: 0.0545 - val_loss: 0.0418\n",
      "Epoch 4/4\n",
      "20252/20252 [==============================] - 4s 184us/sample - loss: 0.0505 - val_loss: 0.0432\n"
     ]
    }
   ],
   "source": [
    "from bert_models import base_bert_model\n",
    "base_bert_model = reload(base_bert_model)\n",
    "base_bert_model_meta_model_with_window_all = reload(base_bert_model_meta_model_with_window_all)\n",
    "study_type_labeler_window_agg = reload(study_type_labeler_window_agg)\n",
    "_study_type_labeler = study_type_labeler_window_agg.StudyTypeLabeler(\n",
    "      \"study_type_multi\",\n",
    "      max_seq_length = 256, batch_size = 16,\n",
    "      gpu_device_num_hub = 0, gpu_device_num = 0,\n",
    "      model_folder = \"../tmp/scibert_scivocab_uncased\",\n",
    "      label_list = list(range(len(study_labels))),\n",
    "      meta_model_folder = \"study_type_multi_meta_agg\",\n",
    "      use_one_layer=True,\n",
    "      keep_prob=0.8,\n",
    "      epochs_num=4,\n",
    "      multilabel=True\n",
    "      )\n",
    "#_study_type_labeler.train_model(train, test, use_tails=True)\n",
    "_study_type_labeler.train_meta_model(train, test, use_tail=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read file ../notebooks/Studies for visualizations (1).xlsx: 0.08s\n",
      "Processed file ../notebooks/Studies for visualizations (1).xlsx: 0.06s\n"
     ]
    }
   ],
   "source": [
    "dfs_data = excel_reader.ExcelReader().read_df_from_excel(\"../notebooks/Studies for visualizations (1).xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] running out of credit : the limitations of mobile tele ##phon ##y in a tanzania ##n agricultural marketing system [SEP] poor farmers often lack credit to purchase agricultural inputs , and rely on their buyers to provide it . this paper considers the effects of mobile phones on traders of peri ##sha ##ble foods ##tu ##ff ##s operating between tanzania ' s southern highl ##ands and dar es sal ##aa ##m ' s wholes ##ale market , with a particular focus on the importance of credit in the relationship between potato and tomato farmers and their wholes ##ale buyers . it argues that the ability to communicate using these new information and communication technologies ( ict ##s ) does not significantly alter the trust relationship between the two groups . it also suggests that farmers , in effect , often have to accept the price they are told their crops are sold for - irrespective of the method of communication used to convey this message - because their buyers are also their credit ##ors . in this situation , many farmers are unable to exploit new mobile phone - based services to seek information on market prices , and potential buyers in other markets . doing so runs the risk of breaking a long - term relationship with a buyer who is willing to supply credit because of their established business interaction . it is suggested that , under a more open system than currently exists in tanzania , mobile - payment ( ' [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] running out of credit : the limitations of mobile tele ##phon ##y in a tanzania ##n agricultural marketing system [SEP] poor farmers often lack credit to purchase agricultural inputs , and rely on their buyers to provide it . this paper considers the effects of mobile phones on traders of peri ##sha ##ble foods ##tu ##ff ##s operating between tanzania ' s southern highl ##ands and dar es sal ##aa ##m ' s wholes ##ale market , with a particular focus on the importance of credit in the relationship between potato and tomato farmers and their wholes ##ale buyers . it argues that the ability to communicate using these new information and communication technologies ( ict ##s ) does not significantly alter the trust relationship between the two groups . it also suggests that farmers , in effect , often have to accept the price they are told their crops are sold for - irrespective of the method of communication used to convey this message - because their buyers are also their credit ##ors . in this situation , many farmers are unable to exploit new mobile phone - based services to seek information on market prices , and potential buyers in other markets . doing so runs the risk of breaking a long - term relationship with a buyer who is willing to supply credit because of their established business interaction . it is suggested that , under a more open system than currently exists in tanzania , mobile - payment ( ' [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 5463 556 131 8388 862 111 4761 131 3650 6052 28971 30126 121 106 27281 30111 7675 10644 429 103 3228 12288 1992 2596 8388 147 13543 7675 5671 422 137 8520 191 547 22810 147 1584 256 205 238 1203 10289 111 1056 131 3650 18113 191 29607 131 9447 22269 503 11107 13004 274 30113 4527 467 27281 2505 112 7663 4902 3053 137 7386 601 1896 4871 30119 2505 112 26626 886 2753 422 190 106 1349 1790 191 111 3243 131 8388 121 111 1654 467 16480 137 16657 12288 137 547 26626 886 22810 205 256 15368 198 111 2495 147 11160 487 407 758 776 137 2498 4198 145 13328 30113 546 1452 302 1357 4222 111 5831 1654 467 111 502 1302 205 256 469 3082 198 12288 422 121 907 422 1992 360 147 3705 111 4048 698 220 16364 547 12760 220 17922 168 579 14387 131 111 551 131 2498 501 147 15505 238 4330 579 923 547 22810 220 469 547 8388 386 205 121 238 4001 422 1164 12288 220 8576 147 11091 758 3650 7949 579 791 2522 147 10453 776 191 2753 6759 422 137 1411 22810 121 494 7838 205 8328 564 7317 111 1265 131 13472 106 1113 579 902 1654 190 106 19588 975 165 10279 147 4494 8388 923 131 547 3452 3948 2019 205 256 165 2995 198 422 604 106 475 1937 429 506 3803 3425 121 27281 422 3650 579 11482 145 2505 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 5463 556 131 8388 862 111 4761 131 3650 6052 28971 30126 121 106 27281 30111 7675 10644 429 103 3228 12288 1992 2596 8388 147 13543 7675 5671 422 137 8520 191 547 22810 147 1584 256 205 238 1203 10289 111 1056 131 3650 18113 191 29607 131 9447 22269 503 11107 13004 274 30113 4527 467 27281 2505 112 7663 4902 3053 137 7386 601 1896 4871 30119 2505 112 26626 886 2753 422 190 106 1349 1790 191 111 3243 131 8388 121 111 1654 467 16480 137 16657 12288 137 547 26626 886 22810 205 256 15368 198 111 2495 147 11160 487 407 758 776 137 2498 4198 145 13328 30113 546 1452 302 1357 4222 111 5831 1654 467 111 502 1302 205 256 469 3082 198 12288 422 121 907 422 1992 360 147 3705 111 4048 698 220 16364 547 12760 220 17922 168 579 14387 131 111 551 131 2498 501 147 15505 238 4330 579 923 547 22810 220 469 547 8388 386 205 121 238 4001 422 1164 12288 220 8576 147 11091 758 3650 7949 579 791 2522 147 10453 776 191 2753 6759 422 137 1411 22810 121 494 7838 205 8328 564 7317 111 1265 131 13472 106 1113 579 902 1654 190 106 19588 975 165 10279 147 4494 8388 923 131 547 3452 3948 2019 205 256 165 2995 198 422 604 106 475 1937 429 506 3803 3425 121 27281 422 3650 579 11482 145 2505 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] ict ##s and farm women : access , use and impact [SEP] this study examines the extent of farm women ' s access to information and communication technologies ( ict ##s ) , their use of ict ##s , the determinants of farm women ' s access to ict ##s , the barriers to ict use , and the impact of ict ##s on farm productivity and women ' s empowerment . the study is based on primary data collected from 120 farm households in selected villages of har ##yan ##a in 2009 . results suggest that farm women ' s access to ict ##s and their usage in the study area are at a low level . however , access to ict ##s has been found to improve the income of farm women households and increase their participation in decision - making . farm women ' s access to ict ##s is largely determined by the socioeconomic status and educational status of the household . women farmers reported use of ict ##s for treatment of sick animals and for getting knowledge on the latest prices of vegetable produce . interestingly , many women farmers having access to ict ##s were also not able to use them because of lack of time and language problem [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] ict ##s and farm women : access , use and impact [SEP] this study examines the extent of farm women ' s access to information and communication technologies ( ict ##s ) , their use of ict ##s , the determinants of farm women ' s access to ict ##s , the barriers to ict use , and the impact of ict ##s on farm productivity and women ' s empowerment . the study is based on primary data collected from 120 farm households in selected villages of har ##yan ##a in 2009 . results suggest that farm women ' s access to ict ##s and their usage in the study area are at a low level . however , access to ict ##s has been found to improve the income of farm women households and increase their participation in decision - making . farm women ' s access to ict ##s is largely determined by the socioeconomic status and educational status of the household . women farmers reported use of ict ##s for treatment of sick animals and for getting knowledge on the latest prices of vegetable produce . interestingly , many women farmers having access to ict ##s were also not able to use them because of lack of time and language problem [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 13328 30113 137 5947 2007 862 1899 422 626 137 2141 103 238 527 15817 111 3796 131 5947 2007 2505 112 1899 147 776 137 2498 4198 145 13328 30113 546 422 547 626 131 13328 30113 422 111 10687 131 5947 2007 2505 112 1899 147 13328 30113 422 111 8719 147 13328 626 422 137 111 2141 131 13328 30113 191 5947 7381 137 2007 2505 112 25295 205 111 527 165 791 191 1916 453 2760 263 5897 5947 9654 121 2350 23070 131 2705 13329 30110 121 4758 205 545 1739 198 5947 2007 2505 112 1899 147 13328 30113 137 547 6323 121 111 527 1590 220 235 106 629 615 205 694 422 1899 147 13328 30113 434 528 797 147 1658 111 5021 131 5947 2007 9654 137 1242 547 6163 121 2015 579 3469 205 5947 2007 2505 112 1899 147 13328 30113 165 5939 1822 214 111 13044 2726 137 6336 2726 131 111 5430 205 2007 12288 1214 626 131 13328 30113 168 922 131 12651 2863 137 168 11657 1767 191 111 13905 6759 131 20288 3299 205 5892 422 1164 2007 12288 2773 1899 147 13328 30113 267 469 302 2357 147 626 1445 923 131 2596 131 532 137 2647 1167 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 13328 30113 137 5947 2007 862 1899 422 626 137 2141 103 238 527 15817 111 3796 131 5947 2007 2505 112 1899 147 776 137 2498 4198 145 13328 30113 546 422 547 626 131 13328 30113 422 111 10687 131 5947 2007 2505 112 1899 147 13328 30113 422 111 8719 147 13328 626 422 137 111 2141 131 13328 30113 191 5947 7381 137 2007 2505 112 25295 205 111 527 165 791 191 1916 453 2760 263 5897 5947 9654 121 2350 23070 131 2705 13329 30110 121 4758 205 545 1739 198 5947 2007 2505 112 1899 147 13328 30113 137 547 6323 121 111 527 1590 220 235 106 629 615 205 694 422 1899 147 13328 30113 434 528 797 147 1658 111 5021 131 5947 2007 9654 137 1242 547 6163 121 2015 579 3469 205 5947 2007 2505 112 1899 147 13328 30113 165 5939 1822 214 111 13044 2726 137 6336 2726 131 111 5430 205 2007 12288 1214 626 131 13328 30113 168 922 131 12651 2863 137 168 11657 1767 191 111 13905 6759 131 20288 3299 205 5892 422 1164 2007 12288 2773 1899 147 13328 30113 267 469 302 2357 147 626 1445 923 131 2596 131 532 137 2647 1167 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] factors effect ##ing change in rice production practices and technologies among small ##holder farmers in kam ##wen ##ge district , uganda [SEP] this study un ##ra ##vel ##ed the factors effect ##ing change in rice production practices and technologies among small ##holder farmers in kam ##wen ##ge district , uganda . a longitudinal study involving six focus group discussions and 100 semi - structured interviews were conducted in august 2015 to february 2016 , and later 21 key inform ##ant interviews in june 2018 to generate data from farmers and local extension staff . while thematic - content analysis was used for the qualitative data , spss v . 18 was used for quantitative data analysis . our findings indicate that changes in rice production practices and technologies were mainly influenced by a combination of factors including technological advancement , farmer ambi ##tions , power dynamics , ed ##aph ##ic and climate variations , access to extension advisory services , use of information and communication technologies ( ict ##s ) such as video and information sharing among farmers . inform ##ants approved the use of video as an important extension service delivery tool in equip ##ping farmers with the necessary knowledge and skills that are key in influencing change in rice production practices and technologies . video also triggers discussions among participants during and after the show through various knowledge sharing mechanisms with serious implications on change in production practices and technologies . as policy and decision makers attempt to reform extension delivery mechanisms [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] factors effect ##ing change in rice production practices and technologies among small ##holder farmers in kam ##wen ##ge district , uganda [SEP] this study un ##ra ##vel ##ed the factors effect ##ing change in rice production practices and technologies among small ##holder farmers in kam ##wen ##ge district , uganda . a longitudinal study involving six focus group discussions and 100 semi - structured interviews were conducted in august 2015 to february 2016 , and later 21 key inform ##ant interviews in june 2018 to generate data from farmers and local extension staff . while thematic - content analysis was used for the qualitative data , spss v . 18 was used for quantitative data analysis . our findings indicate that changes in rice production practices and technologies were mainly influenced by a combination of factors including technological advancement , farmer ambi ##tions , power dynamics , ed ##aph ##ic and climate variations , access to extension advisory services , use of information and communication technologies ( ict ##s ) such as video and information sharing among farmers . inform ##ants approved the use of video as an important extension service delivery tool in equip ##ping farmers with the necessary knowledge and skills that are key in influencing change in rice production practices and technologies . video also triggers discussions among participants during and after the show through various knowledge sharing mechanisms with serious implications on change in production practices and technologies . as policy and decision makers attempt to reform extension delivery mechanisms [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 1391 907 140 1477 121 8058 1865 5423 137 4198 1247 952 13643 12288 121 13629 23740 303 10689 422 26102 103 238 527 254 1942 283 119 111 1391 907 140 1477 121 8058 1865 5423 137 4198 1247 952 13643 12288 121 13629 23740 303 10689 422 26102 205 106 7051 527 5005 2781 1790 583 8020 137 1287 6317 579 7908 8572 267 2728 121 5714 4505 147 6497 5323 422 137 2269 2337 1519 3219 268 8572 121 4914 7491 147 3730 453 263 12288 137 1338 3840 5680 205 969 24717 579 2118 669 241 501 168 111 6526 453 422 13303 171 205 1178 241 501 168 4221 453 669 205 580 2116 2645 198 1334 121 8058 1865 5423 137 4198 267 3680 6452 214 106 2702 131 1391 1471 8797 19546 422 24830 9061 381 422 1221 3277 422 777 747 141 137 6002 4688 422 1899 147 3840 20080 2522 422 626 131 776 137 2498 4198 145 13328 30113 546 555 188 4081 137 776 6404 1247 12288 205 3219 773 6152 111 626 131 4081 188 130 1060 3840 2289 4277 3130 121 6064 5354 12288 190 111 2538 1767 137 5561 198 220 1519 121 12657 1477 121 8058 1865 5423 137 4198 205 4081 469 14602 8020 1247 1914 781 137 647 111 405 833 1711 1767 6404 2571 190 6083 5214 191 1477 121 1865 5423 137 4198 205 188 2951 137 2015 16781 5809 147 9561 3840 4277 2571 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 1391 907 140 1477 121 8058 1865 5423 137 4198 1247 952 13643 12288 121 13629 23740 303 10689 422 26102 103 238 527 254 1942 283 119 111 1391 907 140 1477 121 8058 1865 5423 137 4198 1247 952 13643 12288 121 13629 23740 303 10689 422 26102 205 106 7051 527 5005 2781 1790 583 8020 137 1287 6317 579 7908 8572 267 2728 121 5714 4505 147 6497 5323 422 137 2269 2337 1519 3219 268 8572 121 4914 7491 147 3730 453 263 12288 137 1338 3840 5680 205 969 24717 579 2118 669 241 501 168 111 6526 453 422 13303 171 205 1178 241 501 168 4221 453 669 205 580 2116 2645 198 1334 121 8058 1865 5423 137 4198 267 3680 6452 214 106 2702 131 1391 1471 8797 19546 422 24830 9061 381 422 1221 3277 422 777 747 141 137 6002 4688 422 1899 147 3840 20080 2522 422 626 131 776 137 2498 4198 145 13328 30113 546 555 188 4081 137 776 6404 1247 12288 205 3219 773 6152 111 626 131 4081 188 130 1060 3840 2289 4277 3130 121 6064 5354 12288 190 111 2538 1767 137 5561 198 220 1519 121 12657 1477 121 8058 1865 5423 137 4198 205 4081 469 14602 8020 1247 1914 781 137 647 111 405 833 1711 1767 6404 2571 190 6083 5214 191 1477 121 1865 5423 137 4198 205 188 2951 137 2015 16781 5809 147 9561 3840 4277 2571 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] applying the best - fit framework to assess and strengthen national extension and advisory services [SEP] agricultural extension and advisory services are critical to supporting technological and institutional changes that can improve the live ##li ##hood ##s of small - scale farmers in developing countries . however , many extension services are under - resource ##d , out of date , and need of structural and content changes . however , efforts to systematically strengthen local extension systems often fall into the trap of promoting blue ##prints that are insufficient ##ly adapted to local context . to that end , researchers developed the best - fit framework in the 2000 ##s to provide imp ##etus for pursuit of more locally - tailored extension solutions . today , almost a decade later , researchers test the framework under real - world conditions in a cross - country application . this paper examines the application of this framework across six dimensions and seven countries to formulate a set of best - fit recommendations that are also broadly appreciable . the findings show that it is possible to apply the framework to the analysis of eas across countries while also maintaining a very localized perspective on recommendations . across the seven countries , certain obvious common ##alities exist : the growth in plural ##ism in extension providers , the persistence of weak incentives for extension agents , and the lack of enabling policies . at the same time , innovative solutions to many of the challenges held [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] applying the best - fit framework to assess and strengthen national extension and advisory services [SEP] agricultural extension and advisory services are critical to supporting technological and institutional changes that can improve the live ##li ##hood ##s of small - scale farmers in developing countries . however , many extension services are under - resource ##d , out of date , and need of structural and content changes . however , efforts to systematically strengthen local extension systems often fall into the trap of promoting blue ##prints that are insufficient ##ly adapted to local context . to that end , researchers developed the best - fit framework in the 2000 ##s to provide imp ##etus for pursuit of more locally - tailored extension solutions . today , almost a decade later , researchers test the framework under real - world conditions in a cross - country application . this paper examines the application of this framework across six dimensions and seven countries to formulate a set of best - fit recommendations that are also broadly appreciable . the findings show that it is possible to apply the framework to the analysis of eas across countries while also maintaining a very localized perspective on recommendations . across the seven countries , certain obvious common ##alities exist : the growth in plural ##ism in extension providers , the persistence of weak incentives for extension agents , and the lack of enabling policies . at the same time , innovative solutions to many of the challenges held [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 4880 111 2172 579 2471 2641 147 1285 137 10469 2227 3840 137 20080 2522 103 7675 3840 137 20080 2522 220 2616 147 5182 8797 137 6741 1334 198 300 1658 111 6489 3206 4402 30113 131 952 579 2211 12288 121 3775 3198 205 694 422 1164 3840 2522 220 604 579 3955 30118 422 556 131 4282 422 137 965 131 3276 137 2118 1334 205 694 422 5719 147 11810 10469 1338 3840 1078 1992 3913 690 111 11639 131 10302 5175 12043 198 220 10119 179 8030 147 1338 2220 205 147 198 864 422 4538 1815 111 2172 579 2471 2641 121 111 4708 30113 147 1584 1082 28925 168 19502 131 475 7567 579 17111 3840 2727 205 7121 422 3343 106 10028 2269 422 4538 856 111 2641 604 1332 579 2399 1245 121 106 2057 579 5192 1836 205 238 1203 15817 111 1836 131 238 2641 2186 2781 4795 137 5152 3198 147 15835 106 610 131 2172 579 2471 6869 198 220 469 13771 26405 205 111 2116 405 198 256 165 1263 147 4245 111 2641 147 111 669 131 1997 2186 3198 969 469 7732 106 1248 7560 5211 191 6869 205 2186 111 5152 3198 422 2361 4385 1495 7000 1730 862 111 1503 121 22533 960 121 3840 7739 422 111 11583 131 2973 12493 168 3840 3027 422 137 111 2596 131 10619 5691 205 235 111 855 532 422 11577 2727 147 1164 131 111 5212 7243 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 4880 111 2172 579 2471 2641 147 1285 137 10469 2227 3840 137 20080 2522 103 7675 3840 137 20080 2522 220 2616 147 5182 8797 137 6741 1334 198 300 1658 111 6489 3206 4402 30113 131 952 579 2211 12288 121 3775 3198 205 694 422 1164 3840 2522 220 604 579 3955 30118 422 556 131 4282 422 137 965 131 3276 137 2118 1334 205 694 422 5719 147 11810 10469 1338 3840 1078 1992 3913 690 111 11639 131 10302 5175 12043 198 220 10119 179 8030 147 1338 2220 205 147 198 864 422 4538 1815 111 2172 579 2471 2641 121 111 4708 30113 147 1584 1082 28925 168 19502 131 475 7567 579 17111 3840 2727 205 7121 422 3343 106 10028 2269 422 4538 856 111 2641 604 1332 579 2399 1245 121 106 2057 579 5192 1836 205 238 1203 15817 111 1836 131 238 2641 2186 2781 4795 137 5152 3198 147 15835 106 610 131 2172 579 2471 6869 198 220 469 13771 26405 205 111 2116 405 198 256 165 1263 147 4245 111 2641 147 111 669 131 1997 2186 3198 969 469 7732 106 1248 7560 5211 191 6869 205 2186 111 5152 3198 422 2361 4385 1495 7000 1730 862 111 1503 121 22533 960 121 3840 7739 422 111 11583 131 2973 12493 168 3840 3027 422 137 111 2596 131 10619 5691 205 235 111 855 532 422 11577 2727 147 1164 131 111 5212 7243 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] use of ict in decision - making of agricultural marketing : factors determining of farmers  involvement [SEP] the paper investigated the magnitude of social parameters ' impacts on effective use of information and communication technology ( ict ) in agricultural marketing by the farmers . in addition , how farmers identify information sources and how they access to those sources in selected areas of jama ##lp ##ur district were also examined . pur ##pos ##ive sampling method was used in this investigation and data were collected from eighty farmer ' s thorough survey questionnaire . descriptive statistics , likert scale and multivariate regression model were used to analyze the data . multivariate regression model was specified and estimated to identify the factors affecting use of ict by farmers . the outcome of this study highlights important factors for the use of ict . it is evident from the findings that the users of ict are getting better quality information and are hence making significantly better decisions on all aspects of agricultural marketing . regression analysis revealed that two factors i . e . cultivated land of farmers and level of education were the important factors of using of ict by farmers . modern ict tool such as mobile phone was the most used device by the farmers due to low price and availability . they collected most of the information regarding marketing activities of their produce by mobile phone from other progressive farmers , traders and agricultural extension workers . social imperative findings [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] use of ict in decision - making of agricultural marketing : factors determining of farmers  involvement [SEP] the paper investigated the magnitude of social parameters ' impacts on effective use of information and communication technology ( ict ) in agricultural marketing by the farmers . in addition , how farmers identify information sources and how they access to those sources in selected areas of jama ##lp ##ur district were also examined . pur ##pos ##ive sampling method was used in this investigation and data were collected from eighty farmer ' s thorough survey questionnaire . descriptive statistics , likert scale and multivariate regression model were used to analyze the data . multivariate regression model was specified and estimated to identify the factors affecting use of ict by farmers . the outcome of this study highlights important factors for the use of ict . it is evident from the findings that the users of ict are getting better quality information and are hence making significantly better decisions on all aspects of agricultural marketing . regression analysis revealed that two factors i . e . cultivated land of farmers and level of education were the important factors of using of ict by farmers . modern ict tool such as mobile phone was the most used device by the farmers due to low price and availability . they collected most of the information regarding marketing activities of their produce by mobile phone from other progressive farmers , traders and agricultural extension workers . social imperative findings [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 626 131 13328 121 2015 579 3469 131 7675 10644 862 1391 5735 131 12288 5459 5642 103 111 1203 3003 111 4377 131 1748 1496 2505 7681 191 2115 626 131 776 137 2498 2048 145 13328 546 121 7675 10644 214 111 12288 205 121 867 422 539 12288 2743 776 3117 137 539 698 1899 147 1052 3117 121 2350 2326 131 17394 10509 166 10689 267 469 2959 205 1434 451 1090 3597 551 241 501 121 238 4534 137 453 267 2760 263 27075 24830 2505 112 9763 3241 5692 205 10363 4530 422 22304 2211 137 7642 3089 437 267 501 147 5230 111 453 205 7642 3089 437 241 5723 137 2595 147 2743 111 1391 7586 626 131 13328 214 12288 205 111 3095 131 238 527 11878 1060 1391 168 111 626 131 13328 205 256 165 6899 263 111 2116 198 111 2485 131 13328 220 11657 1883 1671 776 137 220 2217 3469 1357 1883 5207 191 355 3905 131 7675 10644 205 3089 669 2861 198 502 1391 259 205 139 205 17770 2882 131 12288 137 615 131 2870 267 111 1060 1391 131 487 131 13328 214 12288 205 5901 13328 3130 555 188 3650 7949 241 111 755 501 3264 214 111 12288 1074 147 629 4048 137 5232 205 698 2760 755 131 111 776 3560 10644 2687 131 547 3299 214 3650 7949 263 494 8381 12288 422 29607 137 7675 3840 5555 205 1748 19370 2116 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 626 131 13328 121 2015 579 3469 131 7675 10644 862 1391 5735 131 12288 5459 5642 103 111 1203 3003 111 4377 131 1748 1496 2505 7681 191 2115 626 131 776 137 2498 2048 145 13328 546 121 7675 10644 214 111 12288 205 121 867 422 539 12288 2743 776 3117 137 539 698 1899 147 1052 3117 121 2350 2326 131 17394 10509 166 10689 267 469 2959 205 1434 451 1090 3597 551 241 501 121 238 4534 137 453 267 2760 263 27075 24830 2505 112 9763 3241 5692 205 10363 4530 422 22304 2211 137 7642 3089 437 267 501 147 5230 111 453 205 7642 3089 437 241 5723 137 2595 147 2743 111 1391 7586 626 131 13328 214 12288 205 111 3095 131 238 527 11878 1060 1391 168 111 626 131 13328 205 256 165 6899 263 111 2116 198 111 2485 131 13328 220 11657 1883 1671 776 137 220 2217 3469 1357 1883 5207 191 355 3905 131 7675 10644 205 3089 669 2861 198 502 1391 259 205 139 205 17770 2882 131 12288 137 615 131 2870 267 111 1060 1391 131 487 131 13328 214 12288 205 5901 13328 3130 555 188 3650 7949 241 111 755 501 3264 214 111 12288 1074 147 629 4048 137 5232 205 698 2760 755 131 111 776 3560 10644 2687 131 547 3299 214 3650 7949 263 494 8381 12288 422 29607 137 7675 3840 5555 205 1748 19370 2116 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used for model gpu 0\n",
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from study_type_multi\\model.ckpt-2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from study_type_multi\\model.ckpt-2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "results = _study_type_labeler.predict_for_df(dfs_data[[\"title\", \"abstract\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_data_true_labels = []\n",
    "dfs_data_predicted_labels = []\n",
    "study_type_dicts = {\n",
    " 'Observational study': 'Qualitative',\n",
    " 'Modeling study': 'Quantitative',\n",
    " 'Laboratory study': 'Quantitative',\n",
    " 'Review paper': 'Qualitative',\n",
    " 'Field study': 'Quantitative'\n",
    "}\n",
    "for i in range(len(dfs_data)):\n",
    "    if dfs_data[\"Study Design\"].values[i].strip():\n",
    "        \n",
    "        found_labels = set()\n",
    "        for idx, label in enumerate(study_labels):\n",
    "            if results[1][i][idx] == 1 and idx != 3:\n",
    "                found_labels.add(study_type_dicts[label])\n",
    "        if len(found_labels) == 0:\n",
    "            continue\n",
    "        elif len(found_labels) == 1:\n",
    "            dfs_data_predicted_labels.append(list(found_labels)[0])\n",
    "        else:\n",
    "            dfs_data_predicted_labels.append(\"Mixed methods\")\n",
    "        dfs_data_true_labels.append(dfs_data[\"Study Design\"].values[i].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] external evaluation of mobile phone technology - based nutrition and agriculture advisory services in africa : mobile phones , agriculture , and nutrition in ghana [SEP] mn ##utr ##ition was a five - year global initiative supported by fc ##do , organised by group ##e special ##e mobile association ( gsm ##a ) , and implemented by in - country mobile network operators ( mn ##os ) and third - party providers . it sought to use mobile technology to improve the health and nutritional status of children and adults in low - income countries around the world . the nutrition content of the programme aimed to promote behaviour change around key dietary and child feeding practices likely to result in improved nutritional health within a household . [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] external evaluation of mobile phone technology - based nutrition and agriculture advisory services in africa : mobile phones , agriculture , and nutrition in ghana [SEP] mn ##utr ##ition was a five - year global initiative supported by fc ##do , organised by group ##e special ##e mobile association ( gsm ##a ) , and implemented by in - country mobile network operators ( mn ##os ) and third - party providers . it sought to use mobile technology to improve the health and nutritional status of children and adults in low - income countries around the world . the nutrition content of the programme aimed to promote behaviour change around key dietary and child feeding practices likely to result in improved nutritional health within a household . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 3458 2166 131 3650 7949 2048 579 791 8556 137 9879 20080 2522 121 7611 862 3650 18113 422 9879 422 137 8556 121 26274 103 5060 18424 319 241 106 2539 579 996 2523 12570 2810 214 8003 2904 422 26604 214 583 30107 2618 30107 3650 2274 145 23885 30110 546 422 137 3812 214 121 579 5192 3650 934 5669 145 5060 169 546 137 2765 579 9408 7739 205 256 10637 147 626 3650 2048 147 1658 111 947 137 11073 2726 131 1808 137 4311 121 629 579 5021 3198 2715 111 2399 205 111 8556 2118 131 111 8547 7489 147 6241 3993 1477 2715 1519 6867 137 1326 6983 5423 1987 147 1186 121 3007 11073 947 1017 106 5430 205 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 3458 2166 131 3650 7949 2048 579 791 8556 137 9879 20080 2522 121 7611 862 3650 18113 422 9879 422 137 8556 121 26274 103 5060 18424 319 241 106 2539 579 996 2523 12570 2810 214 8003 2904 422 26604 214 583 30107 2618 30107 3650 2274 145 23885 30110 546 422 137 3812 214 121 579 5192 3650 934 5669 145 5060 169 546 137 2765 579 9408 7739 205 256 10637 147 626 3650 2048 147 1658 111 947 137 11073 2726 131 1808 137 4311 121 629 579 5021 3198 2715 111 2399 205 111 8556 2118 131 111 8547 7489 147 6241 3993 1477 2715 1519 6867 137 1326 6983 5423 1987 147 1186 121 3007 11073 947 1017 106 5430 205 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] external evaluation of mobile phone technology - based nutrition and agriculture advisory services in africa : mobile phones , agriculture , and nutrition in ghana [SEP] mn ##utr ##ition was a five - year global initiative supported by fc ##do , organised by group ##e special ##e mobile association ( gsm ##a ) , and implemented by in - country mobile network operators ( mn ##os ) and third - party providers . it sought to use mobile technology to improve the health and nutritional status of children and adults in low - income countries around the world . the nutrition content of the programme aimed to promote behaviour change around key dietary and child feeding practices likely to result in improved nutritional health within a household . [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] external evaluation of mobile phone technology - based nutrition and agriculture advisory services in africa : mobile phones , agriculture , and nutrition in ghana [SEP] mn ##utr ##ition was a five - year global initiative supported by fc ##do , organised by group ##e special ##e mobile association ( gsm ##a ) , and implemented by in - country mobile network operators ( mn ##os ) and third - party providers . it sought to use mobile technology to improve the health and nutritional status of children and adults in low - income countries around the world . the nutrition content of the programme aimed to promote behaviour change around key dietary and child feeding practices likely to result in improved nutritional health within a household . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 3458 2166 131 3650 7949 2048 579 791 8556 137 9879 20080 2522 121 7611 862 3650 18113 422 9879 422 137 8556 121 26274 103 5060 18424 319 241 106 2539 579 996 2523 12570 2810 214 8003 2904 422 26604 214 583 30107 2618 30107 3650 2274 145 23885 30110 546 422 137 3812 214 121 579 5192 3650 934 5669 145 5060 169 546 137 2765 579 9408 7739 205 256 10637 147 626 3650 2048 147 1658 111 947 137 11073 2726 131 1808 137 4311 121 629 579 5021 3198 2715 111 2399 205 111 8556 2118 131 111 8547 7489 147 6241 3993 1477 2715 1519 6867 137 1326 6983 5423 1987 147 1186 121 3007 11073 947 1017 106 5430 205 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 3458 2166 131 3650 7949 2048 579 791 8556 137 9879 20080 2522 121 7611 862 3650 18113 422 9879 422 137 8556 121 26274 103 5060 18424 319 241 106 2539 579 996 2523 12570 2810 214 8003 2904 422 26604 214 583 30107 2618 30107 3650 2274 145 23885 30110 546 422 137 3812 214 121 579 5192 3650 934 5669 145 5060 169 546 137 2765 579 9408 7739 205 256 10637 147 626 3650 2048 147 1658 111 947 137 11073 2726 131 1808 137 4311 121 629 579 5021 3198 2715 111 2399 205 111 8556 2118 131 111 8547 7489 147 6241 3993 1477 2715 1519 6867 137 1326 6983 5423 1987 147 1186 121 3007 11073 947 1017 106 5430 205 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used for model gpu 0\n",
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from study_type_multi\\model.ckpt-2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from study_type_multi\\model.ckpt-2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([array([0.5679293 , 0.05406347, 0.01991221, 0.11234513, 0.03242701],\n",
       "        dtype=float32)], [[1, 0, 0, 0, 0]], [-1])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_study_type_labeler.predict_for_df(dfs_data[[\"title\", \"abstract\"]][152:153])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Cost analysis of nutrition messaging intervention through community-led videos in Odisha'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_data[\"title\"].values[143]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "_count_vec = CountVectorizer(binary=True, analyzer=lambda x: x)\n",
    "transformed_val = _count_vec.fit_transform(dfs_data[\"Study Methods\"].apply(lambda x: [n.strip() for n in x.split(\"|\")])).toarray()\n",
    "pd_counts = pd.DataFrame(transformed_val, columns= [ \"methods_ \" + a for a in _count_vec.get_feature_names()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Mixed methods</th>\n",
       "      <th>Qualitative</th>\n",
       "      <th>Quantitative</th>\n",
       "      <th>methods_</th>\n",
       "      <th>methods_ Action research experiment (similar to Random Control Trial (RCT))</th>\n",
       "      <th>methods_ Bank transaction data</th>\n",
       "      <th>methods_ Data from government and DFS</th>\n",
       "      <th>methods_ Interview/focus group</th>\n",
       "      <th>methods_ Observation</th>\n",
       "      <th>...</th>\n",
       "      <th>methods_ Quiz</th>\n",
       "      <th>methods_ Random Control Trial (RCT)</th>\n",
       "      <th>methods_ Self-reported survey</th>\n",
       "      <th>methods_ Service data</th>\n",
       "      <th>methods_ Similar to Random Control Trial (RCT) (Quasi-experiment)</th>\n",
       "      <th>methods_ Simple field experiment (Quasi-Random Control Trial (RCT)?)</th>\n",
       "      <th>methods_ Transaction data</th>\n",
       "      <th>methods_ Willingness to pay experiment</th>\n",
       "      <th>methods_ on-farm trials</th>\n",
       "      <th>methods_ pseudo-Random Control Trial (RCT)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.160422</td>\n",
       "      <td>-0.089562</td>\n",
       "      <td>-0.160422</td>\n",
       "      <td>0.808452</td>\n",
       "      <td>-0.015754</td>\n",
       "      <td>-0.015754</td>\n",
       "      <td>-0.015754</td>\n",
       "      <td>-0.206948</td>\n",
       "      <td>-0.048507</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015754</td>\n",
       "      <td>-0.087557</td>\n",
       "      <td>-0.148036</td>\n",
       "      <td>-0.053988</td>\n",
       "      <td>-0.015754</td>\n",
       "      <td>-0.015754</td>\n",
       "      <td>-0.015754</td>\n",
       "      <td>-0.015754</td>\n",
       "      <td>-0.015754</td>\n",
       "      <td>-0.015754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mixed methods</th>\n",
       "      <td>-0.160422</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.366378</td>\n",
       "      <td>-0.656250</td>\n",
       "      <td>-0.087142</td>\n",
       "      <td>-0.064448</td>\n",
       "      <td>-0.064448</td>\n",
       "      <td>-0.064448</td>\n",
       "      <td>0.466274</td>\n",
       "      <td>-0.087142</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.064448</td>\n",
       "      <td>-0.045264</td>\n",
       "      <td>0.332892</td>\n",
       "      <td>0.336535</td>\n",
       "      <td>-0.064448</td>\n",
       "      <td>-0.064448</td>\n",
       "      <td>-0.064448</td>\n",
       "      <td>0.098206</td>\n",
       "      <td>-0.064448</td>\n",
       "      <td>-0.064448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qualitative</th>\n",
       "      <td>-0.089562</td>\n",
       "      <td>-0.366378</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.366378</td>\n",
       "      <td>-0.110782</td>\n",
       "      <td>-0.035980</td>\n",
       "      <td>-0.035980</td>\n",
       "      <td>-0.035980</td>\n",
       "      <td>0.399242</td>\n",
       "      <td>0.324141</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035980</td>\n",
       "      <td>-0.199966</td>\n",
       "      <td>-0.303161</td>\n",
       "      <td>-0.123299</td>\n",
       "      <td>-0.035980</td>\n",
       "      <td>-0.035980</td>\n",
       "      <td>-0.035980</td>\n",
       "      <td>-0.035980</td>\n",
       "      <td>-0.035980</td>\n",
       "      <td>-0.035980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Quantitative</th>\n",
       "      <td>-0.160422</td>\n",
       "      <td>-0.656250</td>\n",
       "      <td>-0.366378</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.142787</td>\n",
       "      <td>0.098206</td>\n",
       "      <td>0.098206</td>\n",
       "      <td>0.098206</td>\n",
       "      <td>-0.692125</td>\n",
       "      <td>-0.142787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098206</td>\n",
       "      <td>0.232879</td>\n",
       "      <td>-0.042497</td>\n",
       "      <td>-0.220851</td>\n",
       "      <td>0.098206</td>\n",
       "      <td>0.098206</td>\n",
       "      <td>0.098206</td>\n",
       "      <td>-0.064448</td>\n",
       "      <td>0.098206</td>\n",
       "      <td>0.098206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>methods_</th>\n",
       "      <td>0.808452</td>\n",
       "      <td>-0.087142</td>\n",
       "      <td>-0.110782</td>\n",
       "      <td>-0.142787</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.019487</td>\n",
       "      <td>-0.019487</td>\n",
       "      <td>-0.019487</td>\n",
       "      <td>-0.255981</td>\n",
       "      <td>-0.060000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019487</td>\n",
       "      <td>-0.108302</td>\n",
       "      <td>-0.183110</td>\n",
       "      <td>-0.066779</td>\n",
       "      <td>-0.019487</td>\n",
       "      <td>-0.019487</td>\n",
       "      <td>-0.019487</td>\n",
       "      <td>-0.019487</td>\n",
       "      <td>-0.019487</td>\n",
       "      <td>-0.019487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>methods_ Action research experiment (similar to Random Control Trial (RCT))</th>\n",
       "      <td>-0.015754</td>\n",
       "      <td>-0.064448</td>\n",
       "      <td>-0.035980</td>\n",
       "      <td>0.098206</td>\n",
       "      <td>-0.019487</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.083139</td>\n",
       "      <td>-0.019487</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.035175</td>\n",
       "      <td>-0.059471</td>\n",
       "      <td>-0.021689</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>methods_ Bank transaction data</th>\n",
       "      <td>-0.015754</td>\n",
       "      <td>-0.064448</td>\n",
       "      <td>-0.035980</td>\n",
       "      <td>0.098206</td>\n",
       "      <td>-0.019487</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.083139</td>\n",
       "      <td>-0.019487</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.035175</td>\n",
       "      <td>-0.059471</td>\n",
       "      <td>-0.021689</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>methods_ Data from government and DFS</th>\n",
       "      <td>-0.015754</td>\n",
       "      <td>-0.064448</td>\n",
       "      <td>-0.035980</td>\n",
       "      <td>0.098206</td>\n",
       "      <td>-0.019487</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.083139</td>\n",
       "      <td>-0.019487</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.035175</td>\n",
       "      <td>-0.059471</td>\n",
       "      <td>-0.021689</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>methods_ Interview/focus group</th>\n",
       "      <td>-0.206948</td>\n",
       "      <td>0.466274</td>\n",
       "      <td>0.399242</td>\n",
       "      <td>-0.692125</td>\n",
       "      <td>-0.255981</td>\n",
       "      <td>-0.083139</td>\n",
       "      <td>-0.083139</td>\n",
       "      <td>-0.083139</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.179906</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.083139</td>\n",
       "      <td>-0.291835</td>\n",
       "      <td>0.058950</td>\n",
       "      <td>0.211259</td>\n",
       "      <td>-0.083139</td>\n",
       "      <td>-0.083139</td>\n",
       "      <td>-0.083139</td>\n",
       "      <td>0.076127</td>\n",
       "      <td>-0.083139</td>\n",
       "      <td>-0.083139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>methods_ Observation</th>\n",
       "      <td>-0.048507</td>\n",
       "      <td>-0.087142</td>\n",
       "      <td>0.324141</td>\n",
       "      <td>-0.142787</td>\n",
       "      <td>-0.060000</td>\n",
       "      <td>-0.019487</td>\n",
       "      <td>-0.019487</td>\n",
       "      <td>-0.019487</td>\n",
       "      <td>0.179906</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019487</td>\n",
       "      <td>-0.034712</td>\n",
       "      <td>-0.069603</td>\n",
       "      <td>-0.066779</td>\n",
       "      <td>-0.019487</td>\n",
       "      <td>-0.019487</td>\n",
       "      <td>-0.019487</td>\n",
       "      <td>-0.019487</td>\n",
       "      <td>-0.019487</td>\n",
       "      <td>-0.019487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>methods_ On-farm trials</th>\n",
       "      <td>-0.015754</td>\n",
       "      <td>-0.064448</td>\n",
       "      <td>-0.035980</td>\n",
       "      <td>0.098206</td>\n",
       "      <td>-0.019487</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.083139</td>\n",
       "      <td>-0.019487</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.035175</td>\n",
       "      <td>-0.059471</td>\n",
       "      <td>-0.021689</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>methods_ Pre and post assessment</th>\n",
       "      <td>-0.063914</td>\n",
       "      <td>0.222445</td>\n",
       "      <td>-0.145969</td>\n",
       "      <td>-0.085492</td>\n",
       "      <td>-0.079057</td>\n",
       "      <td>-0.025677</td>\n",
       "      <td>-0.025677</td>\n",
       "      <td>0.246494</td>\n",
       "      <td>0.007315</td>\n",
       "      <td>-0.079057</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025677</td>\n",
       "      <td>-0.084523</td>\n",
       "      <td>0.027936</td>\n",
       "      <td>-0.003200</td>\n",
       "      <td>-0.025677</td>\n",
       "      <td>-0.025677</td>\n",
       "      <td>-0.025677</td>\n",
       "      <td>-0.025677</td>\n",
       "      <td>-0.025677</td>\n",
       "      <td>-0.025677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>methods_ Propensity score matching</th>\n",
       "      <td>-0.022351</td>\n",
       "      <td>-0.091432</td>\n",
       "      <td>-0.051046</td>\n",
       "      <td>0.139325</td>\n",
       "      <td>-0.027647</td>\n",
       "      <td>-0.008979</td>\n",
       "      <td>-0.008979</td>\n",
       "      <td>-0.008979</td>\n",
       "      <td>-0.117950</td>\n",
       "      <td>-0.027647</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008979</td>\n",
       "      <td>-0.049903</td>\n",
       "      <td>-0.084373</td>\n",
       "      <td>-0.030770</td>\n",
       "      <td>-0.008979</td>\n",
       "      <td>-0.008979</td>\n",
       "      <td>-0.008979</td>\n",
       "      <td>-0.008979</td>\n",
       "      <td>-0.008979</td>\n",
       "      <td>-0.008979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>methods_ Quasi-Random Control Trial (RCT)</th>\n",
       "      <td>-0.063914</td>\n",
       "      <td>0.178454</td>\n",
       "      <td>-0.088662</td>\n",
       "      <td>-0.085492</td>\n",
       "      <td>-0.079057</td>\n",
       "      <td>-0.025677</td>\n",
       "      <td>-0.025677</td>\n",
       "      <td>-0.025677</td>\n",
       "      <td>0.050390</td>\n",
       "      <td>-0.079057</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025677</td>\n",
       "      <td>-0.142700</td>\n",
       "      <td>-0.016931</td>\n",
       "      <td>0.420749</td>\n",
       "      <td>-0.025677</td>\n",
       "      <td>-0.025677</td>\n",
       "      <td>-0.025677</td>\n",
       "      <td>-0.025677</td>\n",
       "      <td>-0.025677</td>\n",
       "      <td>-0.025677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>methods_ Quiz</th>\n",
       "      <td>-0.015754</td>\n",
       "      <td>-0.064448</td>\n",
       "      <td>-0.035980</td>\n",
       "      <td>0.098206</td>\n",
       "      <td>-0.019487</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.083139</td>\n",
       "      <td>-0.019487</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.035175</td>\n",
       "      <td>-0.059471</td>\n",
       "      <td>-0.021689</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>methods_ Random Control Trial (RCT)</th>\n",
       "      <td>-0.087557</td>\n",
       "      <td>-0.045264</td>\n",
       "      <td>-0.199966</td>\n",
       "      <td>0.232879</td>\n",
       "      <td>-0.108302</td>\n",
       "      <td>-0.035175</td>\n",
       "      <td>-0.035175</td>\n",
       "      <td>-0.035175</td>\n",
       "      <td>-0.291835</td>\n",
       "      <td>-0.034712</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035175</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.330520</td>\n",
       "      <td>-0.120539</td>\n",
       "      <td>-0.035175</td>\n",
       "      <td>-0.035175</td>\n",
       "      <td>-0.035175</td>\n",
       "      <td>-0.035175</td>\n",
       "      <td>-0.035175</td>\n",
       "      <td>-0.035175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>methods_ Self-reported survey</th>\n",
       "      <td>-0.148036</td>\n",
       "      <td>0.332892</td>\n",
       "      <td>-0.303161</td>\n",
       "      <td>-0.042497</td>\n",
       "      <td>-0.183110</td>\n",
       "      <td>-0.059471</td>\n",
       "      <td>-0.059471</td>\n",
       "      <td>-0.059471</td>\n",
       "      <td>0.058950</td>\n",
       "      <td>-0.069603</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059471</td>\n",
       "      <td>-0.330520</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.106288</td>\n",
       "      <td>-0.059471</td>\n",
       "      <td>-0.059471</td>\n",
       "      <td>-0.059471</td>\n",
       "      <td>-0.059471</td>\n",
       "      <td>-0.059471</td>\n",
       "      <td>-0.059471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>methods_ Service data</th>\n",
       "      <td>-0.053988</td>\n",
       "      <td>0.336535</td>\n",
       "      <td>-0.123299</td>\n",
       "      <td>-0.220851</td>\n",
       "      <td>-0.066779</td>\n",
       "      <td>-0.021689</td>\n",
       "      <td>-0.021689</td>\n",
       "      <td>-0.021689</td>\n",
       "      <td>0.211259</td>\n",
       "      <td>-0.066779</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021689</td>\n",
       "      <td>-0.120539</td>\n",
       "      <td>0.106288</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.021689</td>\n",
       "      <td>-0.021689</td>\n",
       "      <td>-0.021689</td>\n",
       "      <td>-0.021689</td>\n",
       "      <td>-0.021689</td>\n",
       "      <td>-0.021689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>methods_ Similar to Random Control Trial (RCT) (Quasi-experiment)</th>\n",
       "      <td>-0.015754</td>\n",
       "      <td>-0.064448</td>\n",
       "      <td>-0.035980</td>\n",
       "      <td>0.098206</td>\n",
       "      <td>-0.019487</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.083139</td>\n",
       "      <td>-0.019487</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.035175</td>\n",
       "      <td>-0.059471</td>\n",
       "      <td>-0.021689</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>methods_ Simple field experiment (Quasi-Random Control Trial (RCT)?)</th>\n",
       "      <td>-0.015754</td>\n",
       "      <td>-0.064448</td>\n",
       "      <td>-0.035980</td>\n",
       "      <td>0.098206</td>\n",
       "      <td>-0.019487</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.083139</td>\n",
       "      <td>-0.019487</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.035175</td>\n",
       "      <td>-0.059471</td>\n",
       "      <td>-0.021689</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>methods_ Transaction data</th>\n",
       "      <td>-0.015754</td>\n",
       "      <td>-0.064448</td>\n",
       "      <td>-0.035980</td>\n",
       "      <td>0.098206</td>\n",
       "      <td>-0.019487</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.083139</td>\n",
       "      <td>-0.019487</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.035175</td>\n",
       "      <td>-0.059471</td>\n",
       "      <td>-0.021689</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>methods_ Willingness to pay experiment</th>\n",
       "      <td>-0.015754</td>\n",
       "      <td>0.098206</td>\n",
       "      <td>-0.035980</td>\n",
       "      <td>-0.064448</td>\n",
       "      <td>-0.019487</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>0.076127</td>\n",
       "      <td>-0.019487</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.035175</td>\n",
       "      <td>-0.059471</td>\n",
       "      <td>-0.021689</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>methods_ on-farm trials</th>\n",
       "      <td>-0.015754</td>\n",
       "      <td>-0.064448</td>\n",
       "      <td>-0.035980</td>\n",
       "      <td>0.098206</td>\n",
       "      <td>-0.019487</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.083139</td>\n",
       "      <td>-0.019487</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.035175</td>\n",
       "      <td>-0.059471</td>\n",
       "      <td>-0.021689</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.006329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>methods_ pseudo-Random Control Trial (RCT)</th>\n",
       "      <td>-0.015754</td>\n",
       "      <td>-0.064448</td>\n",
       "      <td>-0.035980</td>\n",
       "      <td>0.098206</td>\n",
       "      <td>-0.019487</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.083139</td>\n",
       "      <td>-0.019487</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.035175</td>\n",
       "      <td>-0.059471</td>\n",
       "      <td>-0.021689</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24 rows  24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              Mixed methods  \\\n",
       "                                                    1.000000      -0.160422   \n",
       "Mixed methods                                      -0.160422       1.000000   \n",
       "Qualitative                                        -0.089562      -0.366378   \n",
       "Quantitative                                       -0.160422      -0.656250   \n",
       "methods_                                            0.808452      -0.087142   \n",
       "methods_ Action research experiment (similar to... -0.015754      -0.064448   \n",
       "methods_ Bank transaction data                     -0.015754      -0.064448   \n",
       "methods_ Data from government and DFS              -0.015754      -0.064448   \n",
       "methods_ Interview/focus group                     -0.206948       0.466274   \n",
       "methods_ Observation                               -0.048507      -0.087142   \n",
       "methods_ On-farm trials                            -0.015754      -0.064448   \n",
       "methods_ Pre and post assessment                   -0.063914       0.222445   \n",
       "methods_ Propensity score matching                 -0.022351      -0.091432   \n",
       "methods_ Quasi-Random Control Trial (RCT)          -0.063914       0.178454   \n",
       "methods_ Quiz                                      -0.015754      -0.064448   \n",
       "methods_ Random Control Trial (RCT)                -0.087557      -0.045264   \n",
       "methods_ Self-reported survey                      -0.148036       0.332892   \n",
       "methods_ Service data                              -0.053988       0.336535   \n",
       "methods_ Similar to Random Control Trial (RCT) ... -0.015754      -0.064448   \n",
       "methods_ Simple field experiment (Quasi-Random ... -0.015754      -0.064448   \n",
       "methods_ Transaction data                          -0.015754      -0.064448   \n",
       "methods_ Willingness to pay experiment             -0.015754       0.098206   \n",
       "methods_ on-farm trials                            -0.015754      -0.064448   \n",
       "methods_ pseudo-Random Control Trial (RCT)         -0.015754      -0.064448   \n",
       "\n",
       "                                                    Qualitative  Quantitative  \\\n",
       "                                                      -0.089562     -0.160422   \n",
       "Mixed methods                                         -0.366378     -0.656250   \n",
       "Qualitative                                            1.000000     -0.366378   \n",
       "Quantitative                                          -0.366378      1.000000   \n",
       "methods_                                              -0.110782     -0.142787   \n",
       "methods_ Action research experiment (similar to...    -0.035980      0.098206   \n",
       "methods_ Bank transaction data                        -0.035980      0.098206   \n",
       "methods_ Data from government and DFS                 -0.035980      0.098206   \n",
       "methods_ Interview/focus group                         0.399242     -0.692125   \n",
       "methods_ Observation                                   0.324141     -0.142787   \n",
       "methods_ On-farm trials                               -0.035980      0.098206   \n",
       "methods_ Pre and post assessment                      -0.145969     -0.085492   \n",
       "methods_ Propensity score matching                    -0.051046      0.139325   \n",
       "methods_ Quasi-Random Control Trial (RCT)             -0.088662     -0.085492   \n",
       "methods_ Quiz                                         -0.035980      0.098206   \n",
       "methods_ Random Control Trial (RCT)                   -0.199966      0.232879   \n",
       "methods_ Self-reported survey                         -0.303161     -0.042497   \n",
       "methods_ Service data                                 -0.123299     -0.220851   \n",
       "methods_ Similar to Random Control Trial (RCT) ...    -0.035980      0.098206   \n",
       "methods_ Simple field experiment (Quasi-Random ...    -0.035980      0.098206   \n",
       "methods_ Transaction data                             -0.035980      0.098206   \n",
       "methods_ Willingness to pay experiment                -0.035980     -0.064448   \n",
       "methods_ on-farm trials                               -0.035980      0.098206   \n",
       "methods_ pseudo-Random Control Trial (RCT)            -0.035980      0.098206   \n",
       "\n",
       "                                                    methods_   \\\n",
       "                                                     0.808452   \n",
       "Mixed methods                                       -0.087142   \n",
       "Qualitative                                         -0.110782   \n",
       "Quantitative                                        -0.142787   \n",
       "methods_                                             1.000000   \n",
       "methods_ Action research experiment (similar to...  -0.019487   \n",
       "methods_ Bank transaction data                      -0.019487   \n",
       "methods_ Data from government and DFS               -0.019487   \n",
       "methods_ Interview/focus group                      -0.255981   \n",
       "methods_ Observation                                -0.060000   \n",
       "methods_ On-farm trials                             -0.019487   \n",
       "methods_ Pre and post assessment                    -0.079057   \n",
       "methods_ Propensity score matching                  -0.027647   \n",
       "methods_ Quasi-Random Control Trial (RCT)           -0.079057   \n",
       "methods_ Quiz                                       -0.019487   \n",
       "methods_ Random Control Trial (RCT)                 -0.108302   \n",
       "methods_ Self-reported survey                       -0.183110   \n",
       "methods_ Service data                               -0.066779   \n",
       "methods_ Similar to Random Control Trial (RCT) ...  -0.019487   \n",
       "methods_ Simple field experiment (Quasi-Random ...  -0.019487   \n",
       "methods_ Transaction data                           -0.019487   \n",
       "methods_ Willingness to pay experiment              -0.019487   \n",
       "methods_ on-farm trials                             -0.019487   \n",
       "methods_ pseudo-Random Control Trial (RCT)          -0.019487   \n",
       "\n",
       "                                                    methods_ Action research experiment (similar to Random Control Trial (RCT))  \\\n",
       "                                                                                            -0.015754                             \n",
       "Mixed methods                                                                               -0.064448                             \n",
       "Qualitative                                                                                 -0.035980                             \n",
       "Quantitative                                                                                 0.098206                             \n",
       "methods_                                                                                    -0.019487                             \n",
       "methods_ Action research experiment (similar to...                                           1.000000                             \n",
       "methods_ Bank transaction data                                                              -0.006329                             \n",
       "methods_ Data from government and DFS                                                       -0.006329                             \n",
       "methods_ Interview/focus group                                                              -0.083139                             \n",
       "methods_ Observation                                                                        -0.019487                             \n",
       "methods_ On-farm trials                                                                     -0.006329                             \n",
       "methods_ Pre and post assessment                                                            -0.025677                             \n",
       "methods_ Propensity score matching                                                          -0.008979                             \n",
       "methods_ Quasi-Random Control Trial (RCT)                                                   -0.025677                             \n",
       "methods_ Quiz                                                                               -0.006329                             \n",
       "methods_ Random Control Trial (RCT)                                                         -0.035175                             \n",
       "methods_ Self-reported survey                                                               -0.059471                             \n",
       "methods_ Service data                                                                       -0.021689                             \n",
       "methods_ Similar to Random Control Trial (RCT) ...                                          -0.006329                             \n",
       "methods_ Simple field experiment (Quasi-Random ...                                          -0.006329                             \n",
       "methods_ Transaction data                                                                   -0.006329                             \n",
       "methods_ Willingness to pay experiment                                                      -0.006329                             \n",
       "methods_ on-farm trials                                                                     -0.006329                             \n",
       "methods_ pseudo-Random Control Trial (RCT)                                                  -0.006329                             \n",
       "\n",
       "                                                    methods_ Bank transaction data  \\\n",
       "                                                                         -0.015754   \n",
       "Mixed methods                                                            -0.064448   \n",
       "Qualitative                                                              -0.035980   \n",
       "Quantitative                                                              0.098206   \n",
       "methods_                                                                 -0.019487   \n",
       "methods_ Action research experiment (similar to...                       -0.006329   \n",
       "methods_ Bank transaction data                                            1.000000   \n",
       "methods_ Data from government and DFS                                    -0.006329   \n",
       "methods_ Interview/focus group                                           -0.083139   \n",
       "methods_ Observation                                                     -0.019487   \n",
       "methods_ On-farm trials                                                  -0.006329   \n",
       "methods_ Pre and post assessment                                         -0.025677   \n",
       "methods_ Propensity score matching                                       -0.008979   \n",
       "methods_ Quasi-Random Control Trial (RCT)                                -0.025677   \n",
       "methods_ Quiz                                                            -0.006329   \n",
       "methods_ Random Control Trial (RCT)                                      -0.035175   \n",
       "methods_ Self-reported survey                                            -0.059471   \n",
       "methods_ Service data                                                    -0.021689   \n",
       "methods_ Similar to Random Control Trial (RCT) ...                       -0.006329   \n",
       "methods_ Simple field experiment (Quasi-Random ...                       -0.006329   \n",
       "methods_ Transaction data                                                -0.006329   \n",
       "methods_ Willingness to pay experiment                                   -0.006329   \n",
       "methods_ on-farm trials                                                  -0.006329   \n",
       "methods_ pseudo-Random Control Trial (RCT)                               -0.006329   \n",
       "\n",
       "                                                    methods_ Data from government and DFS  \\\n",
       "                                                                                -0.015754   \n",
       "Mixed methods                                                                   -0.064448   \n",
       "Qualitative                                                                     -0.035980   \n",
       "Quantitative                                                                     0.098206   \n",
       "methods_                                                                        -0.019487   \n",
       "methods_ Action research experiment (similar to...                              -0.006329   \n",
       "methods_ Bank transaction data                                                  -0.006329   \n",
       "methods_ Data from government and DFS                                            1.000000   \n",
       "methods_ Interview/focus group                                                  -0.083139   \n",
       "methods_ Observation                                                            -0.019487   \n",
       "methods_ On-farm trials                                                         -0.006329   \n",
       "methods_ Pre and post assessment                                                 0.246494   \n",
       "methods_ Propensity score matching                                              -0.008979   \n",
       "methods_ Quasi-Random Control Trial (RCT)                                       -0.025677   \n",
       "methods_ Quiz                                                                   -0.006329   \n",
       "methods_ Random Control Trial (RCT)                                             -0.035175   \n",
       "methods_ Self-reported survey                                                   -0.059471   \n",
       "methods_ Service data                                                           -0.021689   \n",
       "methods_ Similar to Random Control Trial (RCT) ...                              -0.006329   \n",
       "methods_ Simple field experiment (Quasi-Random ...                              -0.006329   \n",
       "methods_ Transaction data                                                       -0.006329   \n",
       "methods_ Willingness to pay experiment                                          -0.006329   \n",
       "methods_ on-farm trials                                                         -0.006329   \n",
       "methods_ pseudo-Random Control Trial (RCT)                                      -0.006329   \n",
       "\n",
       "                                                    methods_ Interview/focus group  \\\n",
       "                                                                         -0.206948   \n",
       "Mixed methods                                                             0.466274   \n",
       "Qualitative                                                               0.399242   \n",
       "Quantitative                                                             -0.692125   \n",
       "methods_                                                                 -0.255981   \n",
       "methods_ Action research experiment (similar to...                       -0.083139   \n",
       "methods_ Bank transaction data                                           -0.083139   \n",
       "methods_ Data from government and DFS                                    -0.083139   \n",
       "methods_ Interview/focus group                                            1.000000   \n",
       "methods_ Observation                                                      0.179906   \n",
       "methods_ On-farm trials                                                  -0.083139   \n",
       "methods_ Pre and post assessment                                          0.007315   \n",
       "methods_ Propensity score matching                                       -0.117950   \n",
       "methods_ Quasi-Random Control Trial (RCT)                                 0.050390   \n",
       "methods_ Quiz                                                            -0.083139   \n",
       "methods_ Random Control Trial (RCT)                                      -0.291835   \n",
       "methods_ Self-reported survey                                             0.058950   \n",
       "methods_ Service data                                                     0.211259   \n",
       "methods_ Similar to Random Control Trial (RCT) ...                       -0.083139   \n",
       "methods_ Simple field experiment (Quasi-Random ...                       -0.083139   \n",
       "methods_ Transaction data                                                -0.083139   \n",
       "methods_ Willingness to pay experiment                                    0.076127   \n",
       "methods_ on-farm trials                                                  -0.083139   \n",
       "methods_ pseudo-Random Control Trial (RCT)                               -0.083139   \n",
       "\n",
       "                                                    methods_ Observation  ...  \\\n",
       "                                                               -0.048507  ...   \n",
       "Mixed methods                                                  -0.087142  ...   \n",
       "Qualitative                                                     0.324141  ...   \n",
       "Quantitative                                                   -0.142787  ...   \n",
       "methods_                                                       -0.060000  ...   \n",
       "methods_ Action research experiment (similar to...             -0.019487  ...   \n",
       "methods_ Bank transaction data                                 -0.019487  ...   \n",
       "methods_ Data from government and DFS                          -0.019487  ...   \n",
       "methods_ Interview/focus group                                  0.179906  ...   \n",
       "methods_ Observation                                            1.000000  ...   \n",
       "methods_ On-farm trials                                        -0.019487  ...   \n",
       "methods_ Pre and post assessment                               -0.079057  ...   \n",
       "methods_ Propensity score matching                             -0.027647  ...   \n",
       "methods_ Quasi-Random Control Trial (RCT)                      -0.079057  ...   \n",
       "methods_ Quiz                                                  -0.019487  ...   \n",
       "methods_ Random Control Trial (RCT)                            -0.034712  ...   \n",
       "methods_ Self-reported survey                                  -0.069603  ...   \n",
       "methods_ Service data                                          -0.066779  ...   \n",
       "methods_ Similar to Random Control Trial (RCT) ...             -0.019487  ...   \n",
       "methods_ Simple field experiment (Quasi-Random ...             -0.019487  ...   \n",
       "methods_ Transaction data                                      -0.019487  ...   \n",
       "methods_ Willingness to pay experiment                         -0.019487  ...   \n",
       "methods_ on-farm trials                                        -0.019487  ...   \n",
       "methods_ pseudo-Random Control Trial (RCT)                     -0.019487  ...   \n",
       "\n",
       "                                                    methods_ Quiz  \\\n",
       "                                                        -0.015754   \n",
       "Mixed methods                                           -0.064448   \n",
       "Qualitative                                             -0.035980   \n",
       "Quantitative                                             0.098206   \n",
       "methods_                                                -0.019487   \n",
       "methods_ Action research experiment (similar to...      -0.006329   \n",
       "methods_ Bank transaction data                          -0.006329   \n",
       "methods_ Data from government and DFS                   -0.006329   \n",
       "methods_ Interview/focus group                          -0.083139   \n",
       "methods_ Observation                                    -0.019487   \n",
       "methods_ On-farm trials                                 -0.006329   \n",
       "methods_ Pre and post assessment                        -0.025677   \n",
       "methods_ Propensity score matching                      -0.008979   \n",
       "methods_ Quasi-Random Control Trial (RCT)               -0.025677   \n",
       "methods_ Quiz                                            1.000000   \n",
       "methods_ Random Control Trial (RCT)                     -0.035175   \n",
       "methods_ Self-reported survey                           -0.059471   \n",
       "methods_ Service data                                   -0.021689   \n",
       "methods_ Similar to Random Control Trial (RCT) ...      -0.006329   \n",
       "methods_ Simple field experiment (Quasi-Random ...      -0.006329   \n",
       "methods_ Transaction data                               -0.006329   \n",
       "methods_ Willingness to pay experiment                  -0.006329   \n",
       "methods_ on-farm trials                                 -0.006329   \n",
       "methods_ pseudo-Random Control Trial (RCT)              -0.006329   \n",
       "\n",
       "                                                    methods_ Random Control Trial (RCT)  \\\n",
       "                                                                              -0.087557   \n",
       "Mixed methods                                                                 -0.045264   \n",
       "Qualitative                                                                   -0.199966   \n",
       "Quantitative                                                                   0.232879   \n",
       "methods_                                                                      -0.108302   \n",
       "methods_ Action research experiment (similar to...                            -0.035175   \n",
       "methods_ Bank transaction data                                                -0.035175   \n",
       "methods_ Data from government and DFS                                         -0.035175   \n",
       "methods_ Interview/focus group                                                -0.291835   \n",
       "methods_ Observation                                                          -0.034712   \n",
       "methods_ On-farm trials                                                       -0.035175   \n",
       "methods_ Pre and post assessment                                              -0.084523   \n",
       "methods_ Propensity score matching                                            -0.049903   \n",
       "methods_ Quasi-Random Control Trial (RCT)                                     -0.142700   \n",
       "methods_ Quiz                                                                 -0.035175   \n",
       "methods_ Random Control Trial (RCT)                                            1.000000   \n",
       "methods_ Self-reported survey                                                 -0.330520   \n",
       "methods_ Service data                                                         -0.120539   \n",
       "methods_ Similar to Random Control Trial (RCT) ...                            -0.035175   \n",
       "methods_ Simple field experiment (Quasi-Random ...                            -0.035175   \n",
       "methods_ Transaction data                                                     -0.035175   \n",
       "methods_ Willingness to pay experiment                                        -0.035175   \n",
       "methods_ on-farm trials                                                       -0.035175   \n",
       "methods_ pseudo-Random Control Trial (RCT)                                    -0.035175   \n",
       "\n",
       "                                                    methods_ Self-reported survey  \\\n",
       "                                                                        -0.148036   \n",
       "Mixed methods                                                            0.332892   \n",
       "Qualitative                                                             -0.303161   \n",
       "Quantitative                                                            -0.042497   \n",
       "methods_                                                                -0.183110   \n",
       "methods_ Action research experiment (similar to...                      -0.059471   \n",
       "methods_ Bank transaction data                                          -0.059471   \n",
       "methods_ Data from government and DFS                                   -0.059471   \n",
       "methods_ Interview/focus group                                           0.058950   \n",
       "methods_ Observation                                                    -0.069603   \n",
       "methods_ On-farm trials                                                 -0.059471   \n",
       "methods_ Pre and post assessment                                         0.027936   \n",
       "methods_ Propensity score matching                                      -0.084373   \n",
       "methods_ Quasi-Random Control Trial (RCT)                               -0.016931   \n",
       "methods_ Quiz                                                           -0.059471   \n",
       "methods_ Random Control Trial (RCT)                                     -0.330520   \n",
       "methods_ Self-reported survey                                            1.000000   \n",
       "methods_ Service data                                                    0.106288   \n",
       "methods_ Similar to Random Control Trial (RCT) ...                      -0.059471   \n",
       "methods_ Simple field experiment (Quasi-Random ...                      -0.059471   \n",
       "methods_ Transaction data                                               -0.059471   \n",
       "methods_ Willingness to pay experiment                                  -0.059471   \n",
       "methods_ on-farm trials                                                 -0.059471   \n",
       "methods_ pseudo-Random Control Trial (RCT)                              -0.059471   \n",
       "\n",
       "                                                    methods_ Service data  \\\n",
       "                                                                -0.053988   \n",
       "Mixed methods                                                    0.336535   \n",
       "Qualitative                                                     -0.123299   \n",
       "Quantitative                                                    -0.220851   \n",
       "methods_                                                        -0.066779   \n",
       "methods_ Action research experiment (similar to...              -0.021689   \n",
       "methods_ Bank transaction data                                  -0.021689   \n",
       "methods_ Data from government and DFS                           -0.021689   \n",
       "methods_ Interview/focus group                                   0.211259   \n",
       "methods_ Observation                                            -0.066779   \n",
       "methods_ On-farm trials                                         -0.021689   \n",
       "methods_ Pre and post assessment                                -0.003200   \n",
       "methods_ Propensity score matching                              -0.030770   \n",
       "methods_ Quasi-Random Control Trial (RCT)                        0.420749   \n",
       "methods_ Quiz                                                   -0.021689   \n",
       "methods_ Random Control Trial (RCT)                             -0.120539   \n",
       "methods_ Self-reported survey                                    0.106288   \n",
       "methods_ Service data                                            1.000000   \n",
       "methods_ Similar to Random Control Trial (RCT) ...              -0.021689   \n",
       "methods_ Simple field experiment (Quasi-Random ...              -0.021689   \n",
       "methods_ Transaction data                                       -0.021689   \n",
       "methods_ Willingness to pay experiment                          -0.021689   \n",
       "methods_ on-farm trials                                         -0.021689   \n",
       "methods_ pseudo-Random Control Trial (RCT)                      -0.021689   \n",
       "\n",
       "                                                    methods_ Similar to Random Control Trial (RCT) (Quasi-experiment)  \\\n",
       "                                                                                            -0.015754                   \n",
       "Mixed methods                                                                               -0.064448                   \n",
       "Qualitative                                                                                 -0.035980                   \n",
       "Quantitative                                                                                 0.098206                   \n",
       "methods_                                                                                    -0.019487                   \n",
       "methods_ Action research experiment (similar to...                                          -0.006329                   \n",
       "methods_ Bank transaction data                                                              -0.006329                   \n",
       "methods_ Data from government and DFS                                                       -0.006329                   \n",
       "methods_ Interview/focus group                                                              -0.083139                   \n",
       "methods_ Observation                                                                        -0.019487                   \n",
       "methods_ On-farm trials                                                                     -0.006329                   \n",
       "methods_ Pre and post assessment                                                            -0.025677                   \n",
       "methods_ Propensity score matching                                                          -0.008979                   \n",
       "methods_ Quasi-Random Control Trial (RCT)                                                   -0.025677                   \n",
       "methods_ Quiz                                                                               -0.006329                   \n",
       "methods_ Random Control Trial (RCT)                                                         -0.035175                   \n",
       "methods_ Self-reported survey                                                               -0.059471                   \n",
       "methods_ Service data                                                                       -0.021689                   \n",
       "methods_ Similar to Random Control Trial (RCT) ...                                           1.000000                   \n",
       "methods_ Simple field experiment (Quasi-Random ...                                          -0.006329                   \n",
       "methods_ Transaction data                                                                   -0.006329                   \n",
       "methods_ Willingness to pay experiment                                                      -0.006329                   \n",
       "methods_ on-farm trials                                                                     -0.006329                   \n",
       "methods_ pseudo-Random Control Trial (RCT)                                                  -0.006329                   \n",
       "\n",
       "                                                    methods_ Simple field experiment (Quasi-Random Control Trial (RCT)?)  \\\n",
       "                                                                                            -0.015754                      \n",
       "Mixed methods                                                                               -0.064448                      \n",
       "Qualitative                                                                                 -0.035980                      \n",
       "Quantitative                                                                                 0.098206                      \n",
       "methods_                                                                                    -0.019487                      \n",
       "methods_ Action research experiment (similar to...                                          -0.006329                      \n",
       "methods_ Bank transaction data                                                              -0.006329                      \n",
       "methods_ Data from government and DFS                                                       -0.006329                      \n",
       "methods_ Interview/focus group                                                              -0.083139                      \n",
       "methods_ Observation                                                                        -0.019487                      \n",
       "methods_ On-farm trials                                                                     -0.006329                      \n",
       "methods_ Pre and post assessment                                                            -0.025677                      \n",
       "methods_ Propensity score matching                                                          -0.008979                      \n",
       "methods_ Quasi-Random Control Trial (RCT)                                                   -0.025677                      \n",
       "methods_ Quiz                                                                               -0.006329                      \n",
       "methods_ Random Control Trial (RCT)                                                         -0.035175                      \n",
       "methods_ Self-reported survey                                                               -0.059471                      \n",
       "methods_ Service data                                                                       -0.021689                      \n",
       "methods_ Similar to Random Control Trial (RCT) ...                                          -0.006329                      \n",
       "methods_ Simple field experiment (Quasi-Random ...                                           1.000000                      \n",
       "methods_ Transaction data                                                                   -0.006329                      \n",
       "methods_ Willingness to pay experiment                                                      -0.006329                      \n",
       "methods_ on-farm trials                                                                     -0.006329                      \n",
       "methods_ pseudo-Random Control Trial (RCT)                                                  -0.006329                      \n",
       "\n",
       "                                                    methods_ Transaction data  \\\n",
       "                                                                    -0.015754   \n",
       "Mixed methods                                                       -0.064448   \n",
       "Qualitative                                                         -0.035980   \n",
       "Quantitative                                                         0.098206   \n",
       "methods_                                                            -0.019487   \n",
       "methods_ Action research experiment (similar to...                  -0.006329   \n",
       "methods_ Bank transaction data                                      -0.006329   \n",
       "methods_ Data from government and DFS                               -0.006329   \n",
       "methods_ Interview/focus group                                      -0.083139   \n",
       "methods_ Observation                                                -0.019487   \n",
       "methods_ On-farm trials                                             -0.006329   \n",
       "methods_ Pre and post assessment                                    -0.025677   \n",
       "methods_ Propensity score matching                                  -0.008979   \n",
       "methods_ Quasi-Random Control Trial (RCT)                           -0.025677   \n",
       "methods_ Quiz                                                       -0.006329   \n",
       "methods_ Random Control Trial (RCT)                                 -0.035175   \n",
       "methods_ Self-reported survey                                       -0.059471   \n",
       "methods_ Service data                                               -0.021689   \n",
       "methods_ Similar to Random Control Trial (RCT) ...                  -0.006329   \n",
       "methods_ Simple field experiment (Quasi-Random ...                  -0.006329   \n",
       "methods_ Transaction data                                            1.000000   \n",
       "methods_ Willingness to pay experiment                              -0.006329   \n",
       "methods_ on-farm trials                                             -0.006329   \n",
       "methods_ pseudo-Random Control Trial (RCT)                          -0.006329   \n",
       "\n",
       "                                                    methods_ Willingness to pay experiment  \\\n",
       "                                                                                 -0.015754   \n",
       "Mixed methods                                                                     0.098206   \n",
       "Qualitative                                                                      -0.035980   \n",
       "Quantitative                                                                     -0.064448   \n",
       "methods_                                                                         -0.019487   \n",
       "methods_ Action research experiment (similar to...                               -0.006329   \n",
       "methods_ Bank transaction data                                                   -0.006329   \n",
       "methods_ Data from government and DFS                                            -0.006329   \n",
       "methods_ Interview/focus group                                                    0.076127   \n",
       "methods_ Observation                                                             -0.019487   \n",
       "methods_ On-farm trials                                                          -0.006329   \n",
       "methods_ Pre and post assessment                                                 -0.025677   \n",
       "methods_ Propensity score matching                                               -0.008979   \n",
       "methods_ Quasi-Random Control Trial (RCT)                                        -0.025677   \n",
       "methods_ Quiz                                                                    -0.006329   \n",
       "methods_ Random Control Trial (RCT)                                              -0.035175   \n",
       "methods_ Self-reported survey                                                    -0.059471   \n",
       "methods_ Service data                                                            -0.021689   \n",
       "methods_ Similar to Random Control Trial (RCT) ...                               -0.006329   \n",
       "methods_ Simple field experiment (Quasi-Random ...                               -0.006329   \n",
       "methods_ Transaction data                                                        -0.006329   \n",
       "methods_ Willingness to pay experiment                                            1.000000   \n",
       "methods_ on-farm trials                                                          -0.006329   \n",
       "methods_ pseudo-Random Control Trial (RCT)                                       -0.006329   \n",
       "\n",
       "                                                    methods_ on-farm trials  \\\n",
       "                                                                  -0.015754   \n",
       "Mixed methods                                                     -0.064448   \n",
       "Qualitative                                                       -0.035980   \n",
       "Quantitative                                                       0.098206   \n",
       "methods_                                                          -0.019487   \n",
       "methods_ Action research experiment (similar to...                -0.006329   \n",
       "methods_ Bank transaction data                                    -0.006329   \n",
       "methods_ Data from government and DFS                             -0.006329   \n",
       "methods_ Interview/focus group                                    -0.083139   \n",
       "methods_ Observation                                              -0.019487   \n",
       "methods_ On-farm trials                                           -0.006329   \n",
       "methods_ Pre and post assessment                                  -0.025677   \n",
       "methods_ Propensity score matching                                -0.008979   \n",
       "methods_ Quasi-Random Control Trial (RCT)                         -0.025677   \n",
       "methods_ Quiz                                                     -0.006329   \n",
       "methods_ Random Control Trial (RCT)                               -0.035175   \n",
       "methods_ Self-reported survey                                     -0.059471   \n",
       "methods_ Service data                                             -0.021689   \n",
       "methods_ Similar to Random Control Trial (RCT) ...                -0.006329   \n",
       "methods_ Simple field experiment (Quasi-Random ...                -0.006329   \n",
       "methods_ Transaction data                                         -0.006329   \n",
       "methods_ Willingness to pay experiment                            -0.006329   \n",
       "methods_ on-farm trials                                            1.000000   \n",
       "methods_ pseudo-Random Control Trial (RCT)                        -0.006329   \n",
       "\n",
       "                                                    methods_ pseudo-Random Control Trial (RCT)  \n",
       "                                                                                     -0.015754  \n",
       "Mixed methods                                                                        -0.064448  \n",
       "Qualitative                                                                          -0.035980  \n",
       "Quantitative                                                                          0.098206  \n",
       "methods_                                                                             -0.019487  \n",
       "methods_ Action research experiment (similar to...                                   -0.006329  \n",
       "methods_ Bank transaction data                                                       -0.006329  \n",
       "methods_ Data from government and DFS                                                -0.006329  \n",
       "methods_ Interview/focus group                                                       -0.083139  \n",
       "methods_ Observation                                                                 -0.019487  \n",
       "methods_ On-farm trials                                                              -0.006329  \n",
       "methods_ Pre and post assessment                                                     -0.025677  \n",
       "methods_ Propensity score matching                                                   -0.008979  \n",
       "methods_ Quasi-Random Control Trial (RCT)                                            -0.025677  \n",
       "methods_ Quiz                                                                        -0.006329  \n",
       "methods_ Random Control Trial (RCT)                                                  -0.035175  \n",
       "methods_ Self-reported survey                                                        -0.059471  \n",
       "methods_ Service data                                                                -0.021689  \n",
       "methods_ Similar to Random Control Trial (RCT) ...                                   -0.006329  \n",
       "methods_ Simple field experiment (Quasi-Random ...                                   -0.006329  \n",
       "methods_ Transaction data                                                            -0.006329  \n",
       "methods_ Willingness to pay experiment                                               -0.006329  \n",
       "methods_ on-farm trials                                                              -0.006329  \n",
       "methods_ pseudo-Random Control Trial (RCT)                                            1.000000  \n",
       "\n",
       "[24 rows x 24 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([pd.get_dummies(dfs_data[\"Study Design\"]), pd_counts], axis=1).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3 42  3]\n",
      " [ 5 16  1]\n",
      " [12 33 11]]\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Mixed methods       0.15      0.06      0.09        48\n",
      "  Qualitative       0.18      0.73      0.28        22\n",
      " Quantitative       0.73      0.20      0.31        56\n",
      "\n",
      "    micro avg       0.24      0.24      0.24       126\n",
      "    macro avg       0.35      0.33      0.23       126\n",
      " weighted avg       0.41      0.24      0.22       126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_true=dfs_data_true_labels, y_pred=dfs_data_predicted_labels))\n",
    "print(classification_report(y_true=dfs_data_true_labels, y_pred=dfs_data_predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read file ../tmp/study_type_data/final_study_type_test_data.xlsx: 0.16s\n",
      "Processed file ../tmp/study_type_data/final_study_type_test_data.xlsx: 0.02s\n",
      "INFO:tensorflow:Writing example 0 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] pea ##san ##ts and agriculture [SEP] imperial russia had an overwhelming ##ly pea ##san ##t population and its economy was largely agricultural . the russian empir ##e was not able to compete economically and mil ##ita ##ril ##y with the more  developed  states of north - western europe , north america and japan in the last few decades of its existence . over the preceding two or three centuries , however , russia  s auto ##cr ##atic state had been able successfully to exploit its pea ##san ##t population and agricultural economy to generate the resources , in particular tax revenues and military cons ##cripts , to consol ##idate and maintain its power at home and build a vast empir ##e that came to dominate eastern europe and northern asia . imperial russia  s pea ##san ##ts were , thus , at the bottom of an exploit ##ative social order . for much of the period , moreover , between a third and a half were the ser ##fs of noble land ##own ##ers in a system of bonded labour that emerged in the late sixteen ##th century and lasted until its abol ##ition in 186 ##1 . most of the rest of the pea ##san ##try lived on state lands and were subjected to slightly less one ##rous restrictions and demands . the sub ##ordinate and exploited status of all imperial russia  s pea ##san ##ts is one of the wider contexts in which their ways of life [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] pea ##san ##ts and agriculture [SEP] imperial russia had an overwhelming ##ly pea ##san ##t population and its economy was largely agricultural . the russian empir ##e was not able to compete economically and mil ##ita ##ril ##y with the more  developed  states of north - western europe , north america and japan in the last few decades of its existence . over the preceding two or three centuries , however , russia  s auto ##cr ##atic state had been able successfully to exploit its pea ##san ##t population and agricultural economy to generate the resources , in particular tax revenues and military cons ##cripts , to consol ##idate and maintain its power at home and build a vast empir ##e that came to dominate eastern europe and northern asia . imperial russia  s pea ##san ##ts were , thus , at the bottom of an exploit ##ative social order . for much of the period , moreover , between a third and a half were the ser ##fs of noble land ##own ##ers in a system of bonded labour that emerged in the late sixteen ##th century and lasted until its abol ##ition in 186 ##1 . most of the rest of the pea ##san ##try lived on state lands and were subjected to slightly less one ##rous restrictions and demands . the sub ##ordinate and exploited status of all imperial russia  s pea ##san ##ts is one of the wider contexts in which their ways of life [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 25038 17011 203 137 9879 103 27524 17094 883 130 22954 179 25038 17011 30108 1638 137 633 7425 241 5939 7675 205 111 14658 4153 30107 241 302 2357 147 17080 20003 137 3466 6351 4598 30126 190 111 475 1384 1815 5459 1898 131 4656 579 4539 3371 422 4656 7968 137 4838 121 111 2442 2149 8148 131 633 5074 205 573 111 10663 502 234 874 27232 422 694 422 17094 5459 112 5975 1304 757 1098 883 528 2357 5429 147 11091 633 25038 17011 30108 1638 137 7675 7425 147 3730 111 2965 422 121 1349 3764 22297 137 10945 401 20085 422 147 13181 23691 137 2603 633 1221 235 3417 137 2875 106 10946 4153 30107 198 10425 147 17893 10309 3371 137 7926 10705 205 27524 17094 5459 112 25038 17011 203 267 422 959 422 235 111 5473 131 130 11091 456 1748 993 205 168 1839 131 111 1275 422 2428 422 467 106 2765 137 106 3437 267 111 846 2373 131 28596 2882 1513 270 121 106 429 131 23152 10814 198 10036 121 111 4410 24158 266 8862 137 20376 3088 633 13721 319 121 18683 30130 205 755 131 111 2619 131 111 25038 17011 4272 18118 191 1098 24662 137 267 7178 147 4237 1279 482 9615 9129 137 8515 205 111 414 18147 137 13294 2726 131 355 27524 17094 5459 112 25038 17011 203 165 482 131 111 10147 8653 121 334 547 4568 131 1994 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 25038 17011 203 137 9879 103 27524 17094 883 130 22954 179 25038 17011 30108 1638 137 633 7425 241 5939 7675 205 111 14658 4153 30107 241 302 2357 147 17080 20003 137 3466 6351 4598 30126 190 111 475 1384 1815 5459 1898 131 4656 579 4539 3371 422 4656 7968 137 4838 121 111 2442 2149 8148 131 633 5074 205 573 111 10663 502 234 874 27232 422 694 422 17094 5459 112 5975 1304 757 1098 883 528 2357 5429 147 11091 633 25038 17011 30108 1638 137 7675 7425 147 3730 111 2965 422 121 1349 3764 22297 137 10945 401 20085 422 147 13181 23691 137 2603 633 1221 235 3417 137 2875 106 10946 4153 30107 198 10425 147 17893 10309 3371 137 7926 10705 205 27524 17094 5459 112 25038 17011 203 267 422 959 422 235 111 5473 131 130 11091 456 1748 993 205 168 1839 131 111 1275 422 2428 422 467 106 2765 137 106 3437 267 111 846 2373 131 28596 2882 1513 270 121 106 429 131 23152 10814 198 10036 121 111 4410 24158 266 8862 137 20376 3088 633 13721 319 121 18683 30130 205 755 131 111 2619 131 111 25038 17011 4272 18118 191 1098 24662 137 267 7178 147 4237 1279 482 9615 9129 137 8515 205 111 414 18147 137 13294 2726 131 355 27524 17094 5459 112 25038 17011 203 165 482 131 111 10147 8653 121 334 547 4568 131 1994 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] bean seed dissemination systems in mal ##awi : a strategy [SEP] farmers have shown considerable interest in purchasing seed of newly released dry bean ( phase ##olus vulgaris l . ) varieties in mal ##awi . however , this seed is not available on a large scale to farmers at present . the private seed sector in the country is not interested in bean seed production due to low profit margins . as a self - pollin ##ating crop , once farmers have obtained initial seed stock , it can be recycl ##ed for some time . a strategy has been developed to focus on alternative sustainable informal seed production and dissemination systems , which builds on lessons learn ##t from previous experiences within mal ##awi and other african countries . these include : ( i ) informal seed multiplication , using small ##holder farmers ; ( ii ) informal seed distribution channels , using gro ##cer ##y shop ##s , rural traders , extension agents , health clinics , and ng ##os ; and ( iii ) intens ##ified variety promotion through public ##ity , using poster ##s , leaf ##lets , bro ##ch ##ures and radio messages . these activities are carried out in close collaboration with farmers , ng ##os , extension agencies , village traders and various other institutions . the results from initial sales through informal distribution channels were encouraging and 80 % of the seed was sold [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] bean seed dissemination systems in mal ##awi : a strategy [SEP] farmers have shown considerable interest in purchasing seed of newly released dry bean ( phase ##olus vulgaris l . ) varieties in mal ##awi . however , this seed is not available on a large scale to farmers at present . the private seed sector in the country is not interested in bean seed production due to low profit margins . as a self - pollin ##ating crop , once farmers have obtained initial seed stock , it can be recycl ##ed for some time . a strategy has been developed to focus on alternative sustainable informal seed production and dissemination systems , which builds on lessons learn ##t from previous experiences within mal ##awi and other african countries . these include : ( i ) informal seed multiplication , using small ##holder farmers ; ( ii ) informal seed distribution channels , using gro ##cer ##y shop ##s , rural traders , extension agents , health clinics , and ng ##os ; and ( iii ) intens ##ified variety promotion through public ##ity , using poster ##s , leaf ##lets , bro ##ch ##ures and radio messages . these activities are carried out in close collaboration with farmers , ng ##os , extension agencies , village traders and various other institutions . the results from initial sales through informal distribution channels were encouraging and 80 % of the seed was sold [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 23022 3678 14265 1078 121 1774 18265 862 106 2691 103 12288 360 817 6132 1291 121 22085 3678 131 7737 7163 5965 23022 145 1481 24815 27039 152 205 546 13600 121 1774 18265 205 694 422 238 3678 165 302 1427 191 106 1135 2211 147 12288 235 709 205 111 5225 3678 5672 121 111 5192 165 302 7301 121 23022 3678 1865 1074 147 629 9047 14185 205 188 106 1968 579 26907 560 8501 422 3246 12288 360 1151 1700 3678 5426 422 256 300 195 13956 119 168 693 532 205 106 2691 434 528 1815 147 1790 191 3350 10886 13217 3678 1865 137 14265 1078 422 334 18619 191 15789 6714 30108 263 1061 6274 1017 1774 18265 137 494 7608 3198 205 407 2212 862 145 259 546 13217 3678 11957 422 487 952 13643 12288 1814 145 1264 546 13217 3678 1382 3994 422 487 419 736 30126 14260 30113 422 7046 29607 422 3840 3027 422 947 14862 422 137 3312 169 1814 137 145 2786 546 14733 1237 3835 11185 833 1771 208 422 487 18492 30113 422 5513 6660 422 2096 255 571 137 5203 5847 205 407 2687 220 3065 556 121 3156 7392 190 12288 422 3312 169 422 3840 11934 422 17719 29607 137 1711 494 7238 205 111 545 263 1700 10252 833 13217 1382 3994 267 14931 137 2833 1863 131 111 3678 241 17922 103 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 23022 3678 14265 1078 121 1774 18265 862 106 2691 103 12288 360 817 6132 1291 121 22085 3678 131 7737 7163 5965 23022 145 1481 24815 27039 152 205 546 13600 121 1774 18265 205 694 422 238 3678 165 302 1427 191 106 1135 2211 147 12288 235 709 205 111 5225 3678 5672 121 111 5192 165 302 7301 121 23022 3678 1865 1074 147 629 9047 14185 205 188 106 1968 579 26907 560 8501 422 3246 12288 360 1151 1700 3678 5426 422 256 300 195 13956 119 168 693 532 205 106 2691 434 528 1815 147 1790 191 3350 10886 13217 3678 1865 137 14265 1078 422 334 18619 191 15789 6714 30108 263 1061 6274 1017 1774 18265 137 494 7608 3198 205 407 2212 862 145 259 546 13217 3678 11957 422 487 952 13643 12288 1814 145 1264 546 13217 3678 1382 3994 422 487 419 736 30126 14260 30113 422 7046 29607 422 3840 3027 422 947 14862 422 137 3312 169 1814 137 145 2786 546 14733 1237 3835 11185 833 1771 208 422 487 18492 30113 422 5513 6660 422 2096 255 571 137 5203 5847 205 407 2687 220 3065 556 121 3156 7392 190 12288 422 3312 169 422 3840 11934 422 17719 29607 137 1711 494 7238 205 111 545 263 1700 10252 833 13217 1382 3994 267 14931 137 2833 1863 131 111 3678 241 17922 103 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] small ##holders and niche markets : lessons from the and ##es [SEP] research in bol ##iv ##ia , peru and ecu ##ador focused on the extent to which producers of two widely trade ##d crops , ban ##ana ##s and coffee , are able to take advantage of expanding global markets . in ecu ##ador , ban ##ana production is largely in the hands of independent producers while a hand ##ful of companies control the export trade . these companies have considerable influence on the prices paid to farmers . the majority of farmers are hardly able to cover their production costs . coffee producers are confron ##ted with a market where over - supply in recent years has led to a fall in coffee prices . the ban ##ana and coffee sectors demonstrate that small ##holder producers often find themselves dise ##mp ##ower ##ed and unable to benefit from conventional commodity markets . several non - governmental organisations ( ng ##os ) are working with ban ##ana and coffee producers to tap into niche markets , which include organic , fair trade and go ##ur ##met markets . these offer greater benefits to farmers , by paying them a higher price for their produce , but demand new skills and forms of organisation . research in bol ##iv ##ia , peru and ecu ##ador focused on the extent to which producers of two widely trade ##d crops , ban ##ana ##s and coffee , are able to take advantage of expanding global markets [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] small ##holders and niche markets : lessons from the and ##es [SEP] research in bol ##iv ##ia , peru and ecu ##ador focused on the extent to which producers of two widely trade ##d crops , ban ##ana ##s and coffee , are able to take advantage of expanding global markets . in ecu ##ador , ban ##ana production is largely in the hands of independent producers while a hand ##ful of companies control the export trade . these companies have considerable influence on the prices paid to farmers . the majority of farmers are hardly able to cover their production costs . coffee producers are confron ##ted with a market where over - supply in recent years has led to a fall in coffee prices . the ban ##ana and coffee sectors demonstrate that small ##holder producers often find themselves dise ##mp ##ower ##ed and unable to benefit from conventional commodity markets . several non - governmental organisations ( ng ##os ) are working with ban ##ana and coffee producers to tap into niche markets , which include organic , fair trade and go ##ur ##met markets . these offer greater benefits to farmers , by paying them a higher price for their produce , but demand new skills and forms of organisation . research in bol ##iv ##ia , peru and ecu ##ador focused on the extent to which producers of two widely trade ##d crops , ban ##ana ##s and coffee , are able to take advantage of expanding global markets [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 952 10019 137 17433 7838 862 15789 263 111 137 123 103 849 121 8308 296 426 422 26460 137 29519 17544 4786 191 111 3796 147 334 15793 131 502 4276 4931 30118 12760 422 6550 4846 30113 137 17677 422 220 2357 147 2636 3944 131 13147 2523 7838 205 121 29519 17544 422 6550 4846 1865 165 5939 121 111 12053 131 2229 15793 969 106 1500 1004 131 6724 602 111 11484 4931 205 407 6724 360 6132 2496 191 111 6759 10955 147 12288 205 111 4185 131 12288 220 16136 2357 147 4144 547 1865 3266 205 17677 15793 220 17666 248 190 106 2753 582 573 579 4494 121 2151 1320 434 4030 147 106 3913 121 17677 6759 205 111 6550 4846 137 17677 11329 3768 198 952 13643 15793 1992 1040 5675 1035 5803 7946 119 137 8576 147 4541 263 3744 19821 7838 205 1323 699 579 26342 16526 145 3312 169 546 220 3630 190 6550 4846 137 17677 15793 147 10366 690 17433 7838 422 334 2212 4756 422 5623 4931 137 796 166 1566 7838 205 407 5066 2110 4376 147 12288 422 214 24558 1445 106 1001 4048 168 547 3299 422 563 3880 758 5561 137 3444 131 13202 205 849 121 8308 296 426 422 26460 137 29519 17544 4786 191 111 3796 147 334 15793 131 502 4276 4931 30118 12760 422 6550 4846 30113 137 17677 422 220 2357 147 2636 3944 131 13147 2523 7838 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 952 10019 137 17433 7838 862 15789 263 111 137 123 103 849 121 8308 296 426 422 26460 137 29519 17544 4786 191 111 3796 147 334 15793 131 502 4276 4931 30118 12760 422 6550 4846 30113 137 17677 422 220 2357 147 2636 3944 131 13147 2523 7838 205 121 29519 17544 422 6550 4846 1865 165 5939 121 111 12053 131 2229 15793 969 106 1500 1004 131 6724 602 111 11484 4931 205 407 6724 360 6132 2496 191 111 6759 10955 147 12288 205 111 4185 131 12288 220 16136 2357 147 4144 547 1865 3266 205 17677 15793 220 17666 248 190 106 2753 582 573 579 4494 121 2151 1320 434 4030 147 106 3913 121 17677 6759 205 111 6550 4846 137 17677 11329 3768 198 952 13643 15793 1992 1040 5675 1035 5803 7946 119 137 8576 147 4541 263 3744 19821 7838 205 1323 699 579 26342 16526 145 3312 169 546 220 3630 190 6550 4846 137 17677 15793 147 10366 690 17433 7838 422 334 2212 4756 422 5623 4931 137 796 166 1566 7838 205 407 5066 2110 4376 147 12288 422 214 24558 1445 106 1001 4048 168 547 3299 422 563 3880 758 5561 137 3444 131 13202 205 849 121 8308 296 426 422 26460 137 29519 17544 4786 191 111 3796 147 334 15793 131 502 4276 4931 30118 12760 422 6550 4846 30113 137 17677 422 220 2357 147 2636 3944 131 13147 2523 7838 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] agro ##fore ##stry system combining p . jul ##if ##lora and buff ##el grass in the brazilian semi - arid region . preliminary results [SEP] in a trial at per ##nam ##bu ##co , pros ##op ##is jul ##if ##lora seedlings were planted in 1983 at 3 * 4 m spacing on 4 types of plot : ( 1 ) in an area free of buff ##el grass ( cen ##chr ##us cili ##aris ) and treated using normal cultural practices ; and ( 2 - 4 ) in an area planted with buff ##el grass in 1981 and either not prepared ( 2 ) or prepared by hoe ##ing 1 - or 2 - m diameter clear areas around the seedlings ( ( 3 ) and ( 4 ) respectively ) . plots were fertil ##ized with np ##k fertilizer ( 5 : 14 : 3 ) and treated with 100 g ald ##rin to prevent term ##ite attack . hoe ##ing and weed ##ing was done 20 days after planting and survival and growth measurements made 3 - monthly for 1 yr and at 19 and 30 month old . seedling survival increased with the amount of cultural practice and at 1 yr old was 100 % in ( 1 ) , 85 % in ( 4 ) , 20 % in ( 3 ) and only 5 % in ( 2 ) . growth measurements up to this age followed a similar pattern . at 30 month old , p . [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] agro ##fore ##stry system combining p . jul ##if ##lora and buff ##el grass in the brazilian semi - arid region . preliminary results [SEP] in a trial at per ##nam ##bu ##co , pros ##op ##is jul ##if ##lora seedlings were planted in 1983 at 3 * 4 m spacing on 4 types of plot : ( 1 ) in an area free of buff ##el grass ( cen ##chr ##us cili ##aris ) and treated using normal cultural practices ; and ( 2 - 4 ) in an area planted with buff ##el grass in 1981 and either not prepared ( 2 ) or prepared by hoe ##ing 1 - or 2 - m diameter clear areas around the seedlings ( ( 3 ) and ( 4 ) respectively ) . plots were fertil ##ized with np ##k fertilizer ( 5 : 14 : 3 ) and treated with 100 g ald ##rin to prevent term ##ite attack . hoe ##ing and weed ##ing was done 20 days after planting and survival and growth measurements made 3 - monthly for 1 yr and at 19 and 30 month old . seedling survival increased with the amount of cultural practice and at 1 yr old was 100 % in ( 1 ) , 85 % in ( 4 ) , 20 % in ( 3 ) and only 5 % in ( 2 ) . growth measurements up to this age followed a similar pattern . at 30 month old , p . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 22362 732 13573 429 6484 118 205 13182 193 26229 137 2966 154 10102 121 111 17675 6317 579 29111 1025 205 6538 545 103 121 106 3303 235 309 10085 8077 1173 422 12000 237 129 13182 193 26229 13912 267 27755 121 24509 235 239 1375 286 127 12150 191 286 1910 131 4216 862 145 158 546 121 130 1590 2159 131 2966 154 10102 145 16759 10616 153 28545 19371 546 137 2338 487 1346 6656 5423 1814 137 145 170 579 286 546 121 130 1590 27755 190 2966 154 10102 121 26934 137 1676 302 4092 145 170 546 234 4092 214 25360 140 158 579 234 170 579 127 4381 1716 2326 2715 111 13912 145 145 239 546 137 145 286 546 1222 546 205 6784 267 9585 645 190 5242 30135 22289 145 305 862 1128 862 239 546 137 2338 190 1287 159 15261 6478 147 3363 902 585 3689 205 25360 140 137 27264 140 241 2992 1012 1976 647 26225 137 2842 137 1503 2354 1827 239 579 11762 168 158 10580 137 235 371 137 1339 1978 4289 205 23557 2842 1175 190 111 2407 131 6656 2794 137 235 158 10580 4289 241 1287 1863 121 145 158 546 422 6885 1863 121 145 286 546 422 1012 1863 121 145 239 546 137 617 305 1863 121 145 170 546 205 1503 2354 692 147 238 1407 2594 106 868 1377 205 235 1339 1978 4289 422 118 205 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 22362 732 13573 429 6484 118 205 13182 193 26229 137 2966 154 10102 121 111 17675 6317 579 29111 1025 205 6538 545 103 121 106 3303 235 309 10085 8077 1173 422 12000 237 129 13182 193 26229 13912 267 27755 121 24509 235 239 1375 286 127 12150 191 286 1910 131 4216 862 145 158 546 121 130 1590 2159 131 2966 154 10102 145 16759 10616 153 28545 19371 546 137 2338 487 1346 6656 5423 1814 137 145 170 579 286 546 121 130 1590 27755 190 2966 154 10102 121 26934 137 1676 302 4092 145 170 546 234 4092 214 25360 140 158 579 234 170 579 127 4381 1716 2326 2715 111 13912 145 145 239 546 137 145 286 546 1222 546 205 6784 267 9585 645 190 5242 30135 22289 145 305 862 1128 862 239 546 137 2338 190 1287 159 15261 6478 147 3363 902 585 3689 205 25360 140 137 27264 140 241 2992 1012 1976 647 26225 137 2842 137 1503 2354 1827 239 579 11762 168 158 10580 137 235 371 137 1339 1978 4289 205 23557 2842 1175 190 111 2407 131 6656 2794 137 235 158 10580 4289 241 1287 1863 121 145 158 546 422 6885 1863 121 145 286 546 422 1012 1863 121 145 239 546 137 617 305 1863 121 145 170 546 205 1503 2354 692 147 238 1407 2594 106 868 1377 205 235 1339 1978 4289 422 118 205 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] evaluation of two injection scler ##osa ##nts in the treatment of symptomatic haemorrh ##oids in nigeria ##ns [SEP] objectives : the objective of this study was to compare the efficacy and safety of 5 % phenol in al ##mond oil with 50 % dext ##rose water as scler ##osa ##nts in the treatment of first - to third - degree haemorrh ##oids . patients and methods : this was a prospective , comparative study conducted on eighty patients with first - , second - and third - degree haemorrh ##oids who consent ##ed to treatment by injection scler ##otherapy with either 5 % phenol in al ##mond oil or 50 % dext ##rose water . they were randomised equally into two groups . results : a total of eighty patients including 58 males and 22 females whose age ranged from 19 to 61 years were included in the study . bleeding was the chief symptom which was present in 78 ( 97 . 5 % ) patients , and there was complete resolution of this in 92 . 3 % and 89 . 7 % in the 5 % phenol in al ##mond oil and 50 % dext ##rose water groups , respectively , at 6 months ( p = 0 . 90 ##5 ) . similar degrees of resolution were noted for anal protr ##usion ( 89 . 1 % and 85 . 3 % in the 5 % phenol in al ##mond oil and 50 % dext ##rose water group , respectively [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] evaluation of two injection scler ##osa ##nts in the treatment of symptomatic haemorrh ##oids in nigeria ##ns [SEP] objectives : the objective of this study was to compare the efficacy and safety of 5 % phenol in al ##mond oil with 50 % dext ##rose water as scler ##osa ##nts in the treatment of first - to third - degree haemorrh ##oids . patients and methods : this was a prospective , comparative study conducted on eighty patients with first - , second - and third - degree haemorrh ##oids who consent ##ed to treatment by injection scler ##otherapy with either 5 % phenol in al ##mond oil or 50 % dext ##rose water . they were randomised equally into two groups . results : a total of eighty patients including 58 males and 22 females whose age ranged from 19 to 61 years were included in the study . bleeding was the chief symptom which was present in 78 ( 97 . 5 % ) patients , and there was complete resolution of this in 92 . 3 % and 89 . 7 % in the 5 % phenol in al ##mond oil and 50 % dext ##rose water groups , respectively , at 6 months ( p = 0 . 90 ##5 ) . similar degrees of resolution were noted for anal protr ##usion ( 89 . 1 % and 85 . 3 % in the 5 % phenol in al ##mond oil and 50 % dext ##rose water group , respectively [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 2166 131 502 4209 9518 4591 26219 121 111 922 131 12104 21047 4692 121 17964 1290 103 6454 862 111 3201 131 238 527 241 147 3745 111 4684 137 4104 131 305 1863 15302 121 186 14521 4981 190 1539 1863 28814 5718 1506 188 9518 4591 26219 121 111 922 131 705 579 147 2765 579 2460 21047 4692 205 568 137 1045 862 238 241 106 6880 422 6935 527 2728 191 27075 568 190 705 579 422 971 579 137 2765 579 2460 21047 4692 975 6670 119 147 922 214 4209 9518 3714 190 1676 305 1863 15302 121 186 14521 4981 234 1539 1863 28814 5718 1506 205 698 267 14136 6918 690 502 1302 205 545 862 106 1114 131 27075 568 1471 4878 5165 137 1931 5003 3489 1407 7650 263 371 147 7610 1320 267 1936 121 111 527 205 8401 241 111 16494 8316 334 241 709 121 7898 145 8276 205 305 1863 546 568 422 137 461 241 2467 3421 131 238 121 8698 205 239 1863 137 8765 205 450 1863 121 111 305 1863 15302 121 186 14521 4981 137 1539 1863 28814 5718 1506 1302 422 1222 422 235 370 2499 145 118 275 244 205 3289 30139 546 205 868 5634 131 3421 267 3742 168 438 16830 851 145 8765 205 158 1863 137 6885 205 239 1863 121 111 305 1863 15302 121 186 14521 4981 137 1539 1863 28814 5718 1506 583 422 1222 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 2166 131 502 4209 9518 4591 26219 121 111 922 131 12104 21047 4692 121 17964 1290 103 6454 862 111 3201 131 238 527 241 147 3745 111 4684 137 4104 131 305 1863 15302 121 186 14521 4981 190 1539 1863 28814 5718 1506 188 9518 4591 26219 121 111 922 131 705 579 147 2765 579 2460 21047 4692 205 568 137 1045 862 238 241 106 6880 422 6935 527 2728 191 27075 568 190 705 579 422 971 579 137 2765 579 2460 21047 4692 975 6670 119 147 922 214 4209 9518 3714 190 1676 305 1863 15302 121 186 14521 4981 234 1539 1863 28814 5718 1506 205 698 267 14136 6918 690 502 1302 205 545 862 106 1114 131 27075 568 1471 4878 5165 137 1931 5003 3489 1407 7650 263 371 147 7610 1320 267 1936 121 111 527 205 8401 241 111 16494 8316 334 241 709 121 7898 145 8276 205 305 1863 546 568 422 137 461 241 2467 3421 131 238 121 8698 205 239 1863 137 8765 205 450 1863 121 111 305 1863 15302 121 186 14521 4981 137 1539 1863 28814 5718 1506 1302 422 1222 422 235 370 2499 145 118 275 244 205 3289 30139 546 205 868 5634 131 3421 267 3742 168 438 16830 851 145 8765 205 158 1863 137 6885 205 239 1863 121 111 305 1863 15302 121 186 14521 4981 137 1539 1863 28814 5718 1506 583 422 1222 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used for model gpu 0\n",
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from study_type_multi\\model.ckpt-2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from study_type_multi\\model.ckpt-2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "test_plus_stefan = excel_reader.ExcelReader().read_df_from_excel(\"../tmp/study_type_data/final_study_type_test_data.xlsx\")\n",
    "result = _study_type_labeler.predict_for_df(test_plus_stefan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read file ../tmp/study_type_data/final_study_type_test_data.xlsx: 0.10s\n",
      "Processed file ../tmp/study_type_data/final_study_type_test_data.xlsx: 0.02s\n",
      "INFO:tensorflow:Writing example 0 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] pea ##san ##ts and agriculture [SEP] imperial russia had an overwhelming ##ly pea ##san ##t population and its economy was largely agricultural . the russian empir ##e was not able to compete economically and mil ##ita ##ril ##y with the more  developed  states of north - western europe , north america and japan in the last few decades of its existence . over the preceding two or three centuries , however , russia  s auto ##cr ##atic state had been able successfully to exploit its pea ##san ##t population and agricultural economy to generate the resources , in particular tax revenues and military cons ##cripts , to consol ##idate and maintain its power at home and build a vast empir ##e that came to dominate eastern europe and northern asia . imperial russia  s pea ##san ##ts were , thus , at the bottom of an exploit ##ative social order . for much of the period , moreover , between a third and a half were the ser ##fs of noble land ##own ##ers in a system of bonded labour that emerged in the late sixteen ##th century and lasted until its abol ##ition in 186 ##1 . most of the rest of the pea ##san ##try lived on state lands and were subjected to slightly less one ##rous restrictions and demands . the sub ##ordinate and exploited status of all imperial russia  s pea ##san ##ts is one of the wider contexts in which their ways of life [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] pea ##san ##ts and agriculture [SEP] imperial russia had an overwhelming ##ly pea ##san ##t population and its economy was largely agricultural . the russian empir ##e was not able to compete economically and mil ##ita ##ril ##y with the more  developed  states of north - western europe , north america and japan in the last few decades of its existence . over the preceding two or three centuries , however , russia  s auto ##cr ##atic state had been able successfully to exploit its pea ##san ##t population and agricultural economy to generate the resources , in particular tax revenues and military cons ##cripts , to consol ##idate and maintain its power at home and build a vast empir ##e that came to dominate eastern europe and northern asia . imperial russia  s pea ##san ##ts were , thus , at the bottom of an exploit ##ative social order . for much of the period , moreover , between a third and a half were the ser ##fs of noble land ##own ##ers in a system of bonded labour that emerged in the late sixteen ##th century and lasted until its abol ##ition in 186 ##1 . most of the rest of the pea ##san ##try lived on state lands and were subjected to slightly less one ##rous restrictions and demands . the sub ##ordinate and exploited status of all imperial russia  s pea ##san ##ts is one of the wider contexts in which their ways of life [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 25038 17011 203 137 9879 103 27524 17094 883 130 22954 179 25038 17011 30108 1638 137 633 7425 241 5939 7675 205 111 14658 4153 30107 241 302 2357 147 17080 20003 137 3466 6351 4598 30126 190 111 475 1384 1815 5459 1898 131 4656 579 4539 3371 422 4656 7968 137 4838 121 111 2442 2149 8148 131 633 5074 205 573 111 10663 502 234 874 27232 422 694 422 17094 5459 112 5975 1304 757 1098 883 528 2357 5429 147 11091 633 25038 17011 30108 1638 137 7675 7425 147 3730 111 2965 422 121 1349 3764 22297 137 10945 401 20085 422 147 13181 23691 137 2603 633 1221 235 3417 137 2875 106 10946 4153 30107 198 10425 147 17893 10309 3371 137 7926 10705 205 27524 17094 5459 112 25038 17011 203 267 422 959 422 235 111 5473 131 130 11091 456 1748 993 205 168 1839 131 111 1275 422 2428 422 467 106 2765 137 106 3437 267 111 846 2373 131 28596 2882 1513 270 121 106 429 131 23152 10814 198 10036 121 111 4410 24158 266 8862 137 20376 3088 633 13721 319 121 18683 30130 205 755 131 111 2619 131 111 25038 17011 4272 18118 191 1098 24662 137 267 7178 147 4237 1279 482 9615 9129 137 8515 205 111 414 18147 137 13294 2726 131 355 27524 17094 5459 112 25038 17011 203 165 482 131 111 10147 8653 121 334 547 4568 131 1994 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 25038 17011 203 137 9879 103 27524 17094 883 130 22954 179 25038 17011 30108 1638 137 633 7425 241 5939 7675 205 111 14658 4153 30107 241 302 2357 147 17080 20003 137 3466 6351 4598 30126 190 111 475 1384 1815 5459 1898 131 4656 579 4539 3371 422 4656 7968 137 4838 121 111 2442 2149 8148 131 633 5074 205 573 111 10663 502 234 874 27232 422 694 422 17094 5459 112 5975 1304 757 1098 883 528 2357 5429 147 11091 633 25038 17011 30108 1638 137 7675 7425 147 3730 111 2965 422 121 1349 3764 22297 137 10945 401 20085 422 147 13181 23691 137 2603 633 1221 235 3417 137 2875 106 10946 4153 30107 198 10425 147 17893 10309 3371 137 7926 10705 205 27524 17094 5459 112 25038 17011 203 267 422 959 422 235 111 5473 131 130 11091 456 1748 993 205 168 1839 131 111 1275 422 2428 422 467 106 2765 137 106 3437 267 111 846 2373 131 28596 2882 1513 270 121 106 429 131 23152 10814 198 10036 121 111 4410 24158 266 8862 137 20376 3088 633 13721 319 121 18683 30130 205 755 131 111 2619 131 111 25038 17011 4272 18118 191 1098 24662 137 267 7178 147 4237 1279 482 9615 9129 137 8515 205 111 414 18147 137 13294 2726 131 355 27524 17094 5459 112 25038 17011 203 165 482 131 111 10147 8653 121 334 547 4568 131 1994 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] bean seed dissemination systems in mal ##awi : a strategy [SEP] farmers have shown considerable interest in purchasing seed of newly released dry bean ( phase ##olus vulgaris l . ) varieties in mal ##awi . however , this seed is not available on a large scale to farmers at present . the private seed sector in the country is not interested in bean seed production due to low profit margins . as a self - pollin ##ating crop , once farmers have obtained initial seed stock , it can be recycl ##ed for some time . a strategy has been developed to focus on alternative sustainable informal seed production and dissemination systems , which builds on lessons learn ##t from previous experiences within mal ##awi and other african countries . these include : ( i ) informal seed multiplication , using small ##holder farmers ; ( ii ) informal seed distribution channels , using gro ##cer ##y shop ##s , rural traders , extension agents , health clinics , and ng ##os ; and ( iii ) intens ##ified variety promotion through public ##ity , using poster ##s , leaf ##lets , bro ##ch ##ures and radio messages . these activities are carried out in close collaboration with farmers , ng ##os , extension agencies , village traders and various other institutions . the results from initial sales through informal distribution channels were encouraging and 80 % of the seed was sold [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] bean seed dissemination systems in mal ##awi : a strategy [SEP] farmers have shown considerable interest in purchasing seed of newly released dry bean ( phase ##olus vulgaris l . ) varieties in mal ##awi . however , this seed is not available on a large scale to farmers at present . the private seed sector in the country is not interested in bean seed production due to low profit margins . as a self - pollin ##ating crop , once farmers have obtained initial seed stock , it can be recycl ##ed for some time . a strategy has been developed to focus on alternative sustainable informal seed production and dissemination systems , which builds on lessons learn ##t from previous experiences within mal ##awi and other african countries . these include : ( i ) informal seed multiplication , using small ##holder farmers ; ( ii ) informal seed distribution channels , using gro ##cer ##y shop ##s , rural traders , extension agents , health clinics , and ng ##os ; and ( iii ) intens ##ified variety promotion through public ##ity , using poster ##s , leaf ##lets , bro ##ch ##ures and radio messages . these activities are carried out in close collaboration with farmers , ng ##os , extension agencies , village traders and various other institutions . the results from initial sales through informal distribution channels were encouraging and 80 % of the seed was sold [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 23022 3678 14265 1078 121 1774 18265 862 106 2691 103 12288 360 817 6132 1291 121 22085 3678 131 7737 7163 5965 23022 145 1481 24815 27039 152 205 546 13600 121 1774 18265 205 694 422 238 3678 165 302 1427 191 106 1135 2211 147 12288 235 709 205 111 5225 3678 5672 121 111 5192 165 302 7301 121 23022 3678 1865 1074 147 629 9047 14185 205 188 106 1968 579 26907 560 8501 422 3246 12288 360 1151 1700 3678 5426 422 256 300 195 13956 119 168 693 532 205 106 2691 434 528 1815 147 1790 191 3350 10886 13217 3678 1865 137 14265 1078 422 334 18619 191 15789 6714 30108 263 1061 6274 1017 1774 18265 137 494 7608 3198 205 407 2212 862 145 259 546 13217 3678 11957 422 487 952 13643 12288 1814 145 1264 546 13217 3678 1382 3994 422 487 419 736 30126 14260 30113 422 7046 29607 422 3840 3027 422 947 14862 422 137 3312 169 1814 137 145 2786 546 14733 1237 3835 11185 833 1771 208 422 487 18492 30113 422 5513 6660 422 2096 255 571 137 5203 5847 205 407 2687 220 3065 556 121 3156 7392 190 12288 422 3312 169 422 3840 11934 422 17719 29607 137 1711 494 7238 205 111 545 263 1700 10252 833 13217 1382 3994 267 14931 137 2833 1863 131 111 3678 241 17922 103 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 23022 3678 14265 1078 121 1774 18265 862 106 2691 103 12288 360 817 6132 1291 121 22085 3678 131 7737 7163 5965 23022 145 1481 24815 27039 152 205 546 13600 121 1774 18265 205 694 422 238 3678 165 302 1427 191 106 1135 2211 147 12288 235 709 205 111 5225 3678 5672 121 111 5192 165 302 7301 121 23022 3678 1865 1074 147 629 9047 14185 205 188 106 1968 579 26907 560 8501 422 3246 12288 360 1151 1700 3678 5426 422 256 300 195 13956 119 168 693 532 205 106 2691 434 528 1815 147 1790 191 3350 10886 13217 3678 1865 137 14265 1078 422 334 18619 191 15789 6714 30108 263 1061 6274 1017 1774 18265 137 494 7608 3198 205 407 2212 862 145 259 546 13217 3678 11957 422 487 952 13643 12288 1814 145 1264 546 13217 3678 1382 3994 422 487 419 736 30126 14260 30113 422 7046 29607 422 3840 3027 422 947 14862 422 137 3312 169 1814 137 145 2786 546 14733 1237 3835 11185 833 1771 208 422 487 18492 30113 422 5513 6660 422 2096 255 571 137 5203 5847 205 407 2687 220 3065 556 121 3156 7392 190 12288 422 3312 169 422 3840 11934 422 17719 29607 137 1711 494 7238 205 111 545 263 1700 10252 833 13217 1382 3994 267 14931 137 2833 1863 131 111 3678 241 17922 103 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] small ##holders and niche markets : lessons from the and ##es [SEP] research in bol ##iv ##ia , peru and ecu ##ador focused on the extent to which producers of two widely trade ##d crops , ban ##ana ##s and coffee , are able to take advantage of expanding global markets . in ecu ##ador , ban ##ana production is largely in the hands of independent producers while a hand ##ful of companies control the export trade . these companies have considerable influence on the prices paid to farmers . the majority of farmers are hardly able to cover their production costs . coffee producers are confron ##ted with a market where over - supply in recent years has led to a fall in coffee prices . the ban ##ana and coffee sectors demonstrate that small ##holder producers often find themselves dise ##mp ##ower ##ed and unable to benefit from conventional commodity markets . several non - governmental organisations ( ng ##os ) are working with ban ##ana and coffee producers to tap into niche markets , which include organic , fair trade and go ##ur ##met markets . these offer greater benefits to farmers , by paying them a higher price for their produce , but demand new skills and forms of organisation . research in bol ##iv ##ia , peru and ecu ##ador focused on the extent to which producers of two widely trade ##d crops , ban ##ana ##s and coffee , are able to take advantage of expanding global markets [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] small ##holders and niche markets : lessons from the and ##es [SEP] research in bol ##iv ##ia , peru and ecu ##ador focused on the extent to which producers of two widely trade ##d crops , ban ##ana ##s and coffee , are able to take advantage of expanding global markets . in ecu ##ador , ban ##ana production is largely in the hands of independent producers while a hand ##ful of companies control the export trade . these companies have considerable influence on the prices paid to farmers . the majority of farmers are hardly able to cover their production costs . coffee producers are confron ##ted with a market where over - supply in recent years has led to a fall in coffee prices . the ban ##ana and coffee sectors demonstrate that small ##holder producers often find themselves dise ##mp ##ower ##ed and unable to benefit from conventional commodity markets . several non - governmental organisations ( ng ##os ) are working with ban ##ana and coffee producers to tap into niche markets , which include organic , fair trade and go ##ur ##met markets . these offer greater benefits to farmers , by paying them a higher price for their produce , but demand new skills and forms of organisation . research in bol ##iv ##ia , peru and ecu ##ador focused on the extent to which producers of two widely trade ##d crops , ban ##ana ##s and coffee , are able to take advantage of expanding global markets [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 952 10019 137 17433 7838 862 15789 263 111 137 123 103 849 121 8308 296 426 422 26460 137 29519 17544 4786 191 111 3796 147 334 15793 131 502 4276 4931 30118 12760 422 6550 4846 30113 137 17677 422 220 2357 147 2636 3944 131 13147 2523 7838 205 121 29519 17544 422 6550 4846 1865 165 5939 121 111 12053 131 2229 15793 969 106 1500 1004 131 6724 602 111 11484 4931 205 407 6724 360 6132 2496 191 111 6759 10955 147 12288 205 111 4185 131 12288 220 16136 2357 147 4144 547 1865 3266 205 17677 15793 220 17666 248 190 106 2753 582 573 579 4494 121 2151 1320 434 4030 147 106 3913 121 17677 6759 205 111 6550 4846 137 17677 11329 3768 198 952 13643 15793 1992 1040 5675 1035 5803 7946 119 137 8576 147 4541 263 3744 19821 7838 205 1323 699 579 26342 16526 145 3312 169 546 220 3630 190 6550 4846 137 17677 15793 147 10366 690 17433 7838 422 334 2212 4756 422 5623 4931 137 796 166 1566 7838 205 407 5066 2110 4376 147 12288 422 214 24558 1445 106 1001 4048 168 547 3299 422 563 3880 758 5561 137 3444 131 13202 205 849 121 8308 296 426 422 26460 137 29519 17544 4786 191 111 3796 147 334 15793 131 502 4276 4931 30118 12760 422 6550 4846 30113 137 17677 422 220 2357 147 2636 3944 131 13147 2523 7838 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 952 10019 137 17433 7838 862 15789 263 111 137 123 103 849 121 8308 296 426 422 26460 137 29519 17544 4786 191 111 3796 147 334 15793 131 502 4276 4931 30118 12760 422 6550 4846 30113 137 17677 422 220 2357 147 2636 3944 131 13147 2523 7838 205 121 29519 17544 422 6550 4846 1865 165 5939 121 111 12053 131 2229 15793 969 106 1500 1004 131 6724 602 111 11484 4931 205 407 6724 360 6132 2496 191 111 6759 10955 147 12288 205 111 4185 131 12288 220 16136 2357 147 4144 547 1865 3266 205 17677 15793 220 17666 248 190 106 2753 582 573 579 4494 121 2151 1320 434 4030 147 106 3913 121 17677 6759 205 111 6550 4846 137 17677 11329 3768 198 952 13643 15793 1992 1040 5675 1035 5803 7946 119 137 8576 147 4541 263 3744 19821 7838 205 1323 699 579 26342 16526 145 3312 169 546 220 3630 190 6550 4846 137 17677 15793 147 10366 690 17433 7838 422 334 2212 4756 422 5623 4931 137 796 166 1566 7838 205 407 5066 2110 4376 147 12288 422 214 24558 1445 106 1001 4048 168 547 3299 422 563 3880 758 5561 137 3444 131 13202 205 849 121 8308 296 426 422 26460 137 29519 17544 4786 191 111 3796 147 334 15793 131 502 4276 4931 30118 12760 422 6550 4846 30113 137 17677 422 220 2357 147 2636 3944 131 13147 2523 7838 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] agro ##fore ##stry system combining p . jul ##if ##lora and buff ##el grass in the brazilian semi - arid region . preliminary results [SEP] in a trial at per ##nam ##bu ##co , pros ##op ##is jul ##if ##lora seedlings were planted in 1983 at 3 * 4 m spacing on 4 types of plot : ( 1 ) in an area free of buff ##el grass ( cen ##chr ##us cili ##aris ) and treated using normal cultural practices ; and ( 2 - 4 ) in an area planted with buff ##el grass in 1981 and either not prepared ( 2 ) or prepared by hoe ##ing 1 - or 2 - m diameter clear areas around the seedlings ( ( 3 ) and ( 4 ) respectively ) . plots were fertil ##ized with np ##k fertilizer ( 5 : 14 : 3 ) and treated with 100 g ald ##rin to prevent term ##ite attack . hoe ##ing and weed ##ing was done 20 days after planting and survival and growth measurements made 3 - monthly for 1 yr and at 19 and 30 month old . seedling survival increased with the amount of cultural practice and at 1 yr old was 100 % in ( 1 ) , 85 % in ( 4 ) , 20 % in ( 3 ) and only 5 % in ( 2 ) . growth measurements up to this age followed a similar pattern . at 30 month old , p . [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] agro ##fore ##stry system combining p . jul ##if ##lora and buff ##el grass in the brazilian semi - arid region . preliminary results [SEP] in a trial at per ##nam ##bu ##co , pros ##op ##is jul ##if ##lora seedlings were planted in 1983 at 3 * 4 m spacing on 4 types of plot : ( 1 ) in an area free of buff ##el grass ( cen ##chr ##us cili ##aris ) and treated using normal cultural practices ; and ( 2 - 4 ) in an area planted with buff ##el grass in 1981 and either not prepared ( 2 ) or prepared by hoe ##ing 1 - or 2 - m diameter clear areas around the seedlings ( ( 3 ) and ( 4 ) respectively ) . plots were fertil ##ized with np ##k fertilizer ( 5 : 14 : 3 ) and treated with 100 g ald ##rin to prevent term ##ite attack . hoe ##ing and weed ##ing was done 20 days after planting and survival and growth measurements made 3 - monthly for 1 yr and at 19 and 30 month old . seedling survival increased with the amount of cultural practice and at 1 yr old was 100 % in ( 1 ) , 85 % in ( 4 ) , 20 % in ( 3 ) and only 5 % in ( 2 ) . growth measurements up to this age followed a similar pattern . at 30 month old , p . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 22362 732 13573 429 6484 118 205 13182 193 26229 137 2966 154 10102 121 111 17675 6317 579 29111 1025 205 6538 545 103 121 106 3303 235 309 10085 8077 1173 422 12000 237 129 13182 193 26229 13912 267 27755 121 24509 235 239 1375 286 127 12150 191 286 1910 131 4216 862 145 158 546 121 130 1590 2159 131 2966 154 10102 145 16759 10616 153 28545 19371 546 137 2338 487 1346 6656 5423 1814 137 145 170 579 286 546 121 130 1590 27755 190 2966 154 10102 121 26934 137 1676 302 4092 145 170 546 234 4092 214 25360 140 158 579 234 170 579 127 4381 1716 2326 2715 111 13912 145 145 239 546 137 145 286 546 1222 546 205 6784 267 9585 645 190 5242 30135 22289 145 305 862 1128 862 239 546 137 2338 190 1287 159 15261 6478 147 3363 902 585 3689 205 25360 140 137 27264 140 241 2992 1012 1976 647 26225 137 2842 137 1503 2354 1827 239 579 11762 168 158 10580 137 235 371 137 1339 1978 4289 205 23557 2842 1175 190 111 2407 131 6656 2794 137 235 158 10580 4289 241 1287 1863 121 145 158 546 422 6885 1863 121 145 286 546 422 1012 1863 121 145 239 546 137 617 305 1863 121 145 170 546 205 1503 2354 692 147 238 1407 2594 106 868 1377 205 235 1339 1978 4289 422 118 205 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 22362 732 13573 429 6484 118 205 13182 193 26229 137 2966 154 10102 121 111 17675 6317 579 29111 1025 205 6538 545 103 121 106 3303 235 309 10085 8077 1173 422 12000 237 129 13182 193 26229 13912 267 27755 121 24509 235 239 1375 286 127 12150 191 286 1910 131 4216 862 145 158 546 121 130 1590 2159 131 2966 154 10102 145 16759 10616 153 28545 19371 546 137 2338 487 1346 6656 5423 1814 137 145 170 579 286 546 121 130 1590 27755 190 2966 154 10102 121 26934 137 1676 302 4092 145 170 546 234 4092 214 25360 140 158 579 234 170 579 127 4381 1716 2326 2715 111 13912 145 145 239 546 137 145 286 546 1222 546 205 6784 267 9585 645 190 5242 30135 22289 145 305 862 1128 862 239 546 137 2338 190 1287 159 15261 6478 147 3363 902 585 3689 205 25360 140 137 27264 140 241 2992 1012 1976 647 26225 137 2842 137 1503 2354 1827 239 579 11762 168 158 10580 137 235 371 137 1339 1978 4289 205 23557 2842 1175 190 111 2407 131 6656 2794 137 235 158 10580 4289 241 1287 1863 121 145 158 546 422 6885 1863 121 145 286 546 422 1012 1863 121 145 239 546 137 617 305 1863 121 145 170 546 205 1503 2354 692 147 238 1407 2594 106 868 1377 205 235 1339 1978 4289 422 118 205 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] evaluation of two injection scler ##osa ##nts in the treatment of symptomatic haemorrh ##oids in nigeria ##ns [SEP] objectives : the objective of this study was to compare the efficacy and safety of 5 % phenol in al ##mond oil with 50 % dext ##rose water as scler ##osa ##nts in the treatment of first - to third - degree haemorrh ##oids . patients and methods : this was a prospective , comparative study conducted on eighty patients with first - , second - and third - degree haemorrh ##oids who consent ##ed to treatment by injection scler ##otherapy with either 5 % phenol in al ##mond oil or 50 % dext ##rose water . they were randomised equally into two groups . results : a total of eighty patients including 58 males and 22 females whose age ranged from 19 to 61 years were included in the study . bleeding was the chief symptom which was present in 78 ( 97 . 5 % ) patients , and there was complete resolution of this in 92 . 3 % and 89 . 7 % in the 5 % phenol in al ##mond oil and 50 % dext ##rose water groups , respectively , at 6 months ( p = 0 . 90 ##5 ) . similar degrees of resolution were noted for anal protr ##usion ( 89 . 1 % and 85 . 3 % in the 5 % phenol in al ##mond oil and 50 % dext ##rose water group , respectively [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] evaluation of two injection scler ##osa ##nts in the treatment of symptomatic haemorrh ##oids in nigeria ##ns [SEP] objectives : the objective of this study was to compare the efficacy and safety of 5 % phenol in al ##mond oil with 50 % dext ##rose water as scler ##osa ##nts in the treatment of first - to third - degree haemorrh ##oids . patients and methods : this was a prospective , comparative study conducted on eighty patients with first - , second - and third - degree haemorrh ##oids who consent ##ed to treatment by injection scler ##otherapy with either 5 % phenol in al ##mond oil or 50 % dext ##rose water . they were randomised equally into two groups . results : a total of eighty patients including 58 males and 22 females whose age ranged from 19 to 61 years were included in the study . bleeding was the chief symptom which was present in 78 ( 97 . 5 % ) patients , and there was complete resolution of this in 92 . 3 % and 89 . 7 % in the 5 % phenol in al ##mond oil and 50 % dext ##rose water groups , respectively , at 6 months ( p = 0 . 90 ##5 ) . similar degrees of resolution were noted for anal protr ##usion ( 89 . 1 % and 85 . 3 % in the 5 % phenol in al ##mond oil and 50 % dext ##rose water group , respectively [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 2166 131 502 4209 9518 4591 26219 121 111 922 131 12104 21047 4692 121 17964 1290 103 6454 862 111 3201 131 238 527 241 147 3745 111 4684 137 4104 131 305 1863 15302 121 186 14521 4981 190 1539 1863 28814 5718 1506 188 9518 4591 26219 121 111 922 131 705 579 147 2765 579 2460 21047 4692 205 568 137 1045 862 238 241 106 6880 422 6935 527 2728 191 27075 568 190 705 579 422 971 579 137 2765 579 2460 21047 4692 975 6670 119 147 922 214 4209 9518 3714 190 1676 305 1863 15302 121 186 14521 4981 234 1539 1863 28814 5718 1506 205 698 267 14136 6918 690 502 1302 205 545 862 106 1114 131 27075 568 1471 4878 5165 137 1931 5003 3489 1407 7650 263 371 147 7610 1320 267 1936 121 111 527 205 8401 241 111 16494 8316 334 241 709 121 7898 145 8276 205 305 1863 546 568 422 137 461 241 2467 3421 131 238 121 8698 205 239 1863 137 8765 205 450 1863 121 111 305 1863 15302 121 186 14521 4981 137 1539 1863 28814 5718 1506 1302 422 1222 422 235 370 2499 145 118 275 244 205 3289 30139 546 205 868 5634 131 3421 267 3742 168 438 16830 851 145 8765 205 158 1863 137 6885 205 239 1863 121 111 305 1863 15302 121 186 14521 4981 137 1539 1863 28814 5718 1506 583 422 1222 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 2166 131 502 4209 9518 4591 26219 121 111 922 131 12104 21047 4692 121 17964 1290 103 6454 862 111 3201 131 238 527 241 147 3745 111 4684 137 4104 131 305 1863 15302 121 186 14521 4981 190 1539 1863 28814 5718 1506 188 9518 4591 26219 121 111 922 131 705 579 147 2765 579 2460 21047 4692 205 568 137 1045 862 238 241 106 6880 422 6935 527 2728 191 27075 568 190 705 579 422 971 579 137 2765 579 2460 21047 4692 975 6670 119 147 922 214 4209 9518 3714 190 1676 305 1863 15302 121 186 14521 4981 234 1539 1863 28814 5718 1506 205 698 267 14136 6918 690 502 1302 205 545 862 106 1114 131 27075 568 1471 4878 5165 137 1931 5003 3489 1407 7650 263 371 147 7610 1320 267 1936 121 111 527 205 8401 241 111 16494 8316 334 241 709 121 7898 145 8276 205 305 1863 546 568 422 137 461 241 2467 3421 131 238 121 8698 205 239 1863 137 8765 205 450 1863 121 111 305 1863 15302 121 186 14521 4981 137 1539 1863 28814 5718 1506 1302 422 1222 422 235 370 2499 145 118 275 244 205 3289 30139 546 205 868 5634 131 3421 267 3742 168 438 16830 851 145 8765 205 158 1863 137 6885 205 239 1863 121 111 305 1863 15302 121 186 14521 4981 137 1539 1863 28814 5718 1506 583 422 1222 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used for model gpu 0\n",
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from study_type_multi\\model.ckpt-2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from study_type_multi\\model.ckpt-2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] pea ##san ##ts and agriculture [SEP] imperial russia had an overwhelming ##ly pea ##san ##t population and its economy was largely agricultural . the russian empir ##e was not able to compete economically and mil ##ita ##ril ##y with the more  developed  states of north - western europe , north america and japan in the last few decades of its existence . over the preceding two or three centuries , however , russia  s auto ##cr ##atic state had been able successfully to exploit its pea ##san ##t population and agricultural economy to generate the resources , in particular tax revenues and military cons ##cripts , to consol ##idate and maintain its power at home and build a vast empir ##e that came to dominate eastern europe and northern asia . imperial russia  s pea ##san ##ts were , thus , at the bottom of an exploit ##ative social order . for much of the period , moreover , between a third and a half were the ser ##fs of noble land ##own ##ers in a system of bonded labour that emerged in the late sixteen ##th century and lasted until its abol ##ition in 186 ##1 . most of the rest of the pea ##san ##try lived on state lands and were subjected to slightly less one ##rous restrictions and demands . the sub ##ordinate and exploited status of all imperial russia  s pea ##san ##ts is one of the wider contexts in which their ways of life [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] pea ##san ##ts and agriculture [SEP] imperial russia had an overwhelming ##ly pea ##san ##t population and its economy was largely agricultural . the russian empir ##e was not able to compete economically and mil ##ita ##ril ##y with the more  developed  states of north - western europe , north america and japan in the last few decades of its existence . over the preceding two or three centuries , however , russia  s auto ##cr ##atic state had been able successfully to exploit its pea ##san ##t population and agricultural economy to generate the resources , in particular tax revenues and military cons ##cripts , to consol ##idate and maintain its power at home and build a vast empir ##e that came to dominate eastern europe and northern asia . imperial russia  s pea ##san ##ts were , thus , at the bottom of an exploit ##ative social order . for much of the period , moreover , between a third and a half were the ser ##fs of noble land ##own ##ers in a system of bonded labour that emerged in the late sixteen ##th century and lasted until its abol ##ition in 186 ##1 . most of the rest of the pea ##san ##try lived on state lands and were subjected to slightly less one ##rous restrictions and demands . the sub ##ordinate and exploited status of all imperial russia  s pea ##san ##ts is one of the wider contexts in which their ways of life [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 25038 17011 203 137 9879 103 27524 17094 883 130 22954 179 25038 17011 30108 1638 137 633 7425 241 5939 7675 205 111 14658 4153 30107 241 302 2357 147 17080 20003 137 3466 6351 4598 30126 190 111 475 1384 1815 5459 1898 131 4656 579 4539 3371 422 4656 7968 137 4838 121 111 2442 2149 8148 131 633 5074 205 573 111 10663 502 234 874 27232 422 694 422 17094 5459 112 5975 1304 757 1098 883 528 2357 5429 147 11091 633 25038 17011 30108 1638 137 7675 7425 147 3730 111 2965 422 121 1349 3764 22297 137 10945 401 20085 422 147 13181 23691 137 2603 633 1221 235 3417 137 2875 106 10946 4153 30107 198 10425 147 17893 10309 3371 137 7926 10705 205 27524 17094 5459 112 25038 17011 203 267 422 959 422 235 111 5473 131 130 11091 456 1748 993 205 168 1839 131 111 1275 422 2428 422 467 106 2765 137 106 3437 267 111 846 2373 131 28596 2882 1513 270 121 106 429 131 23152 10814 198 10036 121 111 4410 24158 266 8862 137 20376 3088 633 13721 319 121 18683 30130 205 755 131 111 2619 131 111 25038 17011 4272 18118 191 1098 24662 137 267 7178 147 4237 1279 482 9615 9129 137 8515 205 111 414 18147 137 13294 2726 131 355 27524 17094 5459 112 25038 17011 203 165 482 131 111 10147 8653 121 334 547 4568 131 1994 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 25038 17011 203 137 9879 103 27524 17094 883 130 22954 179 25038 17011 30108 1638 137 633 7425 241 5939 7675 205 111 14658 4153 30107 241 302 2357 147 17080 20003 137 3466 6351 4598 30126 190 111 475 1384 1815 5459 1898 131 4656 579 4539 3371 422 4656 7968 137 4838 121 111 2442 2149 8148 131 633 5074 205 573 111 10663 502 234 874 27232 422 694 422 17094 5459 112 5975 1304 757 1098 883 528 2357 5429 147 11091 633 25038 17011 30108 1638 137 7675 7425 147 3730 111 2965 422 121 1349 3764 22297 137 10945 401 20085 422 147 13181 23691 137 2603 633 1221 235 3417 137 2875 106 10946 4153 30107 198 10425 147 17893 10309 3371 137 7926 10705 205 27524 17094 5459 112 25038 17011 203 267 422 959 422 235 111 5473 131 130 11091 456 1748 993 205 168 1839 131 111 1275 422 2428 422 467 106 2765 137 106 3437 267 111 846 2373 131 28596 2882 1513 270 121 106 429 131 23152 10814 198 10036 121 111 4410 24158 266 8862 137 20376 3088 633 13721 319 121 18683 30130 205 755 131 111 2619 131 111 25038 17011 4272 18118 191 1098 24662 137 267 7178 147 4237 1279 482 9615 9129 137 8515 205 111 414 18147 137 13294 2726 131 355 27524 17094 5459 112 25038 17011 203 165 482 131 111 10147 8653 121 334 547 4568 131 1994 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] pea ##san ##ts and agriculture [SEP] of noble land ##own ##ers in a system of bonded labour that emerged in the late sixteen ##th century and lasted until its abol ##ition in 186 ##1 . most of the rest of the pea ##san ##try lived on state lands and were subjected to slightly less one ##rous restrictions and demands . the sub ##ordinate and exploited status of all imperial russia  s pea ##san ##ts is one of the wider contexts in which their ways of life can be examined . another wider context is the natural environment in which they lived and worked . in the northern half of russia , which was covered in forests , the soils were not very fertile , and the winter ##s long and cold . in the southern half , in contrast , the black earth of the step ##pes was very fertile , the climate warm ##er , but the rainfall was low and unreliable [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] pea ##san ##ts and agriculture [SEP] of noble land ##own ##ers in a system of bonded labour that emerged in the late sixteen ##th century and lasted until its abol ##ition in 186 ##1 . most of the rest of the pea ##san ##try lived on state lands and were subjected to slightly less one ##rous restrictions and demands . the sub ##ordinate and exploited status of all imperial russia  s pea ##san ##ts is one of the wider contexts in which their ways of life can be examined . another wider context is the natural environment in which they lived and worked . in the northern half of russia , which was covered in forests , the soils were not very fertile , and the winter ##s long and cold . in the southern half , in contrast , the black earth of the step ##pes was very fertile , the climate warm ##er , but the rainfall was low and unreliable [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 25038 17011 203 137 9879 103 131 28596 2882 1513 270 121 106 429 131 23152 10814 198 10036 121 111 4410 24158 266 8862 137 20376 3088 633 13721 319 121 18683 30130 205 755 131 111 2619 131 111 25038 17011 4272 18118 191 1098 24662 137 267 7178 147 4237 1279 482 9615 9129 137 8515 205 111 414 18147 137 13294 2726 131 355 27524 17094 5459 112 25038 17011 203 165 482 131 111 10147 8653 121 334 547 4568 131 1994 300 195 2959 205 1783 10147 2220 165 111 2404 1451 121 334 698 18118 137 11182 205 121 111 7926 3437 131 17094 422 334 241 7036 121 12514 422 111 10511 267 302 1248 30086 422 137 111 9305 30113 1113 137 6663 205 121 111 7663 3437 422 121 2144 422 111 3778 6732 131 111 1371 10829 241 1248 30086 422 111 6002 8591 114 422 563 111 12660 241 629 137 22923 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 25038 17011 203 137 9879 103 131 28596 2882 1513 270 121 106 429 131 23152 10814 198 10036 121 111 4410 24158 266 8862 137 20376 3088 633 13721 319 121 18683 30130 205 755 131 111 2619 131 111 25038 17011 4272 18118 191 1098 24662 137 267 7178 147 4237 1279 482 9615 9129 137 8515 205 111 414 18147 137 13294 2726 131 355 27524 17094 5459 112 25038 17011 203 165 482 131 111 10147 8653 121 334 547 4568 131 1994 300 195 2959 205 1783 10147 2220 165 111 2404 1451 121 334 698 18118 137 11182 205 121 111 7926 3437 131 17094 422 334 241 7036 121 12514 422 111 10511 267 302 1248 30086 422 137 111 9305 30113 1113 137 6663 205 121 111 7663 3437 422 121 2144 422 111 3778 6732 131 111 1371 10829 241 1248 30086 422 111 6002 8591 114 422 563 111 12660 241 629 137 22923 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] pea ##san ##ts and agriculture [SEP] and unreliable [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] pea ##san ##ts and agriculture [SEP] and unreliable [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 25038 17011 203 137 9879 103 137 22923 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 25038 17011 203 137 9879 103 137 22923 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] bean seed dissemination systems in mal ##awi : a strategy [SEP] farmers have shown considerable interest in purchasing seed of newly released dry bean ( phase ##olus vulgaris l . ) varieties in mal ##awi . however , this seed is not available on a large scale to farmers at present . the private seed sector in the country is not interested in bean seed production due to low profit margins . as a self - pollin ##ating crop , once farmers have obtained initial seed stock , it can be recycl ##ed for some time . a strategy has been developed to focus on alternative sustainable informal seed production and dissemination systems , which builds on lessons learn ##t from previous experiences within mal ##awi and other african countries . these include : ( i ) informal seed multiplication , using small ##holder farmers ; ( ii ) informal seed distribution channels , using gro ##cer ##y shop ##s , rural traders , extension agents , health clinics , and ng ##os ; and ( iii ) intens ##ified variety promotion through public ##ity , using poster ##s , leaf ##lets , bro ##ch ##ures and radio messages . these activities are carried out in close collaboration with farmers , ng ##os , extension agencies , village traders and various other institutions . the results from initial sales through informal distribution channels were encouraging and 80 % of the seed was sold [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] bean seed dissemination systems in mal ##awi : a strategy [SEP] farmers have shown considerable interest in purchasing seed of newly released dry bean ( phase ##olus vulgaris l . ) varieties in mal ##awi . however , this seed is not available on a large scale to farmers at present . the private seed sector in the country is not interested in bean seed production due to low profit margins . as a self - pollin ##ating crop , once farmers have obtained initial seed stock , it can be recycl ##ed for some time . a strategy has been developed to focus on alternative sustainable informal seed production and dissemination systems , which builds on lessons learn ##t from previous experiences within mal ##awi and other african countries . these include : ( i ) informal seed multiplication , using small ##holder farmers ; ( ii ) informal seed distribution channels , using gro ##cer ##y shop ##s , rural traders , extension agents , health clinics , and ng ##os ; and ( iii ) intens ##ified variety promotion through public ##ity , using poster ##s , leaf ##lets , bro ##ch ##ures and radio messages . these activities are carried out in close collaboration with farmers , ng ##os , extension agencies , village traders and various other institutions . the results from initial sales through informal distribution channels were encouraging and 80 % of the seed was sold [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 23022 3678 14265 1078 121 1774 18265 862 106 2691 103 12288 360 817 6132 1291 121 22085 3678 131 7737 7163 5965 23022 145 1481 24815 27039 152 205 546 13600 121 1774 18265 205 694 422 238 3678 165 302 1427 191 106 1135 2211 147 12288 235 709 205 111 5225 3678 5672 121 111 5192 165 302 7301 121 23022 3678 1865 1074 147 629 9047 14185 205 188 106 1968 579 26907 560 8501 422 3246 12288 360 1151 1700 3678 5426 422 256 300 195 13956 119 168 693 532 205 106 2691 434 528 1815 147 1790 191 3350 10886 13217 3678 1865 137 14265 1078 422 334 18619 191 15789 6714 30108 263 1061 6274 1017 1774 18265 137 494 7608 3198 205 407 2212 862 145 259 546 13217 3678 11957 422 487 952 13643 12288 1814 145 1264 546 13217 3678 1382 3994 422 487 419 736 30126 14260 30113 422 7046 29607 422 3840 3027 422 947 14862 422 137 3312 169 1814 137 145 2786 546 14733 1237 3835 11185 833 1771 208 422 487 18492 30113 422 5513 6660 422 2096 255 571 137 5203 5847 205 407 2687 220 3065 556 121 3156 7392 190 12288 422 3312 169 422 3840 11934 422 17719 29607 137 1711 494 7238 205 111 545 263 1700 10252 833 13217 1382 3994 267 14931 137 2833 1863 131 111 3678 241 17922 103 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 23022 3678 14265 1078 121 1774 18265 862 106 2691 103 12288 360 817 6132 1291 121 22085 3678 131 7737 7163 5965 23022 145 1481 24815 27039 152 205 546 13600 121 1774 18265 205 694 422 238 3678 165 302 1427 191 106 1135 2211 147 12288 235 709 205 111 5225 3678 5672 121 111 5192 165 302 7301 121 23022 3678 1865 1074 147 629 9047 14185 205 188 106 1968 579 26907 560 8501 422 3246 12288 360 1151 1700 3678 5426 422 256 300 195 13956 119 168 693 532 205 106 2691 434 528 1815 147 1790 191 3350 10886 13217 3678 1865 137 14265 1078 422 334 18619 191 15789 6714 30108 263 1061 6274 1017 1774 18265 137 494 7608 3198 205 407 2212 862 145 259 546 13217 3678 11957 422 487 952 13643 12288 1814 145 1264 546 13217 3678 1382 3994 422 487 419 736 30126 14260 30113 422 7046 29607 422 3840 3027 422 947 14862 422 137 3312 169 1814 137 145 2786 546 14733 1237 3835 11185 833 1771 208 422 487 18492 30113 422 5513 6660 422 2096 255 571 137 5203 5847 205 407 2687 220 3065 556 121 3156 7392 190 12288 422 3312 169 422 3840 11934 422 17719 29607 137 1711 494 7238 205 111 545 263 1700 10252 833 13217 1382 3994 267 14931 137 2833 1863 131 111 3678 241 17922 103 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] bean seed dissemination systems in mal ##awi : a strategy [SEP] variety promotion through public ##ity , using poster ##s , leaf ##lets , bro ##ch ##ures and radio messages . these activities are carried out in close collaboration with farmers , ng ##os , extension agencies , village traders and various other institutions . the results from initial sales through informal distribution channels were encouraging and 80 % of the seed was sold [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] bean seed dissemination systems in mal ##awi : a strategy [SEP] variety promotion through public ##ity , using poster ##s , leaf ##lets , bro ##ch ##ures and radio messages . these activities are carried out in close collaboration with farmers , ng ##os , extension agencies , village traders and various other institutions . the results from initial sales through informal distribution channels were encouraging and 80 % of the seed was sold [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 23022 3678 14265 1078 121 1774 18265 862 106 2691 103 3835 11185 833 1771 208 422 487 18492 30113 422 5513 6660 422 2096 255 571 137 5203 5847 205 407 2687 220 3065 556 121 3156 7392 190 12288 422 3312 169 422 3840 11934 422 17719 29607 137 1711 494 7238 205 111 545 263 1700 10252 833 13217 1382 3994 267 14931 137 2833 1863 131 111 3678 241 17922 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 23022 3678 14265 1078 121 1774 18265 862 106 2691 103 3835 11185 833 1771 208 422 487 18492 30113 422 5513 6660 422 2096 255 571 137 5203 5847 205 407 2687 220 3065 556 121 3156 7392 190 12288 422 3312 169 422 3840 11934 422 17719 29607 137 1711 494 7238 205 111 545 263 1700 10252 833 13217 1382 3994 267 14931 137 2833 1863 131 111 3678 241 17922 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used for model gpu 0\n",
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from study_type_multi\\model.ckpt-2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from study_type_multi\\model.ckpt-2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] pea ##san ##ts and agriculture [SEP] able successfully to exploit its pea ##san ##t population and agricultural economy to generate the resources , in particular tax revenues and military cons ##cripts , to consol ##idate and maintain its power at home and build a vast empir ##e that came to dominate eastern europe and northern asia . imperial russia  s pea ##san ##ts were , thus , at the bottom of an exploit ##ative social order . for much of the period , moreover , between a third and a half were the ser ##fs of noble land ##own ##ers in a system of bonded labour that emerged in the late sixteen ##th century and lasted until its abol ##ition in 186 ##1 . most of the rest of the pea ##san ##try lived on state lands and were subjected to slightly less one ##rous restrictions and demands . the sub ##ordinate and exploited status of all imperial russia  s pea ##san ##ts is one of the wider contexts in which their ways of life can be examined . another wider context is the natural environment in which they lived and worked . in the northern half of russia , which was covered in forests , the soils were not very fertile , and the winter ##s long and cold . in the southern half , in contrast , the black earth of the step ##pes was very fertile , the climate warm ##er , but the rainfall was low and unreliable [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] pea ##san ##ts and agriculture [SEP] able successfully to exploit its pea ##san ##t population and agricultural economy to generate the resources , in particular tax revenues and military cons ##cripts , to consol ##idate and maintain its power at home and build a vast empir ##e that came to dominate eastern europe and northern asia . imperial russia  s pea ##san ##ts were , thus , at the bottom of an exploit ##ative social order . for much of the period , moreover , between a third and a half were the ser ##fs of noble land ##own ##ers in a system of bonded labour that emerged in the late sixteen ##th century and lasted until its abol ##ition in 186 ##1 . most of the rest of the pea ##san ##try lived on state lands and were subjected to slightly less one ##rous restrictions and demands . the sub ##ordinate and exploited status of all imperial russia  s pea ##san ##ts is one of the wider contexts in which their ways of life can be examined . another wider context is the natural environment in which they lived and worked . in the northern half of russia , which was covered in forests , the soils were not very fertile , and the winter ##s long and cold . in the southern half , in contrast , the black earth of the step ##pes was very fertile , the climate warm ##er , but the rainfall was low and unreliable [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 25038 17011 203 137 9879 103 2357 5429 147 11091 633 25038 17011 30108 1638 137 7675 7425 147 3730 111 2965 422 121 1349 3764 22297 137 10945 401 20085 422 147 13181 23691 137 2603 633 1221 235 3417 137 2875 106 10946 4153 30107 198 10425 147 17893 10309 3371 137 7926 10705 205 27524 17094 5459 112 25038 17011 203 267 422 959 422 235 111 5473 131 130 11091 456 1748 993 205 168 1839 131 111 1275 422 2428 422 467 106 2765 137 106 3437 267 111 846 2373 131 28596 2882 1513 270 121 106 429 131 23152 10814 198 10036 121 111 4410 24158 266 8862 137 20376 3088 633 13721 319 121 18683 30130 205 755 131 111 2619 131 111 25038 17011 4272 18118 191 1098 24662 137 267 7178 147 4237 1279 482 9615 9129 137 8515 205 111 414 18147 137 13294 2726 131 355 27524 17094 5459 112 25038 17011 203 165 482 131 111 10147 8653 121 334 547 4568 131 1994 300 195 2959 205 1783 10147 2220 165 111 2404 1451 121 334 698 18118 137 11182 205 121 111 7926 3437 131 17094 422 334 241 7036 121 12514 422 111 10511 267 302 1248 30086 422 137 111 9305 30113 1113 137 6663 205 121 111 7663 3437 422 121 2144 422 111 3778 6732 131 111 1371 10829 241 1248 30086 422 111 6002 8591 114 422 563 111 12660 241 629 137 22923 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 25038 17011 203 137 9879 103 2357 5429 147 11091 633 25038 17011 30108 1638 137 7675 7425 147 3730 111 2965 422 121 1349 3764 22297 137 10945 401 20085 422 147 13181 23691 137 2603 633 1221 235 3417 137 2875 106 10946 4153 30107 198 10425 147 17893 10309 3371 137 7926 10705 205 27524 17094 5459 112 25038 17011 203 267 422 959 422 235 111 5473 131 130 11091 456 1748 993 205 168 1839 131 111 1275 422 2428 422 467 106 2765 137 106 3437 267 111 846 2373 131 28596 2882 1513 270 121 106 429 131 23152 10814 198 10036 121 111 4410 24158 266 8862 137 20376 3088 633 13721 319 121 18683 30130 205 755 131 111 2619 131 111 25038 17011 4272 18118 191 1098 24662 137 267 7178 147 4237 1279 482 9615 9129 137 8515 205 111 414 18147 137 13294 2726 131 355 27524 17094 5459 112 25038 17011 203 165 482 131 111 10147 8653 121 334 547 4568 131 1994 300 195 2959 205 1783 10147 2220 165 111 2404 1451 121 334 698 18118 137 11182 205 121 111 7926 3437 131 17094 422 334 241 7036 121 12514 422 111 10511 267 302 1248 30086 422 137 111 9305 30113 1113 137 6663 205 121 111 7663 3437 422 121 2144 422 111 3778 6732 131 111 1371 10829 241 1248 30086 422 111 6002 8591 114 422 563 111 12660 241 629 137 22923 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] bean seed dissemination systems in mal ##awi : a strategy [SEP] farmers have shown considerable interest in purchasing seed of newly released dry bean ( phase ##olus vulgaris l . ) varieties in mal ##awi . however , this seed is not available on a large scale to farmers at present . the private seed sector in the country is not interested in bean seed production due to low profit margins . as a self - pollin ##ating crop , once farmers have obtained initial seed stock , it can be recycl ##ed for some time . a strategy has been developed to focus on alternative sustainable informal seed production and dissemination systems , which builds on lessons learn ##t from previous experiences within mal ##awi and other african countries . these include : ( i ) informal seed multiplication , using small ##holder farmers ; ( ii ) informal seed distribution channels , using gro ##cer ##y shop ##s , rural traders , extension agents , health clinics , and ng ##os ; and ( iii ) intens ##ified variety promotion through public ##ity , using poster ##s , leaf ##lets , bro ##ch ##ures and radio messages . these activities are carried out in close collaboration with farmers , ng ##os , extension agencies , village traders and various other institutions . the results from initial sales through informal distribution channels were encouraging and 80 % of the seed was sold [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] bean seed dissemination systems in mal ##awi : a strategy [SEP] farmers have shown considerable interest in purchasing seed of newly released dry bean ( phase ##olus vulgaris l . ) varieties in mal ##awi . however , this seed is not available on a large scale to farmers at present . the private seed sector in the country is not interested in bean seed production due to low profit margins . as a self - pollin ##ating crop , once farmers have obtained initial seed stock , it can be recycl ##ed for some time . a strategy has been developed to focus on alternative sustainable informal seed production and dissemination systems , which builds on lessons learn ##t from previous experiences within mal ##awi and other african countries . these include : ( i ) informal seed multiplication , using small ##holder farmers ; ( ii ) informal seed distribution channels , using gro ##cer ##y shop ##s , rural traders , extension agents , health clinics , and ng ##os ; and ( iii ) intens ##ified variety promotion through public ##ity , using poster ##s , leaf ##lets , bro ##ch ##ures and radio messages . these activities are carried out in close collaboration with farmers , ng ##os , extension agencies , village traders and various other institutions . the results from initial sales through informal distribution channels were encouraging and 80 % of the seed was sold [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 23022 3678 14265 1078 121 1774 18265 862 106 2691 103 12288 360 817 6132 1291 121 22085 3678 131 7737 7163 5965 23022 145 1481 24815 27039 152 205 546 13600 121 1774 18265 205 694 422 238 3678 165 302 1427 191 106 1135 2211 147 12288 235 709 205 111 5225 3678 5672 121 111 5192 165 302 7301 121 23022 3678 1865 1074 147 629 9047 14185 205 188 106 1968 579 26907 560 8501 422 3246 12288 360 1151 1700 3678 5426 422 256 300 195 13956 119 168 693 532 205 106 2691 434 528 1815 147 1790 191 3350 10886 13217 3678 1865 137 14265 1078 422 334 18619 191 15789 6714 30108 263 1061 6274 1017 1774 18265 137 494 7608 3198 205 407 2212 862 145 259 546 13217 3678 11957 422 487 952 13643 12288 1814 145 1264 546 13217 3678 1382 3994 422 487 419 736 30126 14260 30113 422 7046 29607 422 3840 3027 422 947 14862 422 137 3312 169 1814 137 145 2786 546 14733 1237 3835 11185 833 1771 208 422 487 18492 30113 422 5513 6660 422 2096 255 571 137 5203 5847 205 407 2687 220 3065 556 121 3156 7392 190 12288 422 3312 169 422 3840 11934 422 17719 29607 137 1711 494 7238 205 111 545 263 1700 10252 833 13217 1382 3994 267 14931 137 2833 1863 131 111 3678 241 17922 103 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 23022 3678 14265 1078 121 1774 18265 862 106 2691 103 12288 360 817 6132 1291 121 22085 3678 131 7737 7163 5965 23022 145 1481 24815 27039 152 205 546 13600 121 1774 18265 205 694 422 238 3678 165 302 1427 191 106 1135 2211 147 12288 235 709 205 111 5225 3678 5672 121 111 5192 165 302 7301 121 23022 3678 1865 1074 147 629 9047 14185 205 188 106 1968 579 26907 560 8501 422 3246 12288 360 1151 1700 3678 5426 422 256 300 195 13956 119 168 693 532 205 106 2691 434 528 1815 147 1790 191 3350 10886 13217 3678 1865 137 14265 1078 422 334 18619 191 15789 6714 30108 263 1061 6274 1017 1774 18265 137 494 7608 3198 205 407 2212 862 145 259 546 13217 3678 11957 422 487 952 13643 12288 1814 145 1264 546 13217 3678 1382 3994 422 487 419 736 30126 14260 30113 422 7046 29607 422 3840 3027 422 947 14862 422 137 3312 169 1814 137 145 2786 546 14733 1237 3835 11185 833 1771 208 422 487 18492 30113 422 5513 6660 422 2096 255 571 137 5203 5847 205 407 2687 220 3065 556 121 3156 7392 190 12288 422 3312 169 422 3840 11934 422 17719 29607 137 1711 494 7238 205 111 545 263 1700 10252 833 13217 1382 3994 267 14931 137 2833 1863 131 111 3678 241 17922 103 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] small ##holders and niche markets : lessons from the and ##es [SEP] markets , which include organic , fair trade and go ##ur ##met markets . these offer greater benefits to farmers , by paying them a higher price for their produce , but demand new skills and forms of organisation . research in bol ##iv ##ia , peru and ecu ##ador focused on the extent to which producers of two widely trade ##d crops , ban ##ana ##s and coffee , are able to take advantage of expanding global markets . in ecu ##ador , ban ##ana production is largely in the hands of independent producers while a hand ##ful of companies control the export trade . these companies have considerable influence on the prices paid to farmers . the majority of farmers are hardly able to cover their production costs . coffee producers are confron ##ted with a market where over - supply in recent years has led to a fall in coffee prices . the ban ##ana and coffee sectors demonstrate that small ##holder producers often find themselves dise ##mp ##ower ##ed and unable to benefit from conventional commodity markets . several non - governmental organisations ( ng ##os ) are working with ban ##ana and coffee producers to tap into niche markets , which include organic , fair trade and go ##ur ##met markets . these offer greater benefits to farmers , by paying them a higher price for their produce , but demand new skills and forms of organisation [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] small ##holders and niche markets : lessons from the and ##es [SEP] markets , which include organic , fair trade and go ##ur ##met markets . these offer greater benefits to farmers , by paying them a higher price for their produce , but demand new skills and forms of organisation . research in bol ##iv ##ia , peru and ecu ##ador focused on the extent to which producers of two widely trade ##d crops , ban ##ana ##s and coffee , are able to take advantage of expanding global markets . in ecu ##ador , ban ##ana production is largely in the hands of independent producers while a hand ##ful of companies control the export trade . these companies have considerable influence on the prices paid to farmers . the majority of farmers are hardly able to cover their production costs . coffee producers are confron ##ted with a market where over - supply in recent years has led to a fall in coffee prices . the ban ##ana and coffee sectors demonstrate that small ##holder producers often find themselves dise ##mp ##ower ##ed and unable to benefit from conventional commodity markets . several non - governmental organisations ( ng ##os ) are working with ban ##ana and coffee producers to tap into niche markets , which include organic , fair trade and go ##ur ##met markets . these offer greater benefits to farmers , by paying them a higher price for their produce , but demand new skills and forms of organisation [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 952 10019 137 17433 7838 862 15789 263 111 137 123 103 7838 422 334 2212 4756 422 5623 4931 137 796 166 1566 7838 205 407 5066 2110 4376 147 12288 422 214 24558 1445 106 1001 4048 168 547 3299 422 563 3880 758 5561 137 3444 131 13202 205 849 121 8308 296 426 422 26460 137 29519 17544 4786 191 111 3796 147 334 15793 131 502 4276 4931 30118 12760 422 6550 4846 30113 137 17677 422 220 2357 147 2636 3944 131 13147 2523 7838 205 121 29519 17544 422 6550 4846 1865 165 5939 121 111 12053 131 2229 15793 969 106 1500 1004 131 6724 602 111 11484 4931 205 407 6724 360 6132 2496 191 111 6759 10955 147 12288 205 111 4185 131 12288 220 16136 2357 147 4144 547 1865 3266 205 17677 15793 220 17666 248 190 106 2753 582 573 579 4494 121 2151 1320 434 4030 147 106 3913 121 17677 6759 205 111 6550 4846 137 17677 11329 3768 198 952 13643 15793 1992 1040 5675 1035 5803 7946 119 137 8576 147 4541 263 3744 19821 7838 205 1323 699 579 26342 16526 145 3312 169 546 220 3630 190 6550 4846 137 17677 15793 147 10366 690 17433 7838 422 334 2212 4756 422 5623 4931 137 796 166 1566 7838 205 407 5066 2110 4376 147 12288 422 214 24558 1445 106 1001 4048 168 547 3299 422 563 3880 758 5561 137 3444 131 13202 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 952 10019 137 17433 7838 862 15789 263 111 137 123 103 7838 422 334 2212 4756 422 5623 4931 137 796 166 1566 7838 205 407 5066 2110 4376 147 12288 422 214 24558 1445 106 1001 4048 168 547 3299 422 563 3880 758 5561 137 3444 131 13202 205 849 121 8308 296 426 422 26460 137 29519 17544 4786 191 111 3796 147 334 15793 131 502 4276 4931 30118 12760 422 6550 4846 30113 137 17677 422 220 2357 147 2636 3944 131 13147 2523 7838 205 121 29519 17544 422 6550 4846 1865 165 5939 121 111 12053 131 2229 15793 969 106 1500 1004 131 6724 602 111 11484 4931 205 407 6724 360 6132 2496 191 111 6759 10955 147 12288 205 111 4185 131 12288 220 16136 2357 147 4144 547 1865 3266 205 17677 15793 220 17666 248 190 106 2753 582 573 579 4494 121 2151 1320 434 4030 147 106 3913 121 17677 6759 205 111 6550 4846 137 17677 11329 3768 198 952 13643 15793 1992 1040 5675 1035 5803 7946 119 137 8576 147 4541 263 3744 19821 7838 205 1323 699 579 26342 16526 145 3312 169 546 220 3630 190 6550 4846 137 17677 15793 147 10366 690 17433 7838 422 334 2212 4756 422 5623 4931 137 796 166 1566 7838 205 407 5066 2110 4376 147 12288 422 214 24558 1445 106 1001 4048 168 547 3299 422 563 3880 758 5561 137 3444 131 13202 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] agro ##fore ##stry system combining p . jul ##if ##lora and buff ##el grass in the brazilian semi - arid region . preliminary results [SEP] 2 - m diameter clear areas around the seedlings ( ( 3 ) and ( 4 ) respectively ) . plots were fertil ##ized with np ##k fertilizer ( 5 : 14 : 3 ) and treated with 100 g ald ##rin to prevent term ##ite attack . hoe ##ing and weed ##ing was done 20 days after planting and survival and growth measurements made 3 - monthly for 1 yr and at 19 and 30 month old . seedling survival increased with the amount of cultural practice and at 1 yr old was 100 % in ( 1 ) , 85 % in ( 4 ) , 20 % in ( 3 ) and only 5 % in ( 2 ) . growth measurements up to this age followed a similar pattern . at 30 month old , p . jul ##if ##lora only survived in treatments ( 1 ) and ( 4 ) , where values for survival and biomass production were 100 and 85 % and 17 66 ##9 and 275 ##7 kg / ha dm respectively . values for buff ##el grass production at 30 month old were 0 , 10 05 ##0 , 98 ##71 and 87 ##36 kg / ha dm respectively in treatments ( 1 - 4 ) ; the yield from buff ##el grass alone was 73 ##73 kg / ha [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] agro ##fore ##stry system combining p . jul ##if ##lora and buff ##el grass in the brazilian semi - arid region . preliminary results [SEP] 2 - m diameter clear areas around the seedlings ( ( 3 ) and ( 4 ) respectively ) . plots were fertil ##ized with np ##k fertilizer ( 5 : 14 : 3 ) and treated with 100 g ald ##rin to prevent term ##ite attack . hoe ##ing and weed ##ing was done 20 days after planting and survival and growth measurements made 3 - monthly for 1 yr and at 19 and 30 month old . seedling survival increased with the amount of cultural practice and at 1 yr old was 100 % in ( 1 ) , 85 % in ( 4 ) , 20 % in ( 3 ) and only 5 % in ( 2 ) . growth measurements up to this age followed a similar pattern . at 30 month old , p . jul ##if ##lora only survived in treatments ( 1 ) and ( 4 ) , where values for survival and biomass production were 100 and 85 % and 17 66 ##9 and 275 ##7 kg / ha dm respectively . values for buff ##el grass production at 30 month old were 0 , 10 05 ##0 , 98 ##71 and 87 ##36 kg / ha dm respectively in treatments ( 1 - 4 ) ; the yield from buff ##el grass alone was 73 ##73 kg / ha [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 22362 732 13573 429 6484 118 205 13182 193 26229 137 2966 154 10102 121 111 17675 6317 579 29111 1025 205 6538 545 103 170 579 127 4381 1716 2326 2715 111 13912 145 145 239 546 137 145 286 546 1222 546 205 6784 267 9585 645 190 5242 30135 22289 145 305 862 1128 862 239 546 137 2338 190 1287 159 15261 6478 147 3363 902 585 3689 205 25360 140 137 27264 140 241 2992 1012 1976 647 26225 137 2842 137 1503 2354 1827 239 579 11762 168 158 10580 137 235 371 137 1339 1978 4289 205 23557 2842 1175 190 111 2407 131 6656 2794 137 235 158 10580 4289 241 1287 1863 121 145 158 546 422 6885 1863 121 145 286 546 422 1012 1863 121 145 239 546 137 617 305 1863 121 145 170 546 205 1503 2354 692 147 238 1407 2594 106 868 1377 205 235 1339 1978 4289 422 118 205 13182 193 26229 617 19136 121 4728 145 158 546 137 145 286 546 422 582 988 168 2842 137 7537 1865 267 1287 137 6885 1863 137 1557 7107 30141 137 18887 30145 5036 1352 325 3546 1222 205 988 168 2966 154 10102 1865 235 1339 1978 4289 267 244 422 566 10764 30131 422 7871 6590 137 8468 2752 5036 1352 325 3546 1222 121 4728 145 158 579 286 546 1814 111 2210 263 2966 154 10102 3957 241 8095 5145 5036 1352 325 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 22362 732 13573 429 6484 118 205 13182 193 26229 137 2966 154 10102 121 111 17675 6317 579 29111 1025 205 6538 545 103 170 579 127 4381 1716 2326 2715 111 13912 145 145 239 546 137 145 286 546 1222 546 205 6784 267 9585 645 190 5242 30135 22289 145 305 862 1128 862 239 546 137 2338 190 1287 159 15261 6478 147 3363 902 585 3689 205 25360 140 137 27264 140 241 2992 1012 1976 647 26225 137 2842 137 1503 2354 1827 239 579 11762 168 158 10580 137 235 371 137 1339 1978 4289 205 23557 2842 1175 190 111 2407 131 6656 2794 137 235 158 10580 4289 241 1287 1863 121 145 158 546 422 6885 1863 121 145 286 546 422 1012 1863 121 145 239 546 137 617 305 1863 121 145 170 546 205 1503 2354 692 147 238 1407 2594 106 868 1377 205 235 1339 1978 4289 422 118 205 13182 193 26229 617 19136 121 4728 145 158 546 137 145 286 546 422 582 988 168 2842 137 7537 1865 267 1287 137 6885 1863 137 1557 7107 30141 137 18887 30145 5036 1352 325 3546 1222 205 988 168 2966 154 10102 1865 235 1339 1978 4289 267 244 422 566 10764 30131 422 7871 6590 137 8468 2752 5036 1352 325 3546 1222 121 4728 145 158 579 286 546 1814 111 2210 263 2966 154 10102 3957 241 8095 5145 5036 1352 325 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] evaluation of two injection scler ##osa ##nts in the treatment of symptomatic haemorrh ##oids in nigeria ##ns [SEP] eighty patients including 58 males and 22 females whose age ranged from 19 to 61 years were included in the study . bleeding was the chief symptom which was present in 78 ( 97 . 5 % ) patients , and there was complete resolution of this in 92 . 3 % and 89 . 7 % in the 5 % phenol in al ##mond oil and 50 % dext ##rose water groups , respectively , at 6 months ( p = 0 . 90 ##5 ) . similar degrees of resolution were noted for anal protr ##usion ( 89 . 1 % and 85 . 3 % in the 5 % phenol in al ##mond oil and 50 % dext ##rose water group , respectively , p = 0 . 89 ##9 ) . the overall complication rate was 3 . 6 % with anal mucosa ulcer ##ation being the only complication , occurring in three patients who had 5 % phenol in al ##mond oil . peri - procedure pain , patients satisfaction and acceptability were not significantly different in both groups ( p = 0 . 91 ##2 , 0 . 92 ##8 and 0 . 92 ##6 respectively ) . conclusion : five percent phenol in al ##mond oil and 50 % dext ##rose water are equally effective scler ##osa ##nts in the treatment of haemorrh ##oids with very low complication rates [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] evaluation of two injection scler ##osa ##nts in the treatment of symptomatic haemorrh ##oids in nigeria ##ns [SEP] eighty patients including 58 males and 22 females whose age ranged from 19 to 61 years were included in the study . bleeding was the chief symptom which was present in 78 ( 97 . 5 % ) patients , and there was complete resolution of this in 92 . 3 % and 89 . 7 % in the 5 % phenol in al ##mond oil and 50 % dext ##rose water groups , respectively , at 6 months ( p = 0 . 90 ##5 ) . similar degrees of resolution were noted for anal protr ##usion ( 89 . 1 % and 85 . 3 % in the 5 % phenol in al ##mond oil and 50 % dext ##rose water group , respectively , p = 0 . 89 ##9 ) . the overall complication rate was 3 . 6 % with anal mucosa ulcer ##ation being the only complication , occurring in three patients who had 5 % phenol in al ##mond oil . peri - procedure pain , patients satisfaction and acceptability were not significantly different in both groups ( p = 0 . 91 ##2 , 0 . 92 ##8 and 0 . 92 ##6 respectively ) . conclusion : five percent phenol in al ##mond oil and 50 % dext ##rose water are equally effective scler ##osa ##nts in the treatment of haemorrh ##oids with very low complication rates [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 2166 131 502 4209 9518 4591 26219 121 111 922 131 12104 21047 4692 121 17964 1290 103 27075 568 1471 4878 5165 137 1931 5003 3489 1407 7650 263 371 147 7610 1320 267 1936 121 111 527 205 8401 241 111 16494 8316 334 241 709 121 7898 145 8276 205 305 1863 546 568 422 137 461 241 2467 3421 131 238 121 8698 205 239 1863 137 8765 205 450 1863 121 111 305 1863 15302 121 186 14521 4981 137 1539 1863 28814 5718 1506 1302 422 1222 422 235 370 2499 145 118 275 244 205 3289 30139 546 205 868 5634 131 3421 267 3742 168 438 16830 851 145 8765 205 158 1863 137 6885 205 239 1863 121 111 305 1863 15302 121 186 14521 4981 137 1539 1863 28814 5718 1506 583 422 1222 422 118 275 244 205 8765 30141 546 205 111 2103 11100 1013 241 239 205 370 1863 190 438 12350 11737 150 1558 111 617 11100 422 6734 121 874 568 975 883 305 1863 15302 121 186 14521 4981 205 9447 579 2272 2675 422 568 6735 137 23634 267 302 1357 643 121 655 1302 145 118 275 244 205 9046 30132 422 244 205 8698 30146 137 244 205 8698 30142 1222 546 205 1705 862 2539 2163 15302 121 186 14521 4981 137 1539 1863 28814 5718 1506 220 6918 2115 9518 4591 26219 121 111 922 131 21047 4692 190 1248 629 11100 1975 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 2166 131 502 4209 9518 4591 26219 121 111 922 131 12104 21047 4692 121 17964 1290 103 27075 568 1471 4878 5165 137 1931 5003 3489 1407 7650 263 371 147 7610 1320 267 1936 121 111 527 205 8401 241 111 16494 8316 334 241 709 121 7898 145 8276 205 305 1863 546 568 422 137 461 241 2467 3421 131 238 121 8698 205 239 1863 137 8765 205 450 1863 121 111 305 1863 15302 121 186 14521 4981 137 1539 1863 28814 5718 1506 1302 422 1222 422 235 370 2499 145 118 275 244 205 3289 30139 546 205 868 5634 131 3421 267 3742 168 438 16830 851 145 8765 205 158 1863 137 6885 205 239 1863 121 111 305 1863 15302 121 186 14521 4981 137 1539 1863 28814 5718 1506 583 422 1222 422 118 275 244 205 8765 30141 546 205 111 2103 11100 1013 241 239 205 370 1863 190 438 12350 11737 150 1558 111 617 11100 422 6734 121 874 568 975 883 305 1863 15302 121 186 14521 4981 205 9447 579 2272 2675 422 568 6735 137 23634 267 302 1357 643 121 655 1302 145 118 275 244 205 9046 30132 422 244 205 8698 30146 137 244 205 8698 30142 1222 546 205 1705 862 2539 2163 15302 121 186 14521 4981 137 1539 1863 28814 5718 1506 220 6918 2115 9518 4591 26219 121 111 922 131 21047 4692 190 1248 629 11100 1975 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used for model gpu 0\n",
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from study_type_multi\\model.ckpt-2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from study_type_multi\\model.ckpt-2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "test_plus_stefan = excel_reader.ExcelReader().read_df_from_excel(\"../tmp/study_type_data/final_study_type_test_data.xlsx\")\n",
    "result = _study_type_labeler.predict_with_meta_model(test_plus_stefan, with_head_tail = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read file ../tmp/study_type_data/final_study_type_test_data.xlsx: 0.12s\n",
      "Processed file ../tmp/study_type_data/final_study_type_test_data.xlsx: 0.01s\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../tmp/scibert_scivocab_uncased\n",
      "INFO:tensorflow:Using config: {'_model_dir': '../tmp/study_type_exps_3/study_type_with_tails_Observational study', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000023702D99EB8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '../tmp/study_type_exps_3/study_type_with_tails_Observational study', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000023702D99EB8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config is done\n",
      "labelled_multi.xlsx 4\n",
      "Read file ../tmp/study_type_data\\labelled_multi.xlsx: 0.16s\n",
      "Processed file ../tmp/study_type_data\\labelled_multi.xlsx: 0.04s\n",
      "Read file ../tmp/study_type_data\\labelled_multi.xlsx: 0.11s\n",
      "Processed file ../tmp/study_type_data\\labelled_multi.xlsx: 0.04s\n",
      "Read file ../tmp/study_type_data\\labelled_multi.xlsx: 0.13s\n",
      "Processed file ../tmp/study_type_data\\labelled_multi.xlsx: 0.04s\n",
      "Read file ../tmp/study_type_data\\labelled_multi.xlsx: 0.11s\n",
      "Processed file ../tmp/study_type_data\\labelled_multi.xlsx: 0.04s\n",
      "train_keywords_plus_multi.xlsx 1\n",
      "Read file ../tmp/study_type_data\\train_keywords_plus_multi.xlsx: 1.82s\n",
      "Processed file ../tmp/study_type_data\\train_keywords_plus_multi.xlsx: 1.06s\n",
      "valid_data.xlsx 1\n",
      "Read file ../tmp/study_type_data\\valid_data.xlsx: 0.11s\n",
      "Processed file ../tmp/study_type_data\\valid_data.xlsx: 0.07s\n",
      "Read file ../tmp/study_type_data\\valid_data.xlsx: 0.10s\n",
      "Processed file ../tmp/study_type_data\\valid_data.xlsx: 0.07s\n",
      "Train  20002\n",
      "0    12416\n",
      "1     7586\n",
      "Name: label, dtype: int64\n",
      "Test  888\n",
      "0    546\n",
      "1    342\n",
      "Name: label, dtype: int64\n",
      "0    12416\n",
      "1     7586\n",
      "Name: label, dtype: int64\n",
      "0    546\n",
      "1    342\n",
      "Name: label, dtype: int64\n",
      "INFO:tensorflow:Writing example 0 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] pea ##san ##ts and agriculture [SEP] imperial russia had an overwhelming ##ly pea ##san ##t population and its economy was largely agricultural . the russian empir ##e was not able to compete economically and mil ##ita ##ril ##y with the more  developed  states of north - western europe , north america and japan in the last few decades of its existence . over the preceding two or three centuries , however , russia  s auto ##cr ##atic state had been able successfully to exploit its pea ##san ##t population and agricultural economy to generate the resources , in particular tax revenues and military cons ##cripts , to consol ##idate and maintain its power at home and build a vast empir ##e that came to dominate eastern europe and northern asia . imperial russia  s pea ##san ##ts were , thus , at the bottom of an exploit ##ative social order . for much of the period , moreover , between a third and a half were the ser ##fs of noble land ##own ##ers in a system of bonded labour that emerged in the late sixteen ##th century and lasted until its abol ##ition in 186 ##1 . most of the rest of the pea ##san ##try lived on state lands and were subjected to slightly less one ##rous restrictions and demands . the sub ##ordinate and exploited status of all imperial russia  s pea ##san ##ts is one of the wider contexts in which their ways of life [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] pea ##san ##ts and agriculture [SEP] imperial russia had an overwhelming ##ly pea ##san ##t population and its economy was largely agricultural . the russian empir ##e was not able to compete economically and mil ##ita ##ril ##y with the more  developed  states of north - western europe , north america and japan in the last few decades of its existence . over the preceding two or three centuries , however , russia  s auto ##cr ##atic state had been able successfully to exploit its pea ##san ##t population and agricultural economy to generate the resources , in particular tax revenues and military cons ##cripts , to consol ##idate and maintain its power at home and build a vast empir ##e that came to dominate eastern europe and northern asia . imperial russia  s pea ##san ##ts were , thus , at the bottom of an exploit ##ative social order . for much of the period , moreover , between a third and a half were the ser ##fs of noble land ##own ##ers in a system of bonded labour that emerged in the late sixteen ##th century and lasted until its abol ##ition in 186 ##1 . most of the rest of the pea ##san ##try lived on state lands and were subjected to slightly less one ##rous restrictions and demands . the sub ##ordinate and exploited status of all imperial russia  s pea ##san ##ts is one of the wider contexts in which their ways of life [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 25038 17011 203 137 9879 103 27524 17094 883 130 22954 179 25038 17011 30108 1638 137 633 7425 241 5939 7675 205 111 14658 4153 30107 241 302 2357 147 17080 20003 137 3466 6351 4598 30126 190 111 475 1384 1815 5459 1898 131 4656 579 4539 3371 422 4656 7968 137 4838 121 111 2442 2149 8148 131 633 5074 205 573 111 10663 502 234 874 27232 422 694 422 17094 5459 112 5975 1304 757 1098 883 528 2357 5429 147 11091 633 25038 17011 30108 1638 137 7675 7425 147 3730 111 2965 422 121 1349 3764 22297 137 10945 401 20085 422 147 13181 23691 137 2603 633 1221 235 3417 137 2875 106 10946 4153 30107 198 10425 147 17893 10309 3371 137 7926 10705 205 27524 17094 5459 112 25038 17011 203 267 422 959 422 235 111 5473 131 130 11091 456 1748 993 205 168 1839 131 111 1275 422 2428 422 467 106 2765 137 106 3437 267 111 846 2373 131 28596 2882 1513 270 121 106 429 131 23152 10814 198 10036 121 111 4410 24158 266 8862 137 20376 3088 633 13721 319 121 18683 30130 205 755 131 111 2619 131 111 25038 17011 4272 18118 191 1098 24662 137 267 7178 147 4237 1279 482 9615 9129 137 8515 205 111 414 18147 137 13294 2726 131 355 27524 17094 5459 112 25038 17011 203 165 482 131 111 10147 8653 121 334 547 4568 131 1994 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 25038 17011 203 137 9879 103 27524 17094 883 130 22954 179 25038 17011 30108 1638 137 633 7425 241 5939 7675 205 111 14658 4153 30107 241 302 2357 147 17080 20003 137 3466 6351 4598 30126 190 111 475 1384 1815 5459 1898 131 4656 579 4539 3371 422 4656 7968 137 4838 121 111 2442 2149 8148 131 633 5074 205 573 111 10663 502 234 874 27232 422 694 422 17094 5459 112 5975 1304 757 1098 883 528 2357 5429 147 11091 633 25038 17011 30108 1638 137 7675 7425 147 3730 111 2965 422 121 1349 3764 22297 137 10945 401 20085 422 147 13181 23691 137 2603 633 1221 235 3417 137 2875 106 10946 4153 30107 198 10425 147 17893 10309 3371 137 7926 10705 205 27524 17094 5459 112 25038 17011 203 267 422 959 422 235 111 5473 131 130 11091 456 1748 993 205 168 1839 131 111 1275 422 2428 422 467 106 2765 137 106 3437 267 111 846 2373 131 28596 2882 1513 270 121 106 429 131 23152 10814 198 10036 121 111 4410 24158 266 8862 137 20376 3088 633 13721 319 121 18683 30130 205 755 131 111 2619 131 111 25038 17011 4272 18118 191 1098 24662 137 267 7178 147 4237 1279 482 9615 9129 137 8515 205 111 414 18147 137 13294 2726 131 355 27524 17094 5459 112 25038 17011 203 165 482 131 111 10147 8653 121 334 547 4568 131 1994 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] bean seed dissemination systems in mal ##awi : a strategy [SEP] farmers have shown considerable interest in purchasing seed of newly released dry bean ( phase ##olus vulgaris l . ) varieties in mal ##awi . however , this seed is not available on a large scale to farmers at present . the private seed sector in the country is not interested in bean seed production due to low profit margins . as a self - pollin ##ating crop , once farmers have obtained initial seed stock , it can be recycl ##ed for some time . a strategy has been developed to focus on alternative sustainable informal seed production and dissemination systems , which builds on lessons learn ##t from previous experiences within mal ##awi and other african countries . these include : ( i ) informal seed multiplication , using small ##holder farmers ; ( ii ) informal seed distribution channels , using gro ##cer ##y shop ##s , rural traders , extension agents , health clinics , and ng ##os ; and ( iii ) intens ##ified variety promotion through public ##ity , using poster ##s , leaf ##lets , bro ##ch ##ures and radio messages . these activities are carried out in close collaboration with farmers , ng ##os , extension agencies , village traders and various other institutions . the results from initial sales through informal distribution channels were encouraging and 80 % of the seed was sold [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] bean seed dissemination systems in mal ##awi : a strategy [SEP] farmers have shown considerable interest in purchasing seed of newly released dry bean ( phase ##olus vulgaris l . ) varieties in mal ##awi . however , this seed is not available on a large scale to farmers at present . the private seed sector in the country is not interested in bean seed production due to low profit margins . as a self - pollin ##ating crop , once farmers have obtained initial seed stock , it can be recycl ##ed for some time . a strategy has been developed to focus on alternative sustainable informal seed production and dissemination systems , which builds on lessons learn ##t from previous experiences within mal ##awi and other african countries . these include : ( i ) informal seed multiplication , using small ##holder farmers ; ( ii ) informal seed distribution channels , using gro ##cer ##y shop ##s , rural traders , extension agents , health clinics , and ng ##os ; and ( iii ) intens ##ified variety promotion through public ##ity , using poster ##s , leaf ##lets , bro ##ch ##ures and radio messages . these activities are carried out in close collaboration with farmers , ng ##os , extension agencies , village traders and various other institutions . the results from initial sales through informal distribution channels were encouraging and 80 % of the seed was sold [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 23022 3678 14265 1078 121 1774 18265 862 106 2691 103 12288 360 817 6132 1291 121 22085 3678 131 7737 7163 5965 23022 145 1481 24815 27039 152 205 546 13600 121 1774 18265 205 694 422 238 3678 165 302 1427 191 106 1135 2211 147 12288 235 709 205 111 5225 3678 5672 121 111 5192 165 302 7301 121 23022 3678 1865 1074 147 629 9047 14185 205 188 106 1968 579 26907 560 8501 422 3246 12288 360 1151 1700 3678 5426 422 256 300 195 13956 119 168 693 532 205 106 2691 434 528 1815 147 1790 191 3350 10886 13217 3678 1865 137 14265 1078 422 334 18619 191 15789 6714 30108 263 1061 6274 1017 1774 18265 137 494 7608 3198 205 407 2212 862 145 259 546 13217 3678 11957 422 487 952 13643 12288 1814 145 1264 546 13217 3678 1382 3994 422 487 419 736 30126 14260 30113 422 7046 29607 422 3840 3027 422 947 14862 422 137 3312 169 1814 137 145 2786 546 14733 1237 3835 11185 833 1771 208 422 487 18492 30113 422 5513 6660 422 2096 255 571 137 5203 5847 205 407 2687 220 3065 556 121 3156 7392 190 12288 422 3312 169 422 3840 11934 422 17719 29607 137 1711 494 7238 205 111 545 263 1700 10252 833 13217 1382 3994 267 14931 137 2833 1863 131 111 3678 241 17922 103 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 23022 3678 14265 1078 121 1774 18265 862 106 2691 103 12288 360 817 6132 1291 121 22085 3678 131 7737 7163 5965 23022 145 1481 24815 27039 152 205 546 13600 121 1774 18265 205 694 422 238 3678 165 302 1427 191 106 1135 2211 147 12288 235 709 205 111 5225 3678 5672 121 111 5192 165 302 7301 121 23022 3678 1865 1074 147 629 9047 14185 205 188 106 1968 579 26907 560 8501 422 3246 12288 360 1151 1700 3678 5426 422 256 300 195 13956 119 168 693 532 205 106 2691 434 528 1815 147 1790 191 3350 10886 13217 3678 1865 137 14265 1078 422 334 18619 191 15789 6714 30108 263 1061 6274 1017 1774 18265 137 494 7608 3198 205 407 2212 862 145 259 546 13217 3678 11957 422 487 952 13643 12288 1814 145 1264 546 13217 3678 1382 3994 422 487 419 736 30126 14260 30113 422 7046 29607 422 3840 3027 422 947 14862 422 137 3312 169 1814 137 145 2786 546 14733 1237 3835 11185 833 1771 208 422 487 18492 30113 422 5513 6660 422 2096 255 571 137 5203 5847 205 407 2687 220 3065 556 121 3156 7392 190 12288 422 3312 169 422 3840 11934 422 17719 29607 137 1711 494 7238 205 111 545 263 1700 10252 833 13217 1382 3994 267 14931 137 2833 1863 131 111 3678 241 17922 103 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] small ##holders and niche markets : lessons from the and ##es [SEP] research in bol ##iv ##ia , peru and ecu ##ador focused on the extent to which producers of two widely trade ##d crops , ban ##ana ##s and coffee , are able to take advantage of expanding global markets . in ecu ##ador , ban ##ana production is largely in the hands of independent producers while a hand ##ful of companies control the export trade . these companies have considerable influence on the prices paid to farmers . the majority of farmers are hardly able to cover their production costs . coffee producers are confron ##ted with a market where over - supply in recent years has led to a fall in coffee prices . the ban ##ana and coffee sectors demonstrate that small ##holder producers often find themselves dise ##mp ##ower ##ed and unable to benefit from conventional commodity markets . several non - governmental organisations ( ng ##os ) are working with ban ##ana and coffee producers to tap into niche markets , which include organic , fair trade and go ##ur ##met markets . these offer greater benefits to farmers , by paying them a higher price for their produce , but demand new skills and forms of organisation . research in bol ##iv ##ia , peru and ecu ##ador focused on the extent to which producers of two widely trade ##d crops , ban ##ana ##s and coffee , are able to take advantage of expanding global markets [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] small ##holders and niche markets : lessons from the and ##es [SEP] research in bol ##iv ##ia , peru and ecu ##ador focused on the extent to which producers of two widely trade ##d crops , ban ##ana ##s and coffee , are able to take advantage of expanding global markets . in ecu ##ador , ban ##ana production is largely in the hands of independent producers while a hand ##ful of companies control the export trade . these companies have considerable influence on the prices paid to farmers . the majority of farmers are hardly able to cover their production costs . coffee producers are confron ##ted with a market where over - supply in recent years has led to a fall in coffee prices . the ban ##ana and coffee sectors demonstrate that small ##holder producers often find themselves dise ##mp ##ower ##ed and unable to benefit from conventional commodity markets . several non - governmental organisations ( ng ##os ) are working with ban ##ana and coffee producers to tap into niche markets , which include organic , fair trade and go ##ur ##met markets . these offer greater benefits to farmers , by paying them a higher price for their produce , but demand new skills and forms of organisation . research in bol ##iv ##ia , peru and ecu ##ador focused on the extent to which producers of two widely trade ##d crops , ban ##ana ##s and coffee , are able to take advantage of expanding global markets [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 952 10019 137 17433 7838 862 15789 263 111 137 123 103 849 121 8308 296 426 422 26460 137 29519 17544 4786 191 111 3796 147 334 15793 131 502 4276 4931 30118 12760 422 6550 4846 30113 137 17677 422 220 2357 147 2636 3944 131 13147 2523 7838 205 121 29519 17544 422 6550 4846 1865 165 5939 121 111 12053 131 2229 15793 969 106 1500 1004 131 6724 602 111 11484 4931 205 407 6724 360 6132 2496 191 111 6759 10955 147 12288 205 111 4185 131 12288 220 16136 2357 147 4144 547 1865 3266 205 17677 15793 220 17666 248 190 106 2753 582 573 579 4494 121 2151 1320 434 4030 147 106 3913 121 17677 6759 205 111 6550 4846 137 17677 11329 3768 198 952 13643 15793 1992 1040 5675 1035 5803 7946 119 137 8576 147 4541 263 3744 19821 7838 205 1323 699 579 26342 16526 145 3312 169 546 220 3630 190 6550 4846 137 17677 15793 147 10366 690 17433 7838 422 334 2212 4756 422 5623 4931 137 796 166 1566 7838 205 407 5066 2110 4376 147 12288 422 214 24558 1445 106 1001 4048 168 547 3299 422 563 3880 758 5561 137 3444 131 13202 205 849 121 8308 296 426 422 26460 137 29519 17544 4786 191 111 3796 147 334 15793 131 502 4276 4931 30118 12760 422 6550 4846 30113 137 17677 422 220 2357 147 2636 3944 131 13147 2523 7838 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 952 10019 137 17433 7838 862 15789 263 111 137 123 103 849 121 8308 296 426 422 26460 137 29519 17544 4786 191 111 3796 147 334 15793 131 502 4276 4931 30118 12760 422 6550 4846 30113 137 17677 422 220 2357 147 2636 3944 131 13147 2523 7838 205 121 29519 17544 422 6550 4846 1865 165 5939 121 111 12053 131 2229 15793 969 106 1500 1004 131 6724 602 111 11484 4931 205 407 6724 360 6132 2496 191 111 6759 10955 147 12288 205 111 4185 131 12288 220 16136 2357 147 4144 547 1865 3266 205 17677 15793 220 17666 248 190 106 2753 582 573 579 4494 121 2151 1320 434 4030 147 106 3913 121 17677 6759 205 111 6550 4846 137 17677 11329 3768 198 952 13643 15793 1992 1040 5675 1035 5803 7946 119 137 8576 147 4541 263 3744 19821 7838 205 1323 699 579 26342 16526 145 3312 169 546 220 3630 190 6550 4846 137 17677 15793 147 10366 690 17433 7838 422 334 2212 4756 422 5623 4931 137 796 166 1566 7838 205 407 5066 2110 4376 147 12288 422 214 24558 1445 106 1001 4048 168 547 3299 422 563 3880 758 5561 137 3444 131 13202 205 849 121 8308 296 426 422 26460 137 29519 17544 4786 191 111 3796 147 334 15793 131 502 4276 4931 30118 12760 422 6550 4846 30113 137 17677 422 220 2357 147 2636 3944 131 13147 2523 7838 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] agro ##fore ##stry system combining p . jul ##if ##lora and buff ##el grass in the brazilian semi - arid region . preliminary results [SEP] in a trial at per ##nam ##bu ##co , pros ##op ##is jul ##if ##lora seedlings were planted in 1983 at 3 * 4 m spacing on 4 types of plot : ( 1 ) in an area free of buff ##el grass ( cen ##chr ##us cili ##aris ) and treated using normal cultural practices ; and ( 2 - 4 ) in an area planted with buff ##el grass in 1981 and either not prepared ( 2 ) or prepared by hoe ##ing 1 - or 2 - m diameter clear areas around the seedlings ( ( 3 ) and ( 4 ) respectively ) . plots were fertil ##ized with np ##k fertilizer ( 5 : 14 : 3 ) and treated with 100 g ald ##rin to prevent term ##ite attack . hoe ##ing and weed ##ing was done 20 days after planting and survival and growth measurements made 3 - monthly for 1 yr and at 19 and 30 month old . seedling survival increased with the amount of cultural practice and at 1 yr old was 100 % in ( 1 ) , 85 % in ( 4 ) , 20 % in ( 3 ) and only 5 % in ( 2 ) . growth measurements up to this age followed a similar pattern . at 30 month old , p . [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] agro ##fore ##stry system combining p . jul ##if ##lora and buff ##el grass in the brazilian semi - arid region . preliminary results [SEP] in a trial at per ##nam ##bu ##co , pros ##op ##is jul ##if ##lora seedlings were planted in 1983 at 3 * 4 m spacing on 4 types of plot : ( 1 ) in an area free of buff ##el grass ( cen ##chr ##us cili ##aris ) and treated using normal cultural practices ; and ( 2 - 4 ) in an area planted with buff ##el grass in 1981 and either not prepared ( 2 ) or prepared by hoe ##ing 1 - or 2 - m diameter clear areas around the seedlings ( ( 3 ) and ( 4 ) respectively ) . plots were fertil ##ized with np ##k fertilizer ( 5 : 14 : 3 ) and treated with 100 g ald ##rin to prevent term ##ite attack . hoe ##ing and weed ##ing was done 20 days after planting and survival and growth measurements made 3 - monthly for 1 yr and at 19 and 30 month old . seedling survival increased with the amount of cultural practice and at 1 yr old was 100 % in ( 1 ) , 85 % in ( 4 ) , 20 % in ( 3 ) and only 5 % in ( 2 ) . growth measurements up to this age followed a similar pattern . at 30 month old , p . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 22362 732 13573 429 6484 118 205 13182 193 26229 137 2966 154 10102 121 111 17675 6317 579 29111 1025 205 6538 545 103 121 106 3303 235 309 10085 8077 1173 422 12000 237 129 13182 193 26229 13912 267 27755 121 24509 235 239 1375 286 127 12150 191 286 1910 131 4216 862 145 158 546 121 130 1590 2159 131 2966 154 10102 145 16759 10616 153 28545 19371 546 137 2338 487 1346 6656 5423 1814 137 145 170 579 286 546 121 130 1590 27755 190 2966 154 10102 121 26934 137 1676 302 4092 145 170 546 234 4092 214 25360 140 158 579 234 170 579 127 4381 1716 2326 2715 111 13912 145 145 239 546 137 145 286 546 1222 546 205 6784 267 9585 645 190 5242 30135 22289 145 305 862 1128 862 239 546 137 2338 190 1287 159 15261 6478 147 3363 902 585 3689 205 25360 140 137 27264 140 241 2992 1012 1976 647 26225 137 2842 137 1503 2354 1827 239 579 11762 168 158 10580 137 235 371 137 1339 1978 4289 205 23557 2842 1175 190 111 2407 131 6656 2794 137 235 158 10580 4289 241 1287 1863 121 145 158 546 422 6885 1863 121 145 286 546 422 1012 1863 121 145 239 546 137 617 305 1863 121 145 170 546 205 1503 2354 692 147 238 1407 2594 106 868 1377 205 235 1339 1978 4289 422 118 205 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 22362 732 13573 429 6484 118 205 13182 193 26229 137 2966 154 10102 121 111 17675 6317 579 29111 1025 205 6538 545 103 121 106 3303 235 309 10085 8077 1173 422 12000 237 129 13182 193 26229 13912 267 27755 121 24509 235 239 1375 286 127 12150 191 286 1910 131 4216 862 145 158 546 121 130 1590 2159 131 2966 154 10102 145 16759 10616 153 28545 19371 546 137 2338 487 1346 6656 5423 1814 137 145 170 579 286 546 121 130 1590 27755 190 2966 154 10102 121 26934 137 1676 302 4092 145 170 546 234 4092 214 25360 140 158 579 234 170 579 127 4381 1716 2326 2715 111 13912 145 145 239 546 137 145 286 546 1222 546 205 6784 267 9585 645 190 5242 30135 22289 145 305 862 1128 862 239 546 137 2338 190 1287 159 15261 6478 147 3363 902 585 3689 205 25360 140 137 27264 140 241 2992 1012 1976 647 26225 137 2842 137 1503 2354 1827 239 579 11762 168 158 10580 137 235 371 137 1339 1978 4289 205 23557 2842 1175 190 111 2407 131 6656 2794 137 235 158 10580 4289 241 1287 1863 121 145 158 546 422 6885 1863 121 145 286 546 422 1012 1863 121 145 239 546 137 617 305 1863 121 145 170 546 205 1503 2354 692 147 238 1407 2594 106 868 1377 205 235 1339 1978 4289 422 118 205 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] evaluation of two injection scler ##osa ##nts in the treatment of symptomatic haemorrh ##oids in nigeria ##ns [SEP] objectives : the objective of this study was to compare the efficacy and safety of 5 % phenol in al ##mond oil with 50 % dext ##rose water as scler ##osa ##nts in the treatment of first - to third - degree haemorrh ##oids . patients and methods : this was a prospective , comparative study conducted on eighty patients with first - , second - and third - degree haemorrh ##oids who consent ##ed to treatment by injection scler ##otherapy with either 5 % phenol in al ##mond oil or 50 % dext ##rose water . they were randomised equally into two groups . results : a total of eighty patients including 58 males and 22 females whose age ranged from 19 to 61 years were included in the study . bleeding was the chief symptom which was present in 78 ( 97 . 5 % ) patients , and there was complete resolution of this in 92 . 3 % and 89 . 7 % in the 5 % phenol in al ##mond oil and 50 % dext ##rose water groups , respectively , at 6 months ( p = 0 . 90 ##5 ) . similar degrees of resolution were noted for anal protr ##usion ( 89 . 1 % and 85 . 3 % in the 5 % phenol in al ##mond oil and 50 % dext ##rose water group , respectively [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] evaluation of two injection scler ##osa ##nts in the treatment of symptomatic haemorrh ##oids in nigeria ##ns [SEP] objectives : the objective of this study was to compare the efficacy and safety of 5 % phenol in al ##mond oil with 50 % dext ##rose water as scler ##osa ##nts in the treatment of first - to third - degree haemorrh ##oids . patients and methods : this was a prospective , comparative study conducted on eighty patients with first - , second - and third - degree haemorrh ##oids who consent ##ed to treatment by injection scler ##otherapy with either 5 % phenol in al ##mond oil or 50 % dext ##rose water . they were randomised equally into two groups . results : a total of eighty patients including 58 males and 22 females whose age ranged from 19 to 61 years were included in the study . bleeding was the chief symptom which was present in 78 ( 97 . 5 % ) patients , and there was complete resolution of this in 92 . 3 % and 89 . 7 % in the 5 % phenol in al ##mond oil and 50 % dext ##rose water groups , respectively , at 6 months ( p = 0 . 90 ##5 ) . similar degrees of resolution were noted for anal protr ##usion ( 89 . 1 % and 85 . 3 % in the 5 % phenol in al ##mond oil and 50 % dext ##rose water group , respectively [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 2166 131 502 4209 9518 4591 26219 121 111 922 131 12104 21047 4692 121 17964 1290 103 6454 862 111 3201 131 238 527 241 147 3745 111 4684 137 4104 131 305 1863 15302 121 186 14521 4981 190 1539 1863 28814 5718 1506 188 9518 4591 26219 121 111 922 131 705 579 147 2765 579 2460 21047 4692 205 568 137 1045 862 238 241 106 6880 422 6935 527 2728 191 27075 568 190 705 579 422 971 579 137 2765 579 2460 21047 4692 975 6670 119 147 922 214 4209 9518 3714 190 1676 305 1863 15302 121 186 14521 4981 234 1539 1863 28814 5718 1506 205 698 267 14136 6918 690 502 1302 205 545 862 106 1114 131 27075 568 1471 4878 5165 137 1931 5003 3489 1407 7650 263 371 147 7610 1320 267 1936 121 111 527 205 8401 241 111 16494 8316 334 241 709 121 7898 145 8276 205 305 1863 546 568 422 137 461 241 2467 3421 131 238 121 8698 205 239 1863 137 8765 205 450 1863 121 111 305 1863 15302 121 186 14521 4981 137 1539 1863 28814 5718 1506 1302 422 1222 422 235 370 2499 145 118 275 244 205 3289 30139 546 205 868 5634 131 3421 267 3742 168 438 16830 851 145 8765 205 158 1863 137 6885 205 239 1863 121 111 305 1863 15302 121 186 14521 4981 137 1539 1863 28814 5718 1506 583 422 1222 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 2166 131 502 4209 9518 4591 26219 121 111 922 131 12104 21047 4692 121 17964 1290 103 6454 862 111 3201 131 238 527 241 147 3745 111 4684 137 4104 131 305 1863 15302 121 186 14521 4981 190 1539 1863 28814 5718 1506 188 9518 4591 26219 121 111 922 131 705 579 147 2765 579 2460 21047 4692 205 568 137 1045 862 238 241 106 6880 422 6935 527 2728 191 27075 568 190 705 579 422 971 579 137 2765 579 2460 21047 4692 975 6670 119 147 922 214 4209 9518 3714 190 1676 305 1863 15302 121 186 14521 4981 234 1539 1863 28814 5718 1506 205 698 267 14136 6918 690 502 1302 205 545 862 106 1114 131 27075 568 1471 4878 5165 137 1931 5003 3489 1407 7650 263 371 147 7610 1320 267 1936 121 111 527 205 8401 241 111 16494 8316 334 241 709 121 7898 145 8276 205 305 1863 546 568 422 137 461 241 2467 3421 131 238 121 8698 205 239 1863 137 8765 205 450 1863 121 111 305 1863 15302 121 186 14521 4981 137 1539 1863 28814 5718 1506 1302 422 1222 422 235 370 2499 145 118 275 244 205 3289 30139 546 205 868 5634 131 3421 267 3742 168 438 16830 851 145 8765 205 158 1863 137 6885 205 239 1863 121 111 305 1863 15302 121 186 14521 4981 137 1539 1863 28814 5718 1506 583 422 1222 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Maryia_Ivanina\\AppData\\Local\\Continuum\\anaconda3\\envs\\cornell\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Maryia_Ivanina\\AppData\\Local\\Continuum\\anaconda3\\envs\\cornell\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used for model gpu 0\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Maryia_Ivanina\\AppData\\Local\\Continuum\\anaconda3\\envs\\cornell\\lib\\site-packages\\bert\\modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Maryia_Ivanina\\AppData\\Local\\Continuum\\anaconda3\\envs\\cornell\\lib\\site-packages\\bert\\modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Maryia_Ivanina\\AppData\\Local\\Continuum\\anaconda3\\envs\\cornell\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Maryia_Ivanina\\AppData\\Local\\Continuum\\anaconda3\\envs\\cornell\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../tmp/study_type_exps_3/study_type_with_tails_Observational study\\model.ckpt-7499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../tmp/study_type_exps_3/study_type_with_tails_Observational study\\model.ckpt-7499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] pea ##san ##ts and agriculture [SEP] imperial russia had an overwhelming ##ly pea ##san ##t population and its economy was largely agricultural . the russian empir ##e was not able to compete economically and mil ##ita ##ril ##y with the more  developed  states of north - western europe , north america and japan in the last few decades of its existence . over the preceding two or three centuries , however , russia  s auto ##cr ##atic state had been able successfully to exploit its pea ##san ##t population and agricultural economy to generate the resources , in particular tax revenues and military cons ##cripts , to consol ##idate and maintain its power at home and build a vast empir ##e that came to dominate eastern europe and northern asia . imperial russia  s pea ##san ##ts were , thus , at the bottom of an exploit ##ative social order . for much of the period , moreover , between a third and a half were the ser ##fs of noble land ##own ##ers in a system of bonded labour that emerged in the late sixteen ##th century and lasted until its abol ##ition in 186 ##1 . most of the rest of the pea ##san ##try lived on state lands and were subjected to slightly less one ##rous restrictions and demands . the sub ##ordinate and exploited status of all imperial russia  s pea ##san ##ts is one of the wider contexts in which their ways of life [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] pea ##san ##ts and agriculture [SEP] imperial russia had an overwhelming ##ly pea ##san ##t population and its economy was largely agricultural . the russian empir ##e was not able to compete economically and mil ##ita ##ril ##y with the more  developed  states of north - western europe , north america and japan in the last few decades of its existence . over the preceding two or three centuries , however , russia  s auto ##cr ##atic state had been able successfully to exploit its pea ##san ##t population and agricultural economy to generate the resources , in particular tax revenues and military cons ##cripts , to consol ##idate and maintain its power at home and build a vast empir ##e that came to dominate eastern europe and northern asia . imperial russia  s pea ##san ##ts were , thus , at the bottom of an exploit ##ative social order . for much of the period , moreover , between a third and a half were the ser ##fs of noble land ##own ##ers in a system of bonded labour that emerged in the late sixteen ##th century and lasted until its abol ##ition in 186 ##1 . most of the rest of the pea ##san ##try lived on state lands and were subjected to slightly less one ##rous restrictions and demands . the sub ##ordinate and exploited status of all imperial russia  s pea ##san ##ts is one of the wider contexts in which their ways of life [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 25038 17011 203 137 9879 103 27524 17094 883 130 22954 179 25038 17011 30108 1638 137 633 7425 241 5939 7675 205 111 14658 4153 30107 241 302 2357 147 17080 20003 137 3466 6351 4598 30126 190 111 475 1384 1815 5459 1898 131 4656 579 4539 3371 422 4656 7968 137 4838 121 111 2442 2149 8148 131 633 5074 205 573 111 10663 502 234 874 27232 422 694 422 17094 5459 112 5975 1304 757 1098 883 528 2357 5429 147 11091 633 25038 17011 30108 1638 137 7675 7425 147 3730 111 2965 422 121 1349 3764 22297 137 10945 401 20085 422 147 13181 23691 137 2603 633 1221 235 3417 137 2875 106 10946 4153 30107 198 10425 147 17893 10309 3371 137 7926 10705 205 27524 17094 5459 112 25038 17011 203 267 422 959 422 235 111 5473 131 130 11091 456 1748 993 205 168 1839 131 111 1275 422 2428 422 467 106 2765 137 106 3437 267 111 846 2373 131 28596 2882 1513 270 121 106 429 131 23152 10814 198 10036 121 111 4410 24158 266 8862 137 20376 3088 633 13721 319 121 18683 30130 205 755 131 111 2619 131 111 25038 17011 4272 18118 191 1098 24662 137 267 7178 147 4237 1279 482 9615 9129 137 8515 205 111 414 18147 137 13294 2726 131 355 27524 17094 5459 112 25038 17011 203 165 482 131 111 10147 8653 121 334 547 4568 131 1994 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 25038 17011 203 137 9879 103 27524 17094 883 130 22954 179 25038 17011 30108 1638 137 633 7425 241 5939 7675 205 111 14658 4153 30107 241 302 2357 147 17080 20003 137 3466 6351 4598 30126 190 111 475 1384 1815 5459 1898 131 4656 579 4539 3371 422 4656 7968 137 4838 121 111 2442 2149 8148 131 633 5074 205 573 111 10663 502 234 874 27232 422 694 422 17094 5459 112 5975 1304 757 1098 883 528 2357 5429 147 11091 633 25038 17011 30108 1638 137 7675 7425 147 3730 111 2965 422 121 1349 3764 22297 137 10945 401 20085 422 147 13181 23691 137 2603 633 1221 235 3417 137 2875 106 10946 4153 30107 198 10425 147 17893 10309 3371 137 7926 10705 205 27524 17094 5459 112 25038 17011 203 267 422 959 422 235 111 5473 131 130 11091 456 1748 993 205 168 1839 131 111 1275 422 2428 422 467 106 2765 137 106 3437 267 111 846 2373 131 28596 2882 1513 270 121 106 429 131 23152 10814 198 10036 121 111 4410 24158 266 8862 137 20376 3088 633 13721 319 121 18683 30130 205 755 131 111 2619 131 111 25038 17011 4272 18118 191 1098 24662 137 267 7178 147 4237 1279 482 9615 9129 137 8515 205 111 414 18147 137 13294 2726 131 355 27524 17094 5459 112 25038 17011 203 165 482 131 111 10147 8653 121 334 547 4568 131 1994 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] pea ##san ##ts and agriculture [SEP] of noble land ##own ##ers in a system of bonded labour that emerged in the late sixteen ##th century and lasted until its abol ##ition in 186 ##1 . most of the rest of the pea ##san ##try lived on state lands and were subjected to slightly less one ##rous restrictions and demands . the sub ##ordinate and exploited status of all imperial russia  s pea ##san ##ts is one of the wider contexts in which their ways of life can be examined . another wider context is the natural environment in which they lived and worked . in the northern half of russia , which was covered in forests , the soils were not very fertile , and the winter ##s long and cold . in the southern half , in contrast , the black earth of the step ##pes was very fertile , the climate warm ##er , but the rainfall was low and unreliable [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] pea ##san ##ts and agriculture [SEP] of noble land ##own ##ers in a system of bonded labour that emerged in the late sixteen ##th century and lasted until its abol ##ition in 186 ##1 . most of the rest of the pea ##san ##try lived on state lands and were subjected to slightly less one ##rous restrictions and demands . the sub ##ordinate and exploited status of all imperial russia  s pea ##san ##ts is one of the wider contexts in which their ways of life can be examined . another wider context is the natural environment in which they lived and worked . in the northern half of russia , which was covered in forests , the soils were not very fertile , and the winter ##s long and cold . in the southern half , in contrast , the black earth of the step ##pes was very fertile , the climate warm ##er , but the rainfall was low and unreliable [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 25038 17011 203 137 9879 103 131 28596 2882 1513 270 121 106 429 131 23152 10814 198 10036 121 111 4410 24158 266 8862 137 20376 3088 633 13721 319 121 18683 30130 205 755 131 111 2619 131 111 25038 17011 4272 18118 191 1098 24662 137 267 7178 147 4237 1279 482 9615 9129 137 8515 205 111 414 18147 137 13294 2726 131 355 27524 17094 5459 112 25038 17011 203 165 482 131 111 10147 8653 121 334 547 4568 131 1994 300 195 2959 205 1783 10147 2220 165 111 2404 1451 121 334 698 18118 137 11182 205 121 111 7926 3437 131 17094 422 334 241 7036 121 12514 422 111 10511 267 302 1248 30086 422 137 111 9305 30113 1113 137 6663 205 121 111 7663 3437 422 121 2144 422 111 3778 6732 131 111 1371 10829 241 1248 30086 422 111 6002 8591 114 422 563 111 12660 241 629 137 22923 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 25038 17011 203 137 9879 103 131 28596 2882 1513 270 121 106 429 131 23152 10814 198 10036 121 111 4410 24158 266 8862 137 20376 3088 633 13721 319 121 18683 30130 205 755 131 111 2619 131 111 25038 17011 4272 18118 191 1098 24662 137 267 7178 147 4237 1279 482 9615 9129 137 8515 205 111 414 18147 137 13294 2726 131 355 27524 17094 5459 112 25038 17011 203 165 482 131 111 10147 8653 121 334 547 4568 131 1994 300 195 2959 205 1783 10147 2220 165 111 2404 1451 121 334 698 18118 137 11182 205 121 111 7926 3437 131 17094 422 334 241 7036 121 12514 422 111 10511 267 302 1248 30086 422 137 111 9305 30113 1113 137 6663 205 121 111 7663 3437 422 121 2144 422 111 3778 6732 131 111 1371 10829 241 1248 30086 422 111 6002 8591 114 422 563 111 12660 241 629 137 22923 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] pea ##san ##ts and agriculture [SEP] and unreliable [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] pea ##san ##ts and agriculture [SEP] and unreliable [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 25038 17011 203 137 9879 103 137 22923 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 25038 17011 203 137 9879 103 137 22923 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] bean seed dissemination systems in mal ##awi : a strategy [SEP] farmers have shown considerable interest in purchasing seed of newly released dry bean ( phase ##olus vulgaris l . ) varieties in mal ##awi . however , this seed is not available on a large scale to farmers at present . the private seed sector in the country is not interested in bean seed production due to low profit margins . as a self - pollin ##ating crop , once farmers have obtained initial seed stock , it can be recycl ##ed for some time . a strategy has been developed to focus on alternative sustainable informal seed production and dissemination systems , which builds on lessons learn ##t from previous experiences within mal ##awi and other african countries . these include : ( i ) informal seed multiplication , using small ##holder farmers ; ( ii ) informal seed distribution channels , using gro ##cer ##y shop ##s , rural traders , extension agents , health clinics , and ng ##os ; and ( iii ) intens ##ified variety promotion through public ##ity , using poster ##s , leaf ##lets , bro ##ch ##ures and radio messages . these activities are carried out in close collaboration with farmers , ng ##os , extension agencies , village traders and various other institutions . the results from initial sales through informal distribution channels were encouraging and 80 % of the seed was sold [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] bean seed dissemination systems in mal ##awi : a strategy [SEP] farmers have shown considerable interest in purchasing seed of newly released dry bean ( phase ##olus vulgaris l . ) varieties in mal ##awi . however , this seed is not available on a large scale to farmers at present . the private seed sector in the country is not interested in bean seed production due to low profit margins . as a self - pollin ##ating crop , once farmers have obtained initial seed stock , it can be recycl ##ed for some time . a strategy has been developed to focus on alternative sustainable informal seed production and dissemination systems , which builds on lessons learn ##t from previous experiences within mal ##awi and other african countries . these include : ( i ) informal seed multiplication , using small ##holder farmers ; ( ii ) informal seed distribution channels , using gro ##cer ##y shop ##s , rural traders , extension agents , health clinics , and ng ##os ; and ( iii ) intens ##ified variety promotion through public ##ity , using poster ##s , leaf ##lets , bro ##ch ##ures and radio messages . these activities are carried out in close collaboration with farmers , ng ##os , extension agencies , village traders and various other institutions . the results from initial sales through informal distribution channels were encouraging and 80 % of the seed was sold [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 23022 3678 14265 1078 121 1774 18265 862 106 2691 103 12288 360 817 6132 1291 121 22085 3678 131 7737 7163 5965 23022 145 1481 24815 27039 152 205 546 13600 121 1774 18265 205 694 422 238 3678 165 302 1427 191 106 1135 2211 147 12288 235 709 205 111 5225 3678 5672 121 111 5192 165 302 7301 121 23022 3678 1865 1074 147 629 9047 14185 205 188 106 1968 579 26907 560 8501 422 3246 12288 360 1151 1700 3678 5426 422 256 300 195 13956 119 168 693 532 205 106 2691 434 528 1815 147 1790 191 3350 10886 13217 3678 1865 137 14265 1078 422 334 18619 191 15789 6714 30108 263 1061 6274 1017 1774 18265 137 494 7608 3198 205 407 2212 862 145 259 546 13217 3678 11957 422 487 952 13643 12288 1814 145 1264 546 13217 3678 1382 3994 422 487 419 736 30126 14260 30113 422 7046 29607 422 3840 3027 422 947 14862 422 137 3312 169 1814 137 145 2786 546 14733 1237 3835 11185 833 1771 208 422 487 18492 30113 422 5513 6660 422 2096 255 571 137 5203 5847 205 407 2687 220 3065 556 121 3156 7392 190 12288 422 3312 169 422 3840 11934 422 17719 29607 137 1711 494 7238 205 111 545 263 1700 10252 833 13217 1382 3994 267 14931 137 2833 1863 131 111 3678 241 17922 103 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 23022 3678 14265 1078 121 1774 18265 862 106 2691 103 12288 360 817 6132 1291 121 22085 3678 131 7737 7163 5965 23022 145 1481 24815 27039 152 205 546 13600 121 1774 18265 205 694 422 238 3678 165 302 1427 191 106 1135 2211 147 12288 235 709 205 111 5225 3678 5672 121 111 5192 165 302 7301 121 23022 3678 1865 1074 147 629 9047 14185 205 188 106 1968 579 26907 560 8501 422 3246 12288 360 1151 1700 3678 5426 422 256 300 195 13956 119 168 693 532 205 106 2691 434 528 1815 147 1790 191 3350 10886 13217 3678 1865 137 14265 1078 422 334 18619 191 15789 6714 30108 263 1061 6274 1017 1774 18265 137 494 7608 3198 205 407 2212 862 145 259 546 13217 3678 11957 422 487 952 13643 12288 1814 145 1264 546 13217 3678 1382 3994 422 487 419 736 30126 14260 30113 422 7046 29607 422 3840 3027 422 947 14862 422 137 3312 169 1814 137 145 2786 546 14733 1237 3835 11185 833 1771 208 422 487 18492 30113 422 5513 6660 422 2096 255 571 137 5203 5847 205 407 2687 220 3065 556 121 3156 7392 190 12288 422 3312 169 422 3840 11934 422 17719 29607 137 1711 494 7238 205 111 545 263 1700 10252 833 13217 1382 3994 267 14931 137 2833 1863 131 111 3678 241 17922 103 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] bean seed dissemination systems in mal ##awi : a strategy [SEP] variety promotion through public ##ity , using poster ##s , leaf ##lets , bro ##ch ##ures and radio messages . these activities are carried out in close collaboration with farmers , ng ##os , extension agencies , village traders and various other institutions . the results from initial sales through informal distribution channels were encouraging and 80 % of the seed was sold [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] bean seed dissemination systems in mal ##awi : a strategy [SEP] variety promotion through public ##ity , using poster ##s , leaf ##lets , bro ##ch ##ures and radio messages . these activities are carried out in close collaboration with farmers , ng ##os , extension agencies , village traders and various other institutions . the results from initial sales through informal distribution channels were encouraging and 80 % of the seed was sold [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 23022 3678 14265 1078 121 1774 18265 862 106 2691 103 3835 11185 833 1771 208 422 487 18492 30113 422 5513 6660 422 2096 255 571 137 5203 5847 205 407 2687 220 3065 556 121 3156 7392 190 12288 422 3312 169 422 3840 11934 422 17719 29607 137 1711 494 7238 205 111 545 263 1700 10252 833 13217 1382 3994 267 14931 137 2833 1863 131 111 3678 241 17922 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 23022 3678 14265 1078 121 1774 18265 862 106 2691 103 3835 11185 833 1771 208 422 487 18492 30113 422 5513 6660 422 2096 255 571 137 5203 5847 205 407 2687 220 3065 556 121 3156 7392 190 12288 422 3312 169 422 3840 11934 422 17719 29607 137 1711 494 7238 205 111 545 263 1700 10252 833 13217 1382 3994 267 14931 137 2833 1863 131 111 3678 241 17922 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used for model gpu 0\n",
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../tmp/study_type_exps_3/study_type_with_tails_Observational study\\model.ckpt-7499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../tmp/study_type_exps_3/study_type_with_tails_Observational study\\model.ckpt-7499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] pea ##san ##ts and agriculture [SEP] able successfully to exploit its pea ##san ##t population and agricultural economy to generate the resources , in particular tax revenues and military cons ##cripts , to consol ##idate and maintain its power at home and build a vast empir ##e that came to dominate eastern europe and northern asia . imperial russia  s pea ##san ##ts were , thus , at the bottom of an exploit ##ative social order . for much of the period , moreover , between a third and a half were the ser ##fs of noble land ##own ##ers in a system of bonded labour that emerged in the late sixteen ##th century and lasted until its abol ##ition in 186 ##1 . most of the rest of the pea ##san ##try lived on state lands and were subjected to slightly less one ##rous restrictions and demands . the sub ##ordinate and exploited status of all imperial russia  s pea ##san ##ts is one of the wider contexts in which their ways of life can be examined . another wider context is the natural environment in which they lived and worked . in the northern half of russia , which was covered in forests , the soils were not very fertile , and the winter ##s long and cold . in the southern half , in contrast , the black earth of the step ##pes was very fertile , the climate warm ##er , but the rainfall was low and unreliable [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] pea ##san ##ts and agriculture [SEP] able successfully to exploit its pea ##san ##t population and agricultural economy to generate the resources , in particular tax revenues and military cons ##cripts , to consol ##idate and maintain its power at home and build a vast empir ##e that came to dominate eastern europe and northern asia . imperial russia  s pea ##san ##ts were , thus , at the bottom of an exploit ##ative social order . for much of the period , moreover , between a third and a half were the ser ##fs of noble land ##own ##ers in a system of bonded labour that emerged in the late sixteen ##th century and lasted until its abol ##ition in 186 ##1 . most of the rest of the pea ##san ##try lived on state lands and were subjected to slightly less one ##rous restrictions and demands . the sub ##ordinate and exploited status of all imperial russia  s pea ##san ##ts is one of the wider contexts in which their ways of life can be examined . another wider context is the natural environment in which they lived and worked . in the northern half of russia , which was covered in forests , the soils were not very fertile , and the winter ##s long and cold . in the southern half , in contrast , the black earth of the step ##pes was very fertile , the climate warm ##er , but the rainfall was low and unreliable [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 25038 17011 203 137 9879 103 2357 5429 147 11091 633 25038 17011 30108 1638 137 7675 7425 147 3730 111 2965 422 121 1349 3764 22297 137 10945 401 20085 422 147 13181 23691 137 2603 633 1221 235 3417 137 2875 106 10946 4153 30107 198 10425 147 17893 10309 3371 137 7926 10705 205 27524 17094 5459 112 25038 17011 203 267 422 959 422 235 111 5473 131 130 11091 456 1748 993 205 168 1839 131 111 1275 422 2428 422 467 106 2765 137 106 3437 267 111 846 2373 131 28596 2882 1513 270 121 106 429 131 23152 10814 198 10036 121 111 4410 24158 266 8862 137 20376 3088 633 13721 319 121 18683 30130 205 755 131 111 2619 131 111 25038 17011 4272 18118 191 1098 24662 137 267 7178 147 4237 1279 482 9615 9129 137 8515 205 111 414 18147 137 13294 2726 131 355 27524 17094 5459 112 25038 17011 203 165 482 131 111 10147 8653 121 334 547 4568 131 1994 300 195 2959 205 1783 10147 2220 165 111 2404 1451 121 334 698 18118 137 11182 205 121 111 7926 3437 131 17094 422 334 241 7036 121 12514 422 111 10511 267 302 1248 30086 422 137 111 9305 30113 1113 137 6663 205 121 111 7663 3437 422 121 2144 422 111 3778 6732 131 111 1371 10829 241 1248 30086 422 111 6002 8591 114 422 563 111 12660 241 629 137 22923 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 25038 17011 203 137 9879 103 2357 5429 147 11091 633 25038 17011 30108 1638 137 7675 7425 147 3730 111 2965 422 121 1349 3764 22297 137 10945 401 20085 422 147 13181 23691 137 2603 633 1221 235 3417 137 2875 106 10946 4153 30107 198 10425 147 17893 10309 3371 137 7926 10705 205 27524 17094 5459 112 25038 17011 203 267 422 959 422 235 111 5473 131 130 11091 456 1748 993 205 168 1839 131 111 1275 422 2428 422 467 106 2765 137 106 3437 267 111 846 2373 131 28596 2882 1513 270 121 106 429 131 23152 10814 198 10036 121 111 4410 24158 266 8862 137 20376 3088 633 13721 319 121 18683 30130 205 755 131 111 2619 131 111 25038 17011 4272 18118 191 1098 24662 137 267 7178 147 4237 1279 482 9615 9129 137 8515 205 111 414 18147 137 13294 2726 131 355 27524 17094 5459 112 25038 17011 203 165 482 131 111 10147 8653 121 334 547 4568 131 1994 300 195 2959 205 1783 10147 2220 165 111 2404 1451 121 334 698 18118 137 11182 205 121 111 7926 3437 131 17094 422 334 241 7036 121 12514 422 111 10511 267 302 1248 30086 422 137 111 9305 30113 1113 137 6663 205 121 111 7663 3437 422 121 2144 422 111 3778 6732 131 111 1371 10829 241 1248 30086 422 111 6002 8591 114 422 563 111 12660 241 629 137 22923 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] bean seed dissemination systems in mal ##awi : a strategy [SEP] farmers have shown considerable interest in purchasing seed of newly released dry bean ( phase ##olus vulgaris l . ) varieties in mal ##awi . however , this seed is not available on a large scale to farmers at present . the private seed sector in the country is not interested in bean seed production due to low profit margins . as a self - pollin ##ating crop , once farmers have obtained initial seed stock , it can be recycl ##ed for some time . a strategy has been developed to focus on alternative sustainable informal seed production and dissemination systems , which builds on lessons learn ##t from previous experiences within mal ##awi and other african countries . these include : ( i ) informal seed multiplication , using small ##holder farmers ; ( ii ) informal seed distribution channels , using gro ##cer ##y shop ##s , rural traders , extension agents , health clinics , and ng ##os ; and ( iii ) intens ##ified variety promotion through public ##ity , using poster ##s , leaf ##lets , bro ##ch ##ures and radio messages . these activities are carried out in close collaboration with farmers , ng ##os , extension agencies , village traders and various other institutions . the results from initial sales through informal distribution channels were encouraging and 80 % of the seed was sold [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] bean seed dissemination systems in mal ##awi : a strategy [SEP] farmers have shown considerable interest in purchasing seed of newly released dry bean ( phase ##olus vulgaris l . ) varieties in mal ##awi . however , this seed is not available on a large scale to farmers at present . the private seed sector in the country is not interested in bean seed production due to low profit margins . as a self - pollin ##ating crop , once farmers have obtained initial seed stock , it can be recycl ##ed for some time . a strategy has been developed to focus on alternative sustainable informal seed production and dissemination systems , which builds on lessons learn ##t from previous experiences within mal ##awi and other african countries . these include : ( i ) informal seed multiplication , using small ##holder farmers ; ( ii ) informal seed distribution channels , using gro ##cer ##y shop ##s , rural traders , extension agents , health clinics , and ng ##os ; and ( iii ) intens ##ified variety promotion through public ##ity , using poster ##s , leaf ##lets , bro ##ch ##ures and radio messages . these activities are carried out in close collaboration with farmers , ng ##os , extension agencies , village traders and various other institutions . the results from initial sales through informal distribution channels were encouraging and 80 % of the seed was sold [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 23022 3678 14265 1078 121 1774 18265 862 106 2691 103 12288 360 817 6132 1291 121 22085 3678 131 7737 7163 5965 23022 145 1481 24815 27039 152 205 546 13600 121 1774 18265 205 694 422 238 3678 165 302 1427 191 106 1135 2211 147 12288 235 709 205 111 5225 3678 5672 121 111 5192 165 302 7301 121 23022 3678 1865 1074 147 629 9047 14185 205 188 106 1968 579 26907 560 8501 422 3246 12288 360 1151 1700 3678 5426 422 256 300 195 13956 119 168 693 532 205 106 2691 434 528 1815 147 1790 191 3350 10886 13217 3678 1865 137 14265 1078 422 334 18619 191 15789 6714 30108 263 1061 6274 1017 1774 18265 137 494 7608 3198 205 407 2212 862 145 259 546 13217 3678 11957 422 487 952 13643 12288 1814 145 1264 546 13217 3678 1382 3994 422 487 419 736 30126 14260 30113 422 7046 29607 422 3840 3027 422 947 14862 422 137 3312 169 1814 137 145 2786 546 14733 1237 3835 11185 833 1771 208 422 487 18492 30113 422 5513 6660 422 2096 255 571 137 5203 5847 205 407 2687 220 3065 556 121 3156 7392 190 12288 422 3312 169 422 3840 11934 422 17719 29607 137 1711 494 7238 205 111 545 263 1700 10252 833 13217 1382 3994 267 14931 137 2833 1863 131 111 3678 241 17922 103 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 23022 3678 14265 1078 121 1774 18265 862 106 2691 103 12288 360 817 6132 1291 121 22085 3678 131 7737 7163 5965 23022 145 1481 24815 27039 152 205 546 13600 121 1774 18265 205 694 422 238 3678 165 302 1427 191 106 1135 2211 147 12288 235 709 205 111 5225 3678 5672 121 111 5192 165 302 7301 121 23022 3678 1865 1074 147 629 9047 14185 205 188 106 1968 579 26907 560 8501 422 3246 12288 360 1151 1700 3678 5426 422 256 300 195 13956 119 168 693 532 205 106 2691 434 528 1815 147 1790 191 3350 10886 13217 3678 1865 137 14265 1078 422 334 18619 191 15789 6714 30108 263 1061 6274 1017 1774 18265 137 494 7608 3198 205 407 2212 862 145 259 546 13217 3678 11957 422 487 952 13643 12288 1814 145 1264 546 13217 3678 1382 3994 422 487 419 736 30126 14260 30113 422 7046 29607 422 3840 3027 422 947 14862 422 137 3312 169 1814 137 145 2786 546 14733 1237 3835 11185 833 1771 208 422 487 18492 30113 422 5513 6660 422 2096 255 571 137 5203 5847 205 407 2687 220 3065 556 121 3156 7392 190 12288 422 3312 169 422 3840 11934 422 17719 29607 137 1711 494 7238 205 111 545 263 1700 10252 833 13217 1382 3994 267 14931 137 2833 1863 131 111 3678 241 17922 103 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] small ##holders and niche markets : lessons from the and ##es [SEP] markets , which include organic , fair trade and go ##ur ##met markets . these offer greater benefits to farmers , by paying them a higher price for their produce , but demand new skills and forms of organisation . research in bol ##iv ##ia , peru and ecu ##ador focused on the extent to which producers of two widely trade ##d crops , ban ##ana ##s and coffee , are able to take advantage of expanding global markets . in ecu ##ador , ban ##ana production is largely in the hands of independent producers while a hand ##ful of companies control the export trade . these companies have considerable influence on the prices paid to farmers . the majority of farmers are hardly able to cover their production costs . coffee producers are confron ##ted with a market where over - supply in recent years has led to a fall in coffee prices . the ban ##ana and coffee sectors demonstrate that small ##holder producers often find themselves dise ##mp ##ower ##ed and unable to benefit from conventional commodity markets . several non - governmental organisations ( ng ##os ) are working with ban ##ana and coffee producers to tap into niche markets , which include organic , fair trade and go ##ur ##met markets . these offer greater benefits to farmers , by paying them a higher price for their produce , but demand new skills and forms of organisation [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] small ##holders and niche markets : lessons from the and ##es [SEP] markets , which include organic , fair trade and go ##ur ##met markets . these offer greater benefits to farmers , by paying them a higher price for their produce , but demand new skills and forms of organisation . research in bol ##iv ##ia , peru and ecu ##ador focused on the extent to which producers of two widely trade ##d crops , ban ##ana ##s and coffee , are able to take advantage of expanding global markets . in ecu ##ador , ban ##ana production is largely in the hands of independent producers while a hand ##ful of companies control the export trade . these companies have considerable influence on the prices paid to farmers . the majority of farmers are hardly able to cover their production costs . coffee producers are confron ##ted with a market where over - supply in recent years has led to a fall in coffee prices . the ban ##ana and coffee sectors demonstrate that small ##holder producers often find themselves dise ##mp ##ower ##ed and unable to benefit from conventional commodity markets . several non - governmental organisations ( ng ##os ) are working with ban ##ana and coffee producers to tap into niche markets , which include organic , fair trade and go ##ur ##met markets . these offer greater benefits to farmers , by paying them a higher price for their produce , but demand new skills and forms of organisation [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 952 10019 137 17433 7838 862 15789 263 111 137 123 103 7838 422 334 2212 4756 422 5623 4931 137 796 166 1566 7838 205 407 5066 2110 4376 147 12288 422 214 24558 1445 106 1001 4048 168 547 3299 422 563 3880 758 5561 137 3444 131 13202 205 849 121 8308 296 426 422 26460 137 29519 17544 4786 191 111 3796 147 334 15793 131 502 4276 4931 30118 12760 422 6550 4846 30113 137 17677 422 220 2357 147 2636 3944 131 13147 2523 7838 205 121 29519 17544 422 6550 4846 1865 165 5939 121 111 12053 131 2229 15793 969 106 1500 1004 131 6724 602 111 11484 4931 205 407 6724 360 6132 2496 191 111 6759 10955 147 12288 205 111 4185 131 12288 220 16136 2357 147 4144 547 1865 3266 205 17677 15793 220 17666 248 190 106 2753 582 573 579 4494 121 2151 1320 434 4030 147 106 3913 121 17677 6759 205 111 6550 4846 137 17677 11329 3768 198 952 13643 15793 1992 1040 5675 1035 5803 7946 119 137 8576 147 4541 263 3744 19821 7838 205 1323 699 579 26342 16526 145 3312 169 546 220 3630 190 6550 4846 137 17677 15793 147 10366 690 17433 7838 422 334 2212 4756 422 5623 4931 137 796 166 1566 7838 205 407 5066 2110 4376 147 12288 422 214 24558 1445 106 1001 4048 168 547 3299 422 563 3880 758 5561 137 3444 131 13202 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 952 10019 137 17433 7838 862 15789 263 111 137 123 103 7838 422 334 2212 4756 422 5623 4931 137 796 166 1566 7838 205 407 5066 2110 4376 147 12288 422 214 24558 1445 106 1001 4048 168 547 3299 422 563 3880 758 5561 137 3444 131 13202 205 849 121 8308 296 426 422 26460 137 29519 17544 4786 191 111 3796 147 334 15793 131 502 4276 4931 30118 12760 422 6550 4846 30113 137 17677 422 220 2357 147 2636 3944 131 13147 2523 7838 205 121 29519 17544 422 6550 4846 1865 165 5939 121 111 12053 131 2229 15793 969 106 1500 1004 131 6724 602 111 11484 4931 205 407 6724 360 6132 2496 191 111 6759 10955 147 12288 205 111 4185 131 12288 220 16136 2357 147 4144 547 1865 3266 205 17677 15793 220 17666 248 190 106 2753 582 573 579 4494 121 2151 1320 434 4030 147 106 3913 121 17677 6759 205 111 6550 4846 137 17677 11329 3768 198 952 13643 15793 1992 1040 5675 1035 5803 7946 119 137 8576 147 4541 263 3744 19821 7838 205 1323 699 579 26342 16526 145 3312 169 546 220 3630 190 6550 4846 137 17677 15793 147 10366 690 17433 7838 422 334 2212 4756 422 5623 4931 137 796 166 1566 7838 205 407 5066 2110 4376 147 12288 422 214 24558 1445 106 1001 4048 168 547 3299 422 563 3880 758 5561 137 3444 131 13202 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] agro ##fore ##stry system combining p . jul ##if ##lora and buff ##el grass in the brazilian semi - arid region . preliminary results [SEP] 2 - m diameter clear areas around the seedlings ( ( 3 ) and ( 4 ) respectively ) . plots were fertil ##ized with np ##k fertilizer ( 5 : 14 : 3 ) and treated with 100 g ald ##rin to prevent term ##ite attack . hoe ##ing and weed ##ing was done 20 days after planting and survival and growth measurements made 3 - monthly for 1 yr and at 19 and 30 month old . seedling survival increased with the amount of cultural practice and at 1 yr old was 100 % in ( 1 ) , 85 % in ( 4 ) , 20 % in ( 3 ) and only 5 % in ( 2 ) . growth measurements up to this age followed a similar pattern . at 30 month old , p . jul ##if ##lora only survived in treatments ( 1 ) and ( 4 ) , where values for survival and biomass production were 100 and 85 % and 17 66 ##9 and 275 ##7 kg / ha dm respectively . values for buff ##el grass production at 30 month old were 0 , 10 05 ##0 , 98 ##71 and 87 ##36 kg / ha dm respectively in treatments ( 1 - 4 ) ; the yield from buff ##el grass alone was 73 ##73 kg / ha [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] agro ##fore ##stry system combining p . jul ##if ##lora and buff ##el grass in the brazilian semi - arid region . preliminary results [SEP] 2 - m diameter clear areas around the seedlings ( ( 3 ) and ( 4 ) respectively ) . plots were fertil ##ized with np ##k fertilizer ( 5 : 14 : 3 ) and treated with 100 g ald ##rin to prevent term ##ite attack . hoe ##ing and weed ##ing was done 20 days after planting and survival and growth measurements made 3 - monthly for 1 yr and at 19 and 30 month old . seedling survival increased with the amount of cultural practice and at 1 yr old was 100 % in ( 1 ) , 85 % in ( 4 ) , 20 % in ( 3 ) and only 5 % in ( 2 ) . growth measurements up to this age followed a similar pattern . at 30 month old , p . jul ##if ##lora only survived in treatments ( 1 ) and ( 4 ) , where values for survival and biomass production were 100 and 85 % and 17 66 ##9 and 275 ##7 kg / ha dm respectively . values for buff ##el grass production at 30 month old were 0 , 10 05 ##0 , 98 ##71 and 87 ##36 kg / ha dm respectively in treatments ( 1 - 4 ) ; the yield from buff ##el grass alone was 73 ##73 kg / ha [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 22362 732 13573 429 6484 118 205 13182 193 26229 137 2966 154 10102 121 111 17675 6317 579 29111 1025 205 6538 545 103 170 579 127 4381 1716 2326 2715 111 13912 145 145 239 546 137 145 286 546 1222 546 205 6784 267 9585 645 190 5242 30135 22289 145 305 862 1128 862 239 546 137 2338 190 1287 159 15261 6478 147 3363 902 585 3689 205 25360 140 137 27264 140 241 2992 1012 1976 647 26225 137 2842 137 1503 2354 1827 239 579 11762 168 158 10580 137 235 371 137 1339 1978 4289 205 23557 2842 1175 190 111 2407 131 6656 2794 137 235 158 10580 4289 241 1287 1863 121 145 158 546 422 6885 1863 121 145 286 546 422 1012 1863 121 145 239 546 137 617 305 1863 121 145 170 546 205 1503 2354 692 147 238 1407 2594 106 868 1377 205 235 1339 1978 4289 422 118 205 13182 193 26229 617 19136 121 4728 145 158 546 137 145 286 546 422 582 988 168 2842 137 7537 1865 267 1287 137 6885 1863 137 1557 7107 30141 137 18887 30145 5036 1352 325 3546 1222 205 988 168 2966 154 10102 1865 235 1339 1978 4289 267 244 422 566 10764 30131 422 7871 6590 137 8468 2752 5036 1352 325 3546 1222 121 4728 145 158 579 286 546 1814 111 2210 263 2966 154 10102 3957 241 8095 5145 5036 1352 325 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 22362 732 13573 429 6484 118 205 13182 193 26229 137 2966 154 10102 121 111 17675 6317 579 29111 1025 205 6538 545 103 170 579 127 4381 1716 2326 2715 111 13912 145 145 239 546 137 145 286 546 1222 546 205 6784 267 9585 645 190 5242 30135 22289 145 305 862 1128 862 239 546 137 2338 190 1287 159 15261 6478 147 3363 902 585 3689 205 25360 140 137 27264 140 241 2992 1012 1976 647 26225 137 2842 137 1503 2354 1827 239 579 11762 168 158 10580 137 235 371 137 1339 1978 4289 205 23557 2842 1175 190 111 2407 131 6656 2794 137 235 158 10580 4289 241 1287 1863 121 145 158 546 422 6885 1863 121 145 286 546 422 1012 1863 121 145 239 546 137 617 305 1863 121 145 170 546 205 1503 2354 692 147 238 1407 2594 106 868 1377 205 235 1339 1978 4289 422 118 205 13182 193 26229 617 19136 121 4728 145 158 546 137 145 286 546 422 582 988 168 2842 137 7537 1865 267 1287 137 6885 1863 137 1557 7107 30141 137 18887 30145 5036 1352 325 3546 1222 205 988 168 2966 154 10102 1865 235 1339 1978 4289 267 244 422 566 10764 30131 422 7871 6590 137 8468 2752 5036 1352 325 3546 1222 121 4728 145 158 579 286 546 1814 111 2210 263 2966 154 10102 3957 241 8095 5145 5036 1352 325 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] evaluation of two injection scler ##osa ##nts in the treatment of symptomatic haemorrh ##oids in nigeria ##ns [SEP] eighty patients including 58 males and 22 females whose age ranged from 19 to 61 years were included in the study . bleeding was the chief symptom which was present in 78 ( 97 . 5 % ) patients , and there was complete resolution of this in 92 . 3 % and 89 . 7 % in the 5 % phenol in al ##mond oil and 50 % dext ##rose water groups , respectively , at 6 months ( p = 0 . 90 ##5 ) . similar degrees of resolution were noted for anal protr ##usion ( 89 . 1 % and 85 . 3 % in the 5 % phenol in al ##mond oil and 50 % dext ##rose water group , respectively , p = 0 . 89 ##9 ) . the overall complication rate was 3 . 6 % with anal mucosa ulcer ##ation being the only complication , occurring in three patients who had 5 % phenol in al ##mond oil . peri - procedure pain , patients satisfaction and acceptability were not significantly different in both groups ( p = 0 . 91 ##2 , 0 . 92 ##8 and 0 . 92 ##6 respectively ) . conclusion : five percent phenol in al ##mond oil and 50 % dext ##rose water are equally effective scler ##osa ##nts in the treatment of haemorrh ##oids with very low complication rates [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] evaluation of two injection scler ##osa ##nts in the treatment of symptomatic haemorrh ##oids in nigeria ##ns [SEP] eighty patients including 58 males and 22 females whose age ranged from 19 to 61 years were included in the study . bleeding was the chief symptom which was present in 78 ( 97 . 5 % ) patients , and there was complete resolution of this in 92 . 3 % and 89 . 7 % in the 5 % phenol in al ##mond oil and 50 % dext ##rose water groups , respectively , at 6 months ( p = 0 . 90 ##5 ) . similar degrees of resolution were noted for anal protr ##usion ( 89 . 1 % and 85 . 3 % in the 5 % phenol in al ##mond oil and 50 % dext ##rose water group , respectively , p = 0 . 89 ##9 ) . the overall complication rate was 3 . 6 % with anal mucosa ulcer ##ation being the only complication , occurring in three patients who had 5 % phenol in al ##mond oil . peri - procedure pain , patients satisfaction and acceptability were not significantly different in both groups ( p = 0 . 91 ##2 , 0 . 92 ##8 and 0 . 92 ##6 respectively ) . conclusion : five percent phenol in al ##mond oil and 50 % dext ##rose water are equally effective scler ##osa ##nts in the treatment of haemorrh ##oids with very low complication rates [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 2166 131 502 4209 9518 4591 26219 121 111 922 131 12104 21047 4692 121 17964 1290 103 27075 568 1471 4878 5165 137 1931 5003 3489 1407 7650 263 371 147 7610 1320 267 1936 121 111 527 205 8401 241 111 16494 8316 334 241 709 121 7898 145 8276 205 305 1863 546 568 422 137 461 241 2467 3421 131 238 121 8698 205 239 1863 137 8765 205 450 1863 121 111 305 1863 15302 121 186 14521 4981 137 1539 1863 28814 5718 1506 1302 422 1222 422 235 370 2499 145 118 275 244 205 3289 30139 546 205 868 5634 131 3421 267 3742 168 438 16830 851 145 8765 205 158 1863 137 6885 205 239 1863 121 111 305 1863 15302 121 186 14521 4981 137 1539 1863 28814 5718 1506 583 422 1222 422 118 275 244 205 8765 30141 546 205 111 2103 11100 1013 241 239 205 370 1863 190 438 12350 11737 150 1558 111 617 11100 422 6734 121 874 568 975 883 305 1863 15302 121 186 14521 4981 205 9447 579 2272 2675 422 568 6735 137 23634 267 302 1357 643 121 655 1302 145 118 275 244 205 9046 30132 422 244 205 8698 30146 137 244 205 8698 30142 1222 546 205 1705 862 2539 2163 15302 121 186 14521 4981 137 1539 1863 28814 5718 1506 220 6918 2115 9518 4591 26219 121 111 922 131 21047 4692 190 1248 629 11100 1975 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 2166 131 502 4209 9518 4591 26219 121 111 922 131 12104 21047 4692 121 17964 1290 103 27075 568 1471 4878 5165 137 1931 5003 3489 1407 7650 263 371 147 7610 1320 267 1936 121 111 527 205 8401 241 111 16494 8316 334 241 709 121 7898 145 8276 205 305 1863 546 568 422 137 461 241 2467 3421 131 238 121 8698 205 239 1863 137 8765 205 450 1863 121 111 305 1863 15302 121 186 14521 4981 137 1539 1863 28814 5718 1506 1302 422 1222 422 235 370 2499 145 118 275 244 205 3289 30139 546 205 868 5634 131 3421 267 3742 168 438 16830 851 145 8765 205 158 1863 137 6885 205 239 1863 121 111 305 1863 15302 121 186 14521 4981 137 1539 1863 28814 5718 1506 583 422 1222 422 118 275 244 205 8765 30141 546 205 111 2103 11100 1013 241 239 205 370 1863 190 438 12350 11737 150 1558 111 617 11100 422 6734 121 874 568 975 883 305 1863 15302 121 186 14521 4981 205 9447 579 2272 2675 422 568 6735 137 23634 267 302 1357 643 121 655 1302 145 118 275 244 205 9046 30132 422 244 205 8698 30146 137 244 205 8698 30142 1222 546 205 1705 862 2539 2163 15302 121 186 14521 4981 137 1539 1863 28814 5718 1506 220 6918 2115 9518 4591 26219 121 111 922 131 21047 4692 190 1248 629 11100 1975 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used for model gpu 0\n",
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../tmp/study_type_exps_3/study_type_with_tails_Observational study\\model.ckpt-7499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../tmp/study_type_exps_3/study_type_with_tails_Observational study\\model.ckpt-7499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-5b57b4c54307>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;31m#_study_type_labeler.train_model(train, test, use_tails=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m#_study_type_labeler.train_meta_model(train, test, use_tail=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_study_type_labeler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_with_meta_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_plus_stefan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwith_head_tail\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[0mtest_plus_stefan\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\cornell_project\\discovery\\src\\bert_models\\base_bert_model_meta_model_with_window_all.py\u001b[0m in \u001b[0;36mpredict_with_meta_model\u001b[1;34m(self, df, with_head_tail, recreate_model)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict_with_meta_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwith_head_tail\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecreate_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_dataset_for_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_tail\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwith_head_tail\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmeta_model_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'saved_model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mpredict_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[0msoftmax_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict_probs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\cornell\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    231\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmodel_config\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No model found in config file.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m     \u001b[0mmodel_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    234\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_from_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "from bert_models import base_bert_model_meta_model\n",
    "base_bert_model_meta_model = reload(base_bert_model_meta_model)\n",
    "from bert_models import base_bert_model_meta_model_with_window_all\n",
    "base_bert_model_meta_model_with_window_all = reload(base_bert_model_meta_model_with_window_all)\n",
    "from study_design_type import study_type_labeler\n",
    "study_type_labeler = reload(study_type_labeler)\n",
    "from study_design_type import study_type_labeler_window_agg\n",
    "study_type_labeler_window_agg = reload(study_type_labeler_window_agg)\n",
    "\n",
    "test_plus_stefan = excel_reader.ExcelReader().read_df_from_excel(\"../tmp/study_type_data/final_study_type_test_data.xlsx\")\n",
    "for label in StudyTypeLabels.STUDY_TYPE_LABEL_TO_NUMBER:\n",
    "    if label == \"No category\":\n",
    "        continue\n",
    "    print(label)\n",
    "    _study_type_labeler = study_type_labeler_window_agg.StudyTypeLabeler(\n",
    "        #\"/hdd2/data/maryia_pavlovets/study_type_exps_3/study_type_with_tails_%s\"%label,\n",
    "        \"../tmp/study_type_exps_3/study_type_with_tails_%s\"%label,\n",
    "        max_seq_length = 256, batch_size = 16,\n",
    "      gpu_device_num_hub = 0, gpu_device_num = 0,\n",
    "      model_folder = \"../tmp/scibert_scivocab_uncased\",\n",
    "      label_list = [0,1],\n",
    "      meta_model_folder = \"../tmp/study_type_exps_3/meta_model_%s_1layer_0.8_new_meta\"%label, #\"/hdd2/data/maryia_pavlovets/study_type_exps_3/meta_model_%s_1layer_0.8_new_meta\"%label,\n",
    "      use_one_layer=True,\n",
    "      keep_prob=0.8,\n",
    "      epochs_num=8\n",
    "      )\n",
    "    train, test = load_train_study_type(label, proportions_to_use=2)\n",
    "    print(train[\"label\"].value_counts())\n",
    "    print(test[\"label\"].value_counts())\n",
    "    #_study_type_labeler.train_model(train, test, use_tails=True)\n",
    "    #_study_type_labeler.train_meta_model(train, test, use_tail=True)\n",
    "    r = _study_type_labeler.predict_with_meta_model(test_plus_stefan, with_head_tail = True)\n",
    "    test_plus_stefan[label] = 0.0\n",
    "    for i in range(len(r[0])):\n",
    "        test_plus_stefan[label].values[i] = r[0][i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(doc):\n",
    "    return np.asarray(_study_type_labeler.predict_with_meta_model(pd.DataFrame([(\"title\", d) for d in doc], columns=[\"title\", \"abstract\"]), with_head_tail = True, recreate_model = True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observational study\n",
      "Modeling study\n"
     ]
    }
   ],
   "source": [
    "idx = 15\n",
    "text = test_plus_stefan[\"abstract\"].values[idx]\n",
    "for column in ['Observational study', 'Modeling study', 'Laboratory study', 'Review paper', 'Field study']:\n",
    "    if test_plus_stefan[column].values[idx] > 0.5:\n",
    "        print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "Used for model gpu 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "    \n",
       "\n",
       "        \n",
       "\n",
       "        \n",
       "    \n",
       "        \n",
       "        \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y=0\n",
       "    \n",
       "</b>\n",
       "\n",
       "    \n",
       "    (probability <b>0.615</b>, score <b>-0.470</b>)\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
       "                    Contribution<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.506\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        Highlighted in text (sum)\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 96.88%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.036\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        &lt;BIAS&gt;\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
       "        <span style=\"background-color: hsl(120, 100.00%, 83.04%); opacity: 0.86\" title=\"0.034\">this</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 77.85%); opacity: 0.89\" title=\"0.050\">study</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 75.90%); opacity: 0.90\" title=\"-0.057\">estimates</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 87.88%); opacity: 0.84\" title=\"-0.021\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.25%); opacity: 0.81\" title=\"-0.007\">magnitude</span><span style=\"opacity: 0.80\"> and determinants of </span><span style=\"background-color: hsl(0, 100.00%, 89.09%); opacity: 0.83\" title=\"-0.018\">transaction</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.39%); opacity: 0.83\" title=\"0.018\">costs</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 80.73%); opacity: 0.87\" title=\"-0.041\">incurred</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.14%); opacity: 0.81\" title=\"-0.008\">by</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 85.04%); opacity: 0.85\" title=\"0.029\">smallholder</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 72.63%); opacity: 0.92\" title=\"0.068\">arable</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 91.50%); opacity: 0.82\" title=\"-0.013\">crop</span><span style=\"opacity: 0.80\"> farm-</span><span style=\"background-color: hsl(120, 100.00%, 85.82%); opacity: 0.85\" title=\"0.027\">households</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 72.73%); opacity: 0.92\" title=\"0.068\">in</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 65.91%); opacity: 0.96\" title=\"0.093\">formal</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 98.16%); opacity: 0.80\" title=\"-0.001\">microcredit</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 89.51%); opacity: 0.83\" title=\"-0.017\">market</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 90.73%); opacity: 0.82\" title=\"-0.014\">in</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 90.73%); opacity: 0.82\" title=\"-0.014\">akwa</span><span style=\"opacity: 0.80\"> ibom </span><span style=\"background-color: hsl(0, 100.00%, 87.79%); opacity: 0.84\" title=\"-0.021\">state</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(120, 100.00%, 98.72%); opacity: 0.80\" title=\"0.001\">a</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 81.33%); opacity: 0.87\" title=\"0.039\">multi</span><span style=\"opacity: 0.80\">-</span><span style=\"background-color: hsl(0, 100.00%, 94.65%); opacity: 0.81\" title=\"-0.007\">stage</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 77.67%); opacity: 0.89\" title=\"-0.051\">sampling</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 78.39%); opacity: 0.88\" title=\"-0.049\">technique</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 78.89%); opacity: 0.88\" title=\"0.047\">was</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 79.08%); opacity: 0.88\" title=\"-0.046\">employed</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 81.49%); opacity: 0.87\" title=\"-0.039\">in</span><span style=\"opacity: 0.80\"> choosing </span><span style=\"background-color: hsl(0, 100.00%, 79.98%); opacity: 0.87\" title=\"-0.044\">108</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 79.98%); opacity: 0.87\" title=\"-0.044\">beneficiaries</span><span style=\"opacity: 0.80\"> of </span><span style=\"background-color: hsl(120, 100.00%, 83.06%); opacity: 0.86\" title=\"0.034\">formal</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 92.58%); opacity: 0.82\" title=\"0.011\">microcredit</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(0, 100.00%, 80.05%); opacity: 0.87\" title=\"-0.043\">a</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.48%); opacity: 0.81\" title=\"-0.009\">structured</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 79.21%); opacity: 0.88\" title=\"0.046\">questionnaire</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 79.31%); opacity: 0.88\" title=\"0.046\">was</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 83.74%); opacity: 0.86\" title=\"-0.032\">used</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 69.42%); opacity: 0.94\" title=\"-0.080\">for</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 75.18%); opacity: 0.90\" title=\"-0.059\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 71.04%); opacity: 0.93\" title=\"0.074\">collection</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 75.67%); opacity: 0.90\" title=\"0.058\">of</span><span style=\"opacity: 0.80\"> primary </span><span style=\"background-color: hsl(120, 100.00%, 91.74%); opacity: 0.82\" title=\"0.012\">data</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 83.35%); opacity: 0.86\" title=\"0.033\">from</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 83.35%); opacity: 0.86\" title=\"0.033\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 85.97%); opacity: 0.84\" title=\"0.026\">respondents</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(0, 100.00%, 94.51%); opacity: 0.81\" title=\"-0.007\">descriptive</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 87.68%); opacity: 0.84\" title=\"-0.022\">statistics</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 72.14%); opacity: 0.92\" title=\"0.070\">was</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.82%); opacity: 0.81\" title=\"-0.003\">used</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 84.24%); opacity: 0.85\" title=\"-0.031\">to</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 82.17%); opacity: 0.86\" title=\"-0.037\">calculate</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 97.97%); opacity: 0.80\" title=\"0.002\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.25%); opacity: 0.81\" title=\"-0.007\">magnitude</span><span style=\"opacity: 0.80\"> of </span><span style=\"background-color: hsl(0, 100.00%, 89.09%); opacity: 0.83\" title=\"-0.018\">transaction</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.39%); opacity: 0.83\" title=\"0.018\">costs</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 80.73%); opacity: 0.87\" title=\"-0.041\">incurred</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 81.54%); opacity: 0.87\" title=\"0.039\">by</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 79.09%); opacity: 0.88\" title=\"0.046\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 85.82%); opacity: 0.85\" title=\"0.027\">households</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.43%); opacity: 0.82\" title=\"0.009\">in</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 82.68%); opacity: 0.86\" title=\"-0.035\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 82.19%); opacity: 0.86\" title=\"-0.037\">microcredit</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.71%); opacity: 0.81\" title=\"0.008\">market</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.05%); opacity: 0.80\" title=\"-0.003\">and</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 82.66%); opacity: 0.86\" title=\"-0.035\">multiple</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 88.10%); opacity: 0.84\" title=\"-0.021\">regression</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.35%); opacity: 0.83\" title=\"0.018\">technique</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 78.89%); opacity: 0.88\" title=\"0.047\">was</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 79.08%); opacity: 0.88\" title=\"-0.046\">employed</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 81.49%); opacity: 0.87\" title=\"-0.039\">in</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.67%); opacity: 0.82\" title=\"0.012\">determining</span><span style=\"opacity: 0.80\"> the </span><span style=\"background-color: hsl(0, 100.00%, 87.43%); opacity: 0.84\" title=\"-0.022\">significant</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 77.62%); opacity: 0.89\" title=\"0.051\">factors</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.89%); opacity: 0.81\" title=\"-0.005\">that</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.24%); opacity: 0.81\" title=\"-0.006\">influence</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 77.95%); opacity: 0.89\" title=\"0.050\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 89.09%); opacity: 0.83\" title=\"-0.018\">transaction</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.39%); opacity: 0.83\" title=\"0.018\">costs</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 80.73%); opacity: 0.87\" title=\"-0.041\">incurred</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 85.41%); opacity: 0.85\" title=\"0.028\">by</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 74.41%); opacity: 0.91\" title=\"0.062\">households</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.43%); opacity: 0.82\" title=\"0.009\">in</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 69.73%); opacity: 0.93\" title=\"-0.079\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 69.73%); opacity: 0.93\" title=\"-0.079\">market</span><span style=\"opacity: 0.80\">. the </span><span style=\"background-color: hsl(0, 100.00%, 89.11%); opacity: 0.83\" title=\"-0.018\">result</span><span style=\"opacity: 0.80\"> obtained showed </span><span style=\"background-color: hsl(120, 100.00%, 90.15%); opacity: 0.83\" title=\"0.016\">that</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 67.62%); opacity: 0.95\" title=\"0.087\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 71.89%); opacity: 0.92\" title=\"0.071\">most</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 83.05%); opacity: 0.86\" title=\"0.034\">important</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 72.78%); opacity: 0.92\" title=\"0.068\">transaction</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 80.09%); opacity: 0.87\" title=\"-0.043\">cost</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 83.55%); opacity: 0.86\" title=\"0.033\">component</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 86.49%); opacity: 0.84\" title=\"0.025\">was</span><span style=\"opacity: 0.80\"> opportunity </span><span style=\"background-color: hsl(0, 100.00%, 98.55%); opacity: 0.80\" title=\"-0.001\">cost</span><span style=\"opacity: 0.80\"> of </span><span style=\"background-color: hsl(120, 100.00%, 83.70%); opacity: 0.86\" title=\"0.032\">time</span><span style=\"opacity: 0.80\"> (</span><span style=\"background-color: hsl(120, 100.00%, 76.92%); opacity: 0.89\" title=\"0.053\">naira</span><span style=\"opacity: 0.80\"> 5576.</span><span style=\"background-color: hsl(120, 100.00%, 89.69%); opacity: 0.83\" title=\"0.017\">41</span><span style=\"opacity: 0.80\">), </span><span style=\"background-color: hsl(0, 100.00%, 96.69%); opacity: 0.81\" title=\"-0.003\">followed</span><span style=\"opacity: 0.80\"> by </span><span style=\"background-color: hsl(0, 100.00%, 84.93%); opacity: 0.85\" title=\"-0.029\">travelling</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 84.48%); opacity: 0.85\" title=\"0.030\">cost</span><span style=\"opacity: 0.80\"> (</span><span style=\"background-color: hsl(120, 100.00%, 88.03%); opacity: 0.84\" title=\"0.021\">naira</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 92.07%); opacity: 0.82\" title=\"-0.012\">3272</span><span style=\"opacity: 0.80\">.62). </span><span style=\"background-color: hsl(120, 100.00%, 73.08%); opacity: 0.91\" title=\"0.066\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 68.70%); opacity: 0.94\" title=\"0.082\">study</span><span style=\"opacity: 0.80\"> also showed that </span><span style=\"background-color: hsl(0, 100.00%, 79.30%); opacity: 0.88\" title=\"-0.046\">loan</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 92.08%); opacity: 0.82\" title=\"0.012\">size</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(120, 100.00%, 66.44%); opacity: 0.96\" title=\"0.091\">interest</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 78.59%); opacity: 0.88\" title=\"0.048\">cost</span><span style=\"opacity: 0.80\"> and waiting </span><span style=\"background-color: hsl(120, 100.00%, 89.40%); opacity: 0.83\" title=\"0.018\">period</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 88.12%); opacity: 0.84\" title=\"-0.021\">had</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.82%); opacity: 0.83\" title=\"0.017\">significant</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 88.69%); opacity: 0.83\" title=\"0.019\">positive</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 81.23%); opacity: 0.87\" title=\"-0.040\">relationship</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 86.65%); opacity: 0.84\" title=\"-0.024\">with</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.77%); opacity: 0.81\" title=\"0.005\">microcredit</span><span style=\"opacity: 0.80\"> transaction </span><span style=\"background-color: hsl(0, 100.00%, 98.55%); opacity: 0.80\" title=\"-0.001\">cost</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 86.84%); opacity: 0.84\" title=\"0.024\">at</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 85.67%); opacity: 0.85\" title=\"-0.027\">1</span><span style=\"opacity: 0.80\">%; </span><span style=\"background-color: hsl(0, 100.00%, 96.41%); opacity: 0.81\" title=\"-0.004\">distance</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 98.62%); opacity: 0.80\" title=\"-0.001\">to</span><span style=\"opacity: 0.80\"> lending </span><span style=\"background-color: hsl(120, 100.00%, 82.51%); opacity: 0.86\" title=\"0.036\">institution</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 71.81%); opacity: 0.92\" title=\"0.071\">and</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 78.13%); opacity: 0.88\" title=\"0.049\">savings</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 87.75%); opacity: 0.84\" title=\"-0.022\">were</span><span style=\"opacity: 0.80\"> both </span><span style=\"background-color: hsl(120, 100.00%, 84.82%); opacity: 0.85\" title=\"0.029\">positively</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.14%); opacity: 0.81\" title=\"0.004\">signed</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.74%); opacity: 0.83\" title=\"0.017\">but</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 87.18%); opacity: 0.84\" title=\"-0.023\">significantly</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.79%); opacity: 0.81\" title=\"0.005\">related</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 85.40%); opacity: 0.85\" title=\"0.028\">to</span><span style=\"opacity: 0.80\"> transaction </span><span style=\"background-color: hsl(0, 100.00%, 98.55%); opacity: 0.80\" title=\"-0.001\">cost</span><span style=\"opacity: 0.80\"> at </span><span style=\"background-color: hsl(0, 100.00%, 92.52%); opacity: 0.82\" title=\"-0.011\">10</span><span style=\"opacity: 0.80\">% while education </span><span style=\"background-color: hsl(0, 100.00%, 88.04%); opacity: 0.84\" title=\"-0.021\">had</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 98.13%); opacity: 0.80\" title=\"0.001\">a</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 87.43%); opacity: 0.84\" title=\"-0.022\">significant</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 78.10%); opacity: 0.88\" title=\"-0.050\">negative</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 81.23%); opacity: 0.87\" title=\"-0.040\">relationship</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 86.56%); opacity: 0.84\" title=\"-0.025\">with</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 85.05%); opacity: 0.85\" title=\"-0.029\">transaction</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 98.55%); opacity: 0.80\" title=\"-0.001\">cost</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 86.84%); opacity: 0.84\" title=\"0.024\">at</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 85.67%); opacity: 0.85\" title=\"-0.027\">1</span><span style=\"opacity: 0.80\">%. </span><span style=\"background-color: hsl(120, 100.00%, 85.89%); opacity: 0.85\" title=\"0.026\">to</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 90.97%); opacity: 0.82\" title=\"0.014\">drastically</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 86.05%); opacity: 0.84\" title=\"-0.026\">reduce</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 77.70%); opacity: 0.89\" title=\"0.051\">microcredit</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 89.09%); opacity: 0.83\" title=\"-0.018\">transaction</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 82.94%); opacity: 0.86\" title=\"0.035\">costs</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(120, 100.00%, 81.39%); opacity: 0.87\" title=\"0.039\">policies</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.00%); opacity: 0.81\" title=\"-0.006\">that</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 82.13%); opacity: 0.86\" title=\"0.037\">will</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 81.72%); opacity: 0.87\" title=\"0.038\">enable</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 83.71%); opacity: 0.86\" title=\"-0.032\">farmers</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 88.42%); opacity: 0.83\" title=\"0.020\">to</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 81.27%); opacity: 0.87\" title=\"0.040\">attain</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 85.19%); opacity: 0.85\" title=\"0.028\">higher</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 75.43%); opacity: 0.90\" title=\"0.058\">level</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 84.46%); opacity: 0.85\" title=\"0.030\">of</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 84.46%); opacity: 0.85\" title=\"0.030\">education</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 81.66%); opacity: 0.87\" title=\"0.038\">are</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 90.32%); opacity: 0.83\" title=\"0.015\">advocated</span><span style=\"opacity: 0.80\">; </span><span style=\"background-color: hsl(0, 100.00%, 75.93%); opacity: 0.90\" title=\"-0.057\">credit</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 81.44%); opacity: 0.87\" title=\"0.039\">administrative</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 81.44%); opacity: 0.87\" title=\"0.039\">procedure</span><span style=\"opacity: 0.80\"> that is less time </span><span style=\"background-color: hsl(0, 100.00%, 89.22%); opacity: 0.83\" title=\"-0.018\">wasting</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.00%); opacity: 0.82\" title=\"-0.010\">should</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 60.00%); opacity: 1.00\" title=\"-0.117\">be</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 76.80%); opacity: 0.89\" title=\"-0.054\">put</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 86.67%); opacity: 0.84\" title=\"0.024\">in</span><span style=\"opacity: 0.80\"> place </span><span style=\"background-color: hsl(0, 100.00%, 82.21%); opacity: 0.86\" title=\"-0.037\">in</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 92.55%); opacity: 0.82\" title=\"-0.011\">financial</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 86.73%); opacity: 0.84\" title=\"0.024\">institutions</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 76.66%); opacity: 0.89\" title=\"-0.054\">that</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 70.71%); opacity: 0.93\" title=\"-0.075\">offer</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.70%); opacity: 0.82\" title=\"0.012\">microcredit</span><span style=\"opacity: 0.80\"> to </span><span style=\"background-color: hsl(120, 100.00%, 85.63%); opacity: 0.85\" title=\"0.027\">ensure</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 68.07%); opacity: 0.94\" title=\"0.085\">timely</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.26%); opacity: 0.80\" title=\"-0.003\">disbursement</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 79.21%); opacity: 0.88\" title=\"-0.046\">of</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.41%); opacity: 0.80\" title=\"-0.000\">microcredit</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 98.26%); opacity: 0.80\" title=\"0.001\">facilities</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.72%); opacity: 0.81\" title=\"0.003\">to</span><span style=\"opacity: 0.80\"> farm households; and sanction in </span><span style=\"background-color: hsl(0, 100.00%, 79.61%); opacity: 0.88\" title=\"-0.045\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 74.66%); opacity: 0.90\" title=\"-0.061\">form</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 80.21%); opacity: 0.87\" title=\"-0.043\">of</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 77.37%); opacity: 0.89\" title=\"-0.052\">heavy</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 92.73%); opacity: 0.82\" title=\"0.010\">monetary</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 76.88%); opacity: 0.89\" title=\"0.053\">fine</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.74%); opacity: 0.81\" title=\"0.008\">should</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 79.56%); opacity: 0.88\" title=\"-0.045\">be</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 82.55%); opacity: 0.86\" title=\"-0.036\">meted</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 87.73%); opacity: 0.84\" title=\"0.022\">out</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.47%); opacity: 0.81\" title=\"0.005\">to</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 84.08%); opacity: 0.85\" title=\"0.031\">financial</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 86.73%); opacity: 0.84\" title=\"0.024\">institutions</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 84.96%); opacity: 0.85\" title=\"0.029\">that</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 84.96%); opacity: 0.85\" title=\"0.029\">fail</span><span style=\"opacity: 0.80\"> to </span><span style=\"background-color: hsl(120, 100.00%, 85.63%); opacity: 0.85\" title=\"0.027\">ensure</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 68.07%); opacity: 0.94\" title=\"0.085\">timely</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.26%); opacity: 0.80\" title=\"-0.003\">disbursement</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 79.21%); opacity: 0.88\" title=\"-0.046\">of</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.41%); opacity: 0.80\" title=\"-0.000\">microcredit</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 98.26%); opacity: 0.80\" title=\"0.001\">facilities</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 87.67%); opacity: 0.84\" title=\"0.022\">to</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.01%); opacity: 0.83\" title=\"0.018\">farmers</span>\n",
       "    </p>\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bert_models import base_bert_model_meta_model\n",
    "base_bert_model_meta_model = reload(base_bert_model_meta_model)\n",
    "from study_design_type import study_type_labeler\n",
    "study_type_labeler = reload(study_type_labeler)\n",
    "import eli5\n",
    "from eli5.lime import TextExplainer\n",
    "\n",
    "label = \"Observational study\"\n",
    "_study_type_labeler = study_type_labeler.StudyTypeLabeler(\n",
    "            \"/hdd2/data/maryia_pavlovets/study_type_exps/study_type_with_tails_%s\"%label,\n",
    "            max_seq_length = 256, batch_size = 16,\n",
    "          gpu_device_num_hub = 0, gpu_device_num = 0,\n",
    "          model_folder = \"../model/scibert_scivocab_uncased\",\n",
    "          label_list = [0,1],\n",
    "          meta_model_folder = \"/hdd2/data/maryia_pavlovets/study_type_exps/meta_model_%s_2layer_0.8\"%label,\n",
    "          use_one_layer=False,\n",
    "          keep_prob=0.8,\n",
    "          epochs_num=8\n",
    "          )\n",
    "te = TextExplainer(random_state=42)\n",
    "te.fit(text, predict_proba)\n",
    "te.show_prediction(target_names=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read file ../tmp/study_type_data/final_study_type_test_data.xlsx: 0.04s\n",
      "Processed file ../tmp/study_type_data/final_study_type_test_data.xlsx: 0.01s\n"
     ]
    }
   ],
   "source": [
    "test_plus_stefan_true = excel_reader.ExcelReader().read_df_from_excel(\"../tmp/study_type_data/final_study_type_test_data.xlsx\")\n",
    "results_true = []\n",
    "for i in range(len(test_plus_stefan_true)):\n",
    "    results_ind = []\n",
    "    for column in ['Observational study', 'Modeling study', 'Laboratory study', 'Review paper', 'Field study']:\n",
    "        if test_plus_stefan_true[column].values[i] == 1:\n",
    "            results_ind.append(column)\n",
    "    if not results_ind:\n",
    "        results_ind.append(\"No category\")\n",
    "    results_true.append(results_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = []\n",
    "for i in range(len(result[0])):\n",
    "    results_ind = []\n",
    "    for idx, column in enumerate(['Observational study', 'Modeling study', 'Laboratory study', 'Review paper', 'Field study']):\n",
    "        if result[0][i][idx] >= 0.5:\n",
    "            results_ind.append(column)\n",
    "    if not results_ind:\n",
    "        results_ind.append(\"No category\")\n",
    "    model_results.append(results_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = []\n",
    "for i in range(len(result)):\n",
    "    results_ind = []\n",
    "    for column in ['Observational study', 'Modeling study', 'Laboratory study', 'Review paper', 'Field study']:\n",
    "        if test_plus_stefan[column].values[i] >= 0.5:\n",
    "            results_ind.append(column)\n",
    "    if not results_ind:\n",
    "        results_ind.append(\"No category\")\n",
    "    model_results.append(results_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observational study\n",
      "[[136   6]\n",
      " [ 19  89]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.96      0.92       142\n",
      "           1       0.94      0.82      0.88       108\n",
      "\n",
      "   micro avg       0.90      0.90      0.90       250\n",
      "   macro avg       0.91      0.89      0.90       250\n",
      "weighted avg       0.90      0.90      0.90       250\n",
      "\n",
      "F1 score:  0.8963361032326549\n",
      "Modeling study\n",
      "[[181   9]\n",
      " [  9  51]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95       190\n",
      "           1       0.85      0.85      0.85        60\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       250\n",
      "   macro avg       0.90      0.90      0.90       250\n",
      "weighted avg       0.93      0.93      0.93       250\n",
      "\n",
      "F1 score:  0.9013157894736841\n",
      "Laboratory study\n",
      "[[216   0]\n",
      " [  6  28]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99       216\n",
      "           1       1.00      0.82      0.90        34\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       250\n",
      "   macro avg       0.99      0.91      0.94       250\n",
      "weighted avg       0.98      0.98      0.98       250\n",
      "\n",
      "F1 score:  0.9447635881573133\n",
      "Review paper\n",
      "[[212   4]\n",
      " [  2  32]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       216\n",
      "           1       0.89      0.94      0.91        34\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       250\n",
      "   macro avg       0.94      0.96      0.95       250\n",
      "weighted avg       0.98      0.98      0.98       250\n",
      "\n",
      "F1 score:  0.9501661129568106\n",
      "Field study\n",
      "[[218   0]\n",
      " [  8  24]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       218\n",
      "           1       1.00      0.75      0.86        32\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       250\n",
      "   macro avg       0.98      0.88      0.92       250\n",
      "weighted avg       0.97      0.97      0.97       250\n",
      "\n",
      "F1 score:  0.9195624195624195\n",
      "No category\n",
      "[[210  10]\n",
      " [  7  23]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96       220\n",
      "           1       0.70      0.77      0.73        30\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       250\n",
      "   macro avg       0.83      0.86      0.85       250\n",
      "weighted avg       0.94      0.93      0.93       250\n",
      "\n",
      "F1 score:  0.8456285641640333\n",
      "0.9 0.8566666666666667 0.8671999999999995\n",
      "[[24  0  0  2  1  0]\n",
      " [ 0 28  2  1  0  0]\n",
      " [ 0  0 35  2  0  0]\n",
      " [ 0  0  0 23  4  3]\n",
      " [ 0  0  2  3 88  0]\n",
      " [ 0  0  0  2  0 30]]\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "        Field study       1.00      0.89      0.94        27\n",
      "   Laboratory study       1.00      0.90      0.95        31\n",
      "     Modeling study       0.90      0.95      0.92        37\n",
      "        No category       0.70      0.77      0.73        30\n",
      "Observational study       0.95      0.95      0.95        93\n",
      "       Review paper       0.91      0.94      0.92        32\n",
      "\n",
      "          micro avg       0.91      0.91      0.91       250\n",
      "          macro avg       0.91      0.90      0.90       250\n",
      "       weighted avg       0.92      0.91      0.91       250\n",
      "\n",
      "F1 score:  0.9018089761525837\n"
     ]
    }
   ],
   "source": [
    "#final model with agg meta\n",
    "calculate_metrics(results_true, model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def calculate_metrics(results_true, model_results):\n",
    "    for label in StudyTypeLabels.STUDY_TYPE_LABEL_TO_NUMBER:\n",
    "        print(label)\n",
    "        res_y = [(1 if label in labels else 0) for labels in model_results]\n",
    "        test_y = [(1 if label in labels else 0) for labels in results_true]\n",
    "        print(confusion_matrix(test_y, res_y))\n",
    "        print(classification_report(test_y, res_y))\n",
    "        print(\"F1 score: \",f1_score(test_y, res_y, average=\"macro\"))\n",
    "\n",
    "    cnt = len(results_true)\n",
    "    cnt_precision = 0\n",
    "    cnt_recall = 0\n",
    "    cnt_f1 = 0\n",
    "    cnt_some_not_found = 0\n",
    "    test_y_one = []\n",
    "    res_y_one = []\n",
    "    for i in range(len(results_true)):\n",
    "        cnt_intersect = len(set(results_true[i]).intersection(set(model_results[i])))\n",
    "        if cnt_intersect > 0:\n",
    "            intersected = list(set(results_true[i]).intersection(set(model_results[i])))\n",
    "            priority_labels_data = [\n",
    "                (label, priority_labels[label]) for label in intersected]\n",
    "            if len(priority_labels_data):\n",
    "                result_label = sorted(priority_labels_data, key=lambda x: x[1])[0][0]\n",
    "            else:\n",
    "                result_label = 5\n",
    "            test_y_one.append(result_label)\n",
    "            res_y_one.append(result_label)\n",
    "        else:\n",
    "            priority_labels_data = [\n",
    "                (label, priority_labels[label]) for label in results_true[i]]\n",
    "            if len(priority_labels_data):\n",
    "                result_label = sorted(priority_labels_data, key=lambda x: x[1])[0][0]\n",
    "            else:\n",
    "                result_label = 5\n",
    "            test_y_one.append(result_label)\n",
    "            priority_labels_data = [\n",
    "                (label, priority_labels[label]) for label in model_results[i]]\n",
    "            if len(priority_labels_data):\n",
    "                result_label = sorted(priority_labels_data, key=lambda x: x[1])[0][0]\n",
    "            else:\n",
    "                result_label = 5\n",
    "            res_y_one.append(result_label)\n",
    "        cnt_correct = len(results_true[i])\n",
    "        cnt_found = len(model_results[i])\n",
    "        precision = 0 if cnt_found == 0 else (cnt_intersect/cnt_found)\n",
    "        recall = 0 if cnt_correct == 0 else (cnt_intersect/cnt_correct)\n",
    "        f1 = 0\n",
    "        if (precision + recall) > 0:\n",
    "            f1 = 2*precision*recall/(precision + recall)\n",
    "        cnt_precision += precision\n",
    "        cnt_recall += recall\n",
    "        cnt_f1 += f1\n",
    "    print(cnt_precision/cnt, cnt_recall/cnt, cnt_f1/cnt)\n",
    "    print(confusion_matrix(test_y_one, res_y_one))\n",
    "    print(classification_report(test_y_one, res_y_one))\n",
    "    print(\"F1 score: \",f1_score(test_y_one, res_y_one, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observational study\n",
      "[[133   9]\n",
      " [ 39  69]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.94      0.85       142\n",
      "           1       0.88      0.64      0.74       108\n",
      "\n",
      "   micro avg       0.81      0.81      0.81       250\n",
      "   macro avg       0.83      0.79      0.79       250\n",
      "weighted avg       0.82      0.81      0.80       250\n",
      "\n",
      "F1 score:  0.7945346209163755\n",
      "Modeling study\n",
      "[[175  15]\n",
      " [  8  52]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94       190\n",
      "           1       0.78      0.87      0.82        60\n",
      "\n",
      "   micro avg       0.91      0.91      0.91       250\n",
      "   macro avg       0.87      0.89      0.88       250\n",
      "weighted avg       0.91      0.91      0.91       250\n",
      "\n",
      "F1 score:  0.8786177197019274\n",
      "Laboratory study\n",
      "[[207   9]\n",
      " [  2  32]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.97       216\n",
      "           1       0.78      0.94      0.85        34\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       250\n",
      "   macro avg       0.89      0.95      0.91       250\n",
      "weighted avg       0.96      0.96      0.96       250\n",
      "\n",
      "F1 score:  0.9137254901960785\n",
      "Review paper\n",
      "[[212   4]\n",
      " [  6  28]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98       216\n",
      "           1       0.88      0.82      0.85        34\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       250\n",
      "   macro avg       0.92      0.90      0.91       250\n",
      "weighted avg       0.96      0.96      0.96       250\n",
      "\n",
      "F1 score:  0.9127216869152354\n",
      "Field study\n",
      "[[209   9]\n",
      " [  5  27]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97       218\n",
      "           1       0.75      0.84      0.79        32\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       250\n",
      "   macro avg       0.86      0.90      0.88       250\n",
      "weighted avg       0.95      0.94      0.95       250\n",
      "\n",
      "F1 score:  0.8808551198257081\n",
      "No category\n",
      "[[190  30]\n",
      " [  8  22]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.86      0.91       220\n",
      "           1       0.42      0.73      0.54        30\n",
      "\n",
      "   micro avg       0.85      0.85      0.85       250\n",
      "   macro avg       0.69      0.80      0.72       250\n",
      "weighted avg       0.90      0.85      0.86       250\n",
      "\n",
      "F1 score:  0.7228381374722839\n",
      "0.7653333333333333 0.776 0.7555999999999997\n",
      "[[27  0  0  3  0  0]\n",
      " [ 0 31  1  1  0  0]\n",
      " [ 0  0 33  2  1  0]\n",
      " [ 0  1  1 22  3  3]\n",
      " [ 1  0  4 20 67  0]\n",
      " [ 0  0  1  4  0 24]]\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "        Field study       0.96      0.90      0.93        30\n",
      "   Laboratory study       0.97      0.94      0.95        33\n",
      "     Modeling study       0.82      0.92      0.87        36\n",
      "        No category       0.42      0.73      0.54        30\n",
      "Observational study       0.94      0.73      0.82        92\n",
      "       Review paper       0.89      0.83      0.86        29\n",
      "\n",
      "          micro avg       0.82      0.82      0.82       250\n",
      "          macro avg       0.84      0.84      0.83       250\n",
      "       weighted avg       0.86      0.82      0.83       250\n",
      "\n",
      "F1 score:  0.8281859669672368\n"
     ]
    }
   ],
   "source": [
    "#one vs others model\n",
    "calculate_metrics(results_true, model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5    45482\n",
      "1    18173\n",
      "0     6057\n",
      "3     1569\n",
      "4      135\n",
      "2       43\n",
      "Name: label, dtype: int64\n",
      "Used for model gpu 2\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "Used for model gpu 2\n",
      "Used for model gpu 2\n",
      "Saving...\n",
      "Saved to new_sample_for_test.xlsx\n",
      "Saving...\n",
      "Saved to ../notebooks/study_type_all_left_dataset_5_filtered.xlsx\n"
     ]
    }
   ],
   "source": [
    "_study_type_labeler.generate_test_file(\"../notebooks/study_type_all_left_dataset_5_filtered.xlsx\",\"new_sample_for_test.xlsx\",size=80, withBoost=True,use_tail=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covid_data 0.xlsx\n",
      "covid_data\n",
      "Read file ../tmp/covid_data/0.xlsx: 0.13s\n",
      "Processed file ../tmp/covid_data/0.xlsx: 0.25s\n",
      "510\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  179.39667963981628\n",
      "Saving...\n",
      "Saved to ../tmp/covid_data/0.xlsx\n",
      "dfs_data 0.xlsx\n",
      "dfs_data\n",
      "Read file ../tmp/dfs_data/0.xlsx: 2.05s\n",
      "Processed file ../tmp/dfs_data/0.xlsx: 4.79s\n",
      "7370\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  1631.616812467575\n",
      "Saving...\n",
      "Saved to ../tmp/dfs_data/0.xlsx\n",
      "teams_dataset 0.xlsx\n",
      "teams_dataset\n",
      "Read file ../tmp/teams_dataset/0.xlsx: 27.51s\n",
      "Processed file ../tmp/teams_dataset/0.xlsx: 57.20s\n",
      "100000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  20731.1172580719\n",
      "Saving...\n",
      "Saved to ../tmp/teams_dataset/0.xlsx\n",
      "teams_dataset 1.xlsx\n",
      "teams_dataset\n",
      "Read file ../tmp/teams_dataset/1.xlsx: 0.58s\n",
      "Processed file ../tmp/teams_dataset/1.xlsx: 1.32s\n",
      "2160\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  555.4902129173279\n",
      "Saving...\n",
      "Saved to ../tmp/teams_dataset/1.xlsx\n",
      "big_dataset_1 15.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/15.xlsx: 2.33s\n",
      "Processed file ../tmp/big_dataset_1/15.xlsx: 5.67s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  2063.6003267765045\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/15.xlsx\n",
      "big_dataset_1 43.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/43.xlsx: 2.43s\n",
      "Processed file ../tmp/big_dataset_1/43.xlsx: 5.87s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  2152.4186673164368\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/43.xlsx\n",
      "big_dataset_1 26.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/26.xlsx: 2.49s\n",
      "Processed file ../tmp/big_dataset_1/26.xlsx: 5.80s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  2166.691602706909\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/26.xlsx\n",
      "big_dataset_1 27.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/27.xlsx: 2.33s\n",
      "Processed file ../tmp/big_dataset_1/27.xlsx: 5.85s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  2112.0331161022186\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/27.xlsx\n",
      "big_dataset_1 45.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/45.xlsx: 2.48s\n",
      "Processed file ../tmp/big_dataset_1/45.xlsx: 5.43s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  2194.962564945221\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/45.xlsx\n",
      "big_dataset_1 35.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/35.xlsx: 2.31s\n",
      "Processed file ../tmp/big_dataset_1/35.xlsx: 5.89s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  2092.8131761550903\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/35.xlsx\n",
      "big_dataset_1 0.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/0.xlsx: 3.72s\n",
      "Processed file ../tmp/big_dataset_1/0.xlsx: 5.82s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  2291.7342462539673\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/0.xlsx\n",
      "big_dataset_1 2.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/2.xlsx: 3.69s\n",
      "Processed file ../tmp/big_dataset_1/2.xlsx: 5.64s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  2236.759585380554\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/2.xlsx\n",
      "big_dataset_1 49.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/49.xlsx: 2.50s\n",
      "Processed file ../tmp/big_dataset_1/49.xlsx: 6.18s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  2245.037200450897\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/49.xlsx\n",
      "big_dataset_1 47.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/47.xlsx: 3.63s\n",
      "Processed file ../tmp/big_dataset_1/47.xlsx: 5.50s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  2396.1375658512115\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/47.xlsx\n",
      "big_dataset_1 6.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/6.xlsx: 2.38s\n",
      "Processed file ../tmp/big_dataset_1/6.xlsx: 5.20s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  2332.4124596118927\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/6.xlsx\n",
      "big_dataset_1 11.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/11.xlsx: 2.39s\n",
      "Processed file ../tmp/big_dataset_1/11.xlsx: 5.19s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  2368.330906867981\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/11.xlsx\n",
      "big_dataset_1 24.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/24.xlsx: 3.46s\n",
      "Processed file ../tmp/big_dataset_1/24.xlsx: 5.17s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  2302.3347673416138\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/24.xlsx\n",
      "big_dataset_1 37.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/37.xlsx: 2.34s\n",
      "Processed file ../tmp/big_dataset_1/37.xlsx: 6.25s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  2337.7047855854034\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/37.xlsx\n",
      "big_dataset_1 31.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/31.xlsx: 2.33s\n",
      "Processed file ../tmp/big_dataset_1/31.xlsx: 5.08s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  2185.4276506900787\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/31.xlsx\n",
      "big_dataset_1 13.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/13.xlsx: 2.34s\n",
      "Processed file ../tmp/big_dataset_1/13.xlsx: 5.11s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  2385.426644563675\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/13.xlsx\n",
      "big_dataset_1 3.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/3.xlsx: 3.01s\n",
      "Processed file ../tmp/big_dataset_1/3.xlsx: 5.20s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  2404.0552701950073\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/3.xlsx\n",
      "big_dataset_1 40.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/40.xlsx: 2.36s\n",
      "Processed file ../tmp/big_dataset_1/40.xlsx: 6.40s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  2455.3782444000244\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/40.xlsx\n",
      "big_dataset_1 32.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/32.xlsx: 2.31s\n",
      "Processed file ../tmp/big_dataset_1/32.xlsx: 5.06s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  2302.787623167038\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/32.xlsx\n",
      "big_dataset_1 36.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/36.xlsx: 2.31s\n",
      "Processed file ../tmp/big_dataset_1/36.xlsx: 5.02s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  2421.2104349136353\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/36.xlsx\n",
      "big_dataset_1 25.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/25.xlsx: 2.63s\n",
      "Processed file ../tmp/big_dataset_1/25.xlsx: 5.14s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  2471.7006001472473\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/25.xlsx\n",
      "big_dataset_1 8.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/8.xlsx: 2.69s\n",
      "Processed file ../tmp/big_dataset_1/8.xlsx: 7.38s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  2571.4766702651978\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/8.xlsx\n",
      "big_dataset_1 46.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/46.xlsx: 2.60s\n",
      "Processed file ../tmp/big_dataset_1/46.xlsx: 5.47s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  2689.9079682826996\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/46.xlsx\n",
      "big_dataset_1 21.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/21.xlsx: 2.32s\n",
      "Processed file ../tmp/big_dataset_1/21.xlsx: 6.76s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  2493.975914955139\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/21.xlsx\n",
      "big_dataset_1 9.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/9.xlsx: 2.34s\n",
      "Processed file ../tmp/big_dataset_1/9.xlsx: 5.12s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  2571.327895641327\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/9.xlsx\n",
      "big_dataset_1 28.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/28.xlsx: 2.28s\n",
      "Processed file ../tmp/big_dataset_1/28.xlsx: 5.00s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  2436.136074781418\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/28.xlsx\n",
      "big_dataset_1 41.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/41.xlsx: 2.34s\n",
      "Processed file ../tmp/big_dataset_1/41.xlsx: 5.09s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  2625.199424982071\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/41.xlsx\n",
      "big_dataset_1 17.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/17.xlsx: 2.29s\n",
      "Processed file ../tmp/big_dataset_1/17.xlsx: 6.84s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  2537.2222197055817\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/17.xlsx\n",
      "big_dataset_1 16.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/16.xlsx: 4.05s\n",
      "Processed file ../tmp/big_dataset_1/16.xlsx: 5.10s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  2544.7018415927887\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/16.xlsx\n",
      "big_dataset_1 34.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/34.xlsx: 2.29s\n",
      "Processed file ../tmp/big_dataset_1/34.xlsx: 7.02s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  2585.8689765930176\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/34.xlsx\n",
      "big_dataset_1 22.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/22.xlsx: 2.29s\n",
      "Processed file ../tmp/big_dataset_1/22.xlsx: 6.91s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  2663.563889026642\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/22.xlsx\n",
      "big_dataset_1 1.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/1.xlsx: 2.84s\n",
      "Processed file ../tmp/big_dataset_1/1.xlsx: 5.65s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  2764.5092899799347\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/1.xlsx\n",
      "big_dataset_1 50.xlsx\n",
      "big_dataset_1_new\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read file ../tmp/big_dataset_1/50.xlsx: 2.15s\n",
      "Processed file ../tmp/big_dataset_1/50.xlsx: 4.47s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  2783.85972571373\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/50.xlsx\n",
      "big_dataset_1 30.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/30.xlsx: 2.28s\n",
      "Processed file ../tmp/big_dataset_1/30.xlsx: 5.00s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  2641.103017807007\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/30.xlsx\n",
      "big_dataset_1 7.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/7.xlsx: 4.54s\n",
      "Processed file ../tmp/big_dataset_1/7.xlsx: 5.11s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  2840.433693408966\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/7.xlsx\n",
      "big_dataset_1 20.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/20.xlsx: 2.27s\n",
      "Processed file ../tmp/big_dataset_1/20.xlsx: 5.02s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  2704.8552746772766\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/20.xlsx\n",
      "big_dataset_1 12.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/12.xlsx: 2.36s\n",
      "Processed file ../tmp/big_dataset_1/12.xlsx: 5.14s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  2920.2452251911163\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/12.xlsx\n",
      "big_dataset_1 19.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/19.xlsx: 2.29s\n",
      "Processed file ../tmp/big_dataset_1/19.xlsx: 4.99s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  2821.627286195755\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/19.xlsx\n",
      "big_dataset_1 10.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/10.xlsx: 2.33s\n",
      "Processed file ../tmp/big_dataset_1/10.xlsx: 5.11s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  2911.615680217743\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/10.xlsx\n",
      "big_dataset_1 42.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/42.xlsx: 2.31s\n",
      "Processed file ../tmp/big_dataset_1/42.xlsx: 5.06s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  2920.113545894623\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/42.xlsx\n",
      "big_dataset_1 33.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/33.xlsx: 2.28s\n",
      "Processed file ../tmp/big_dataset_1/33.xlsx: 5.08s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  2867.5196645259857\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/33.xlsx\n",
      "big_dataset_1 5.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/5.xlsx: 2.32s\n",
      "Processed file ../tmp/big_dataset_1/5.xlsx: 5.07s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  3044.3591525554657\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/5.xlsx\n",
      "big_dataset_1 48.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/48.xlsx: 2.52s\n",
      "Processed file ../tmp/big_dataset_1/48.xlsx: 7.76s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  2960.9487726688385\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/48.xlsx\n",
      "big_dataset_1 14.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/14.xlsx: 2.29s\n",
      "Processed file ../tmp/big_dataset_1/14.xlsx: 5.09s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  3037.5165255069733\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/14.xlsx\n",
      "big_dataset_1 51.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/51.xlsx: 0.10s\n",
      "Processed file ../tmp/big_dataset_1/51.xlsx: 0.15s\n",
      "311\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  1116.9689316749573\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/51.xlsx\n",
      "big_dataset_1 38.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/38.xlsx: 2.31s\n",
      "Processed file ../tmp/big_dataset_1/38.xlsx: 5.00s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  3186.699833869934\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/38.xlsx\n",
      "big_dataset_1 23.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/23.xlsx: 4.91s\n",
      "Processed file ../tmp/big_dataset_1/23.xlsx: 5.29s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  3154.107394218445\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/23.xlsx\n",
      "big_dataset_1 44.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/44.xlsx: 2.32s\n",
      "Processed file ../tmp/big_dataset_1/44.xlsx: 5.18s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  2900.18603849411\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/44.xlsx\n",
      "big_dataset_1 39.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/39.xlsx: 2.74s\n",
      "Processed file ../tmp/big_dataset_1/39.xlsx: 5.13s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  3253.212907075882\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/39.xlsx\n",
      "big_dataset_1 29.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/29.xlsx: 2.29s\n",
      "Processed file ../tmp/big_dataset_1/29.xlsx: 5.06s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  3177.088039159775\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/29.xlsx\n",
      "big_dataset_1 18.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/18.xlsx: 2.31s\n",
      "Processed file ../tmp/big_dataset_1/18.xlsx: 5.08s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  3261.8016135692596\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/18.xlsx\n",
      "big_dataset_1 4.xlsx\n",
      "big_dataset_1_new\n",
      "Read file ../tmp/big_dataset_1/4.xlsx: 2.48s\n",
      "Processed file ../tmp/big_dataset_1/4.xlsx: 5.29s\n",
      "10000\n",
      "Observational study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Modeling study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Laboratory study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Review paper\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Field study\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "Config is done\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Used for model gpu 0\n",
      "Finished  3347.9645614624023\n",
      "Saving...\n",
      "Saved to ../tmp/big_dataset_1_new/4.xlsx\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from bert_models import base_bert_model_meta_model\n",
    "base_bert_model_meta_model = reload(base_bert_model_meta_model)\n",
    "from bert_models import base_bert_model_meta_model_with_window_all\n",
    "base_bert_model_meta_model_with_window_all = reload(base_bert_model_meta_model_with_window_all)\n",
    "from study_design_type import study_type_labeler\n",
    "study_type_labeler = reload(study_type_labeler)\n",
    "from study_design_type import study_type_labeler_window_agg\n",
    "study_type_labeler_window_agg = reload(study_type_labeler_window_agg)\n",
    "\n",
    "\n",
    "for folder in [\"covid_data\", \"dfs_data\", \"teams_dataset\", \"big_dataset_1\"]:\n",
    "    for filename in os.listdir(os.path.join(\"../tmp\", folder)):\n",
    "        print(folder, filename)\n",
    "        new_folder = (folder+\"_new\") if folder == \"big_dataset_1\" else folder\n",
    "        print(new_folder)\n",
    "        if folder != new_folder and os.path.exists(os.path.join(\"../tmp\", new_folder, filename)):\n",
    "            print(\"Already processed\")\n",
    "            continue\n",
    "        start = time()\n",
    "        df = excel_reader.ExcelReader().read_df_from_excel(os.path.join(\"../tmp\", folder, filename))\n",
    "        df[\"study_type\"] = \"\"\n",
    "        for i in range(len(df)):\n",
    "            df[\"study_type\"].values[i] = []\n",
    "        print(len(df))\n",
    "\n",
    "        for label in StudyTypeLabels.STUDY_TYPE_LABEL_TO_NUMBER:\n",
    "            if label == \"No category\":\n",
    "                continue\n",
    "            print(label)\n",
    "            _study_type_labeler = study_type_labeler_window_agg.StudyTypeLabeler(\n",
    "                \"/hdd2/data/maryia_pavlovets/study_type_exps_3/study_type_with_tails_%s\"%label,\n",
    "                max_seq_length = 256, batch_size = 16,\n",
    "              gpu_device_num_hub = 0, gpu_device_num = 0,\n",
    "              model_folder = \"../model/scibert_scivocab_uncased\",\n",
    "              label_list = [0,1],\n",
    "              meta_model_folder = \"/hdd2/data/maryia_pavlovets/study_type_exps_3/meta_model_%s_1layer_0.8_new_meta\"%label,\n",
    "              use_one_layer=True,\n",
    "              keep_prob=0.8,\n",
    "              epochs_num=8\n",
    "              )\n",
    "            \n",
    "            r = _study_type_labeler.predict_with_meta_model(df[[\"title\", \"abstract\"]], with_head_tail = True)\n",
    "            for i in range(len(r[0])):\n",
    "                if r[0][i][1] >= 0.5:\n",
    "                    df[\"study_type\"].values[i].append(label)\n",
    "        \n",
    "        for i in range(len(df)):\n",
    "            if not len(df[\"study_type\"].values[i]):\n",
    "                df[\"study_type\"].values[i] = [\"No category\"]\n",
    "        print(\"Finished \", (time() - start))\n",
    "        os.makedirs(os.path.join(\"../tmp\", new_folder), exist_ok=True)\n",
    "        excel_writer.ExcelWriter().save_df_in_excel(df, os.path.join(\"../tmp\", new_folder, filename))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Predicting Movie Reviews with BERT on TF Hub.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
